---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Combining Forecast Result and Forecast Evaluation

**Literature:**

From the reading list:

+ HW: Judgmental Forecasting and Forecast Adjustments (Combining forecasts)
+ HW: Managing the Forecast Process

From the slides:

+ Diebold-Mariano and other tests for equal forecast accuracy: http://www.phdeconomics.sssup.it/documents/Lesson19.pdf


## Why, when and how to combine forecasts

### Why do we combine forecasts?

If you have different approaches and you know that they are both contribution with some valuable information, making discrete selection, hence one forecast method, will permanently exclude the information from the other model.

Hence, by combining methods, we are able to collect more information in the model, that would not be captured with only using a single model.


### When to combine forecasts?

Combining models is usually applied with methods, that are not from the same group, e.g., ARIMA and ARIMA, but instead e.g., ARIMA and exponential smoothing, as it approaches data in two different ways.

Also, you often want to include different models that contain different information, to capture different perspectives, e.g., in terms of data analysis or in terms of different variables.


### How to combine forecasts?

See an example in section \@ref(ex:AirPassengers)

**How to combine forecasts?**
	
+ You assign weights to the forecasts, hence:

\begin{equation}
F_{combined}=w_1F_1+w_2F_2
(\#eq:CombinedForecast)
\end{equation}

**How to find the weights? The following are different methods;**
	
+ Nelson combination method! Here there are restrictions on the weights (How much? I think to one.)
+ Granger-Ramanathan - it has shown to be better than Nelson combination method.
	+ This has no restrictions, the weights must not sum to 1.
+ Time-varying weights: for each period of time, we are calculate the weight is each period of time.

**Diebold-Mariano test**

*Are MSE's of different forecasts equal?*

+ Says that, we are going to compare two forecasts and see if there is a difference in performance or not.
  + Hence: H0: MSE1=MSE1
    + This use Diebold-Mariano test statistic
		+ If we are not able to reject H0, then there
+ Rule of thumb, if you have two different methods yielding the same result, then you can make a DM test to see if the MSEs are actually different, if not, then you should consider combining
	+ But notice, if the models are the same approach, then you should probably just pick one


## Exercises

### Air passengers {#ex:AirPassengers}

Loading the data

```{r}
df <- read_excel("Data/Week49/AirPassengers.xlsx")

y <- ts(df[,2] #The passengers variable
        ,start=1949
        ,frequency = 12)
tsdisplay(y)
```

We see that there is clearly an upwards trend and seasonality. Also the variance appear to be increasing, hence perhaps the composition is multiplicative.

Also looking at the ACF, it become clearer with the seasonality and trend.

To validate models, we are going to split the sample into two partitions, train and test data.

```{r}
# In-sample (75%) and out-of-sample (25%) split
insamp <- ts(y[1:108] #75% of the data
             ,frequency = 12)
outsamp <- y[109:144] #The rest 25%. We dont care about frequency, as we just need the observation for comparison
l<-length(outsamp) #generate a number called "l" equal to the length of the test set
```


#### Producing forecasts + combined forecast

We are going to make two forecasts:

1. ARIMA
2. HoltWinters

##### 1. Constructing forecasts

###### a. Forecast 1 - ARIMA

Now we can do the first forecasts.

```{r}
fit <- auto.arima(y = insamp
                  ,seasonal = TRUE) #This is in fact redundant, but as we see seasons, we can just as well just tell R, that this is the case

summary(fit) #model of choice is ARIMA(1,1,0)(0,1,0)[12]
```

We get a model 1,1,0 model, implying an AR and integration.

As the ACF tend towards 0 and the pacf drastically drops after the first lag, we expected the AR(1) to be applicable, and since the data is clearly not stationary, the first order differences are also expected.

Also we see (0,1,0)[12], meaning that each period is repeated for each twelve months

Now we can make the model diagnostics.

```{r}
tsdisplay(residuals(fit)
          ,main = 'Model Residuals')
```

We see that the residuals appear to be stationary. The ACF only has two spikes that appear to be significant, but otherwise nothing crucial.

Now we can forecast with horizon l, corresponding with the length of the hold-out sample.

```{r}
fcast <- forecast(fit,h = l)
{plot(fcast) 
lines(y)}
```

Now we can assess the out of sample performance.

```{r}
accuracy(object = fcast$mean,x = outsamp)
```

We see that the RMSE being 22.13, hence MSE being $22.13^2 = 489.7369$

Now we must make another model, that is candidate of the combined model.

###### a. Forecast 2 - HoltWinters

```{r}
fit2 <- HoltWinters(insamp)
fcast2 <- forecast(fit2, h=l)
fit2
```

We see that it suggests alpha of 24% and beta with 5%, that is expected, as we clearly see a trend.

We see that the last 12 observations are included for consideration.

As we have trends and seasons, then it is Winters exponential smoothing.

```{r}
{plot(fcast2) 
lines(y)}
```

Now we can assess the accuracy on the hold out sample.

```{r}
accuracy(fcast2$mean, outsamp)
```

We see that the RMSE = 28.81


##### 2. Diebold-Mariano test 

The DM test assess the MSE of each model and make as statistical test to assess if they are significantly different from each other.

Where H0: the forecasts have the same accuracy

```{r}
dm.test(residuals(fcast)
        ,residuals(fcast2)
        ,h = l)
```

We see that the p-value is 0.15, hence we are not able to reject on a 5% level. Hence it is fair to assume, that both models are equally good.

##### 3. Combining the forecasts

There are two approaches to this:

a. Nelson
b. Granger-Ramanathan: This is often better.

###### a. Nelson Combination Method - her code is put here, but not further explained

Just use GR.

```{r}
combfitN <- lm(outsamp ~ fcast$mean + fcast2$mean)
summary(combfitN)
#the intercept is sgnificant => there is a bias, we need to correct the data for it

outsampcor<-outsamp-combfitN$coefficients[1] #where combfitN$coefficients[1] picks out the intercept value from the estimated regression
# Now want to run an OLS without an intercept on the corrected (debiased data)
#with respect to a restriction on the weights:  w1 + w2 = 1
fitW <- lm(outsampcor ~ 0+ offset(fcast$mean) + I(fcast2$mean-fcast$mean))
coef_2 <- coef(fitW)
beta_1 <- 1 - coef_2 #the weight is negative, would prefer a different combination method in this case
beta_2 <- coef_2
#beta_1 and beta_2 will give you the weigths. 
# Now can use those weights to obtain a combination forecast
combfcastN <-beta_1*fcast$mean+beta_2*fcast2$mean
plot(combfcastN)
accuracy(combfcastN, outsamp) #can see that in this case the forecast combination performes worse than the individual forecasts
```


###### b. Granger-Ramanathan

```{r}
combfit <- lm(outsamp ~ fcast$mean + fcast2$mean) #Mean as we want point estimates
summary(combfit) #the coefficients in the regression will give you the weights
```

We see that forecast 1 (ARIMA) has weight 0.6794 where forecast 2 (HW) has a weight of 0.5734

```{r}
combfcast <- ts(combfit$fitted.values, frequency = 12)
plot(combfcast)
```

The combined forecast is plotted. It looks as expected, and appear to be replicating the trend and the seasonality. Let us see what accuracy it gets.

```{r}
accuracy(combfcast, outsamp)
```

We see that the RMSE is 14.50

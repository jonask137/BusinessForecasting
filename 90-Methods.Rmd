
# Methods and Performance Measurement {#Methods}

## Forecasting Methods {#ForecastMethods}

**Naive forecasts**

*This is merely the current period is assumed to be the best predictor for the future, hence it can be written as:*

\begin{equation} 
\hat{Y}_{t+1}=Y_t
(\#eq:NaiveForecast)
\end{equation}

where, $Y_t$ = the last period, hence $\hat{Y}_{t+1}$ = the following period.

Therefore, the error can merely be written as: $e_t=Y_{t+1}-\hat{Y}_{t+1}$, being the actual amount compared with the foretasted value.

One can make several iterations to account for trending, the growth rate, or seasonal data. Those being:

+ $\hat{Y}_{t+1}=Y_t+(Y_t-Y_{t-1})$, to account for trending data (non stationary data)
+ $\hat{Y}_{t+1}=Y_t * \frac{Y_t}{Y_{t-1}}$, to account for the growth rate, notice that it only assess the growth rate to the prior period.
+ $\hat{Y}_{t+1}=Y_{t-3}+\frac{Y_t-Y_{t-4}}{4}$, to account for  quarterly trending data, the periods can naturally be changed by changing the formula, e.g. to 12. ***but notice, that this is just replicating previous periods, hence also previous seasons***

### Using Averages

We have the following:

1. Simple averages, which merely takes the average of all observations.
2. Moving averages, which account for the given time frame. This can be extended by,
3. Double moving averages, often seen when you need the center value of a period consisting of an even number of periods, where there is no actual median value, thus one can extend the MA with a double MA.

#### Simple Averages

*One may assume that it is sufficient to apply the average of all observations, to predict the next period, hence we can say:*

\begin{equation} 
\hat{Y}_{t+1}=\frac{1}{n}\sum^t_{i=1}Y_i
(\#eq:SimpleAverages)
\end{equation} 


This is appropriate if the data has shown historical stability, thus without seasons, trends and etc.

#### Moving Average (MA) {#MA}

*One may apply a moving average instead, accounting for k periods, also one could extend this by adding weights. For practical purposes only a k period MA is show:*

\begin{equation}
\hat{Y}_{t+1}=\frac{Y_t+Y_{t-1}+...+Y_{t-k}}{k}
(\#eq:MA)
\end{equation}

*this may be applied to remove seasonal effect, either by k=4 or 12 if the data is respectively quarterly or monthly*

#### Double Moving Average

*This is simply doing moving averages twice, hence it is an extension of equation \@ref(eq:MA)*

As mentioned, often seen when one wants the median when using an even number of periods, e.g. 12 months, hence double MA can be applied.

\begin{equation}
M_t=\hat{Y}_{t+1}=\frac{Y_t+Y_{t-1}+...+Y_{t-k+1}}{k}
(\#eq:DMA)
\end{equation}

\begin{equation}
M'_t=\frac{M_t+M_{t-1}+...+M_{t-k+1}}{k}
(\#eq:DMA2)
\end{equation}

\begin{equation}
a_t=M_t+\left(M_t-M_t\right)
\end{equation}

\begin{equation}
b_t=\frac{2}{k-1}\left(M_t-M'_t\right)
\end{equation}

Thence we are able to say:

\begin{equation}
\hat{Y}_{t+p}=a_t+b_t*p
(\#eq:DMAPPeriods)
\end{equation}


### Linear regressions

+ Linear regression with a trend: that is normal linear regression, where the trend is added as a counter, which will account for the trend, given it is linear.
+ Linear regression with seasonal dummies and a trend: That is making dummy variables for each period, using `seasonaldummary()`, to have a variable accounting for each period. The you can add a trend variable `seq(1,n,1)`, e.g., linear counter, when having linear trend.
  + See \@ref(ex:MonthlySalesData)

### Non linear regressions

+ Non linear regression with trend
+ Causal regression

### Smoothing methods

#### Exponential smoothing

*This is exponentially weighted moving average of all historical values, meaning that the most recent value will be assigned the most weight. Hence we merely add different weights to past periods, thus there is no specific way to adjust for trend and seasonality, which is a limitation of exponential smoothing, it can be written as:*

\begin{equation}
\hat{Y}_{t+1}=\alpha Y_t+\left(1-\alpha\right)\hat{Y}_t
\end{equation}
thus:
\begin{equation}
=\hat{Y}_t+\alpha(Y_t-\hat{Y}_t)
\end{equation}

Where $\alpha$ = the smoothing constant, thus is can be between 0 and 1. The higher alpha the largest weight to the most recent observation.

**Then how to choose the smoothing parameter $\alpha$?**

+ For stable predictions, choose a high alpha
+ For sensitive predictions, choose low alpha
+ Test different alpha values and compare the models using the performance measures in section \@ref(PerformanceMeasurements).

#### Holt's exponential smoothing

*Exponential smoothing method with adjustment for trend, hence we introduce a new tuning parameter, hence we have $\alpha$ and $\beta$*

+ $\alpha$ = Weight to the most recent observations
+ $\beta$ = adjustment for trend. R will automatically set this, when beta = TRUE

Hence the smoothing now consists of two elements:

1. The level estimate

\begin{equation}
L_t=\alpha Y_t+\left(1-\alpha\right)\left(L_{t-1}+T_{t-1}\right)
(\#eq:LevelEstimateHolts)
\end{equation}

2. The trend estimate

\begin{equation}
T_t=\beta\left(L_t-L_{t-1}\right)+\left(1-\beta\right)T_{t-1}
(\#eq:TrendEstimateHolts)
\end{equation}

Thus, the forecasting of p periods into the future, can be explained by:

\begin{equation}
\hat{Y}_{t+p}=L_t+pT_t
(\#eq:ForecastPPeriodsHolts)
\end{equation}

**To apply**: use `HoltWinters()` and select parameters that lowers the performance measurements.

When one assigns large weights the model will become more sensitive to changes in the observed data.

One can either set the initial value to 0 or take the average of the first few observations.

If $\alpha = \beta$, then we have the Brown's double exponential smoothing model.

If $\beta$ = 0, then we merely have a simple exponential smoothing.


#### Winters' exponential smoothing

*Exponential smoothing method with adjustment for trend and seasonality, hence we introduce two new tuning parameters, hence we have $\alpha$, $\beta$ (as in Holt's) and $\gamma$*

Hence the smoothing now consists of three elements:

1. The level estimate

\begin{equation}
L_t=\alpha\frac{Y_t}{S_{t-s}}+\left(1-\alpha\right)\left(L_{t-1}+T_{t-1}\right)
(\#eq:LevelEstimateWinters)
\end{equation}

2. The trend estimate

\begin{equation}
T_t=\beta\left(L_t-L_{t-1}\right)+\left(1-\beta\right)T_{t-1}
(\#eq:TrendEstimateWinters)
\end{equation}

3. The seasonality estimate

\begin{equation}
S_t=\gamma\frac{Y_t}{L_t}+\left(1-\gamma\right)S_{t-s}
(\#eq:SeasonEstimateWinters)
\end{equation}

Thus, the forecasting of p periods into the future, can be explained by:

\begin{equation}
\hat{Y}_{t+p}=\left(L_t+pT_t\right)S_{t-s+p}
(\#eq:ForecastPPeriodsWinters)
\end{equation}

**To apply**: use `HoltWinters()` and select parameters that lowers the performance measurements.

When one assigns large weights the model will become more sensitive to changes in the observed data.

One can either set the initial value to 0 or take the average of the first few observations.

if $\beta = \gamma = 0$ the model is merely simple exponential smoothing.


#### Moving Averages, see section \@ref(MA)


### ARIMA

*Decomposition of the time series*

+ AR: Autoregressive, giving coefficients to lagged values
+ MA: Giving residuals to previous residuals
+ ARMA: Combination of AR and MA in a stationary setting
+ ARIMA: Combination of AR and MA in a nonstationary setting


### Dynamic forecasting

That is when variables are assumed to be affecting each other, hence combination of autoregression (using y series lags) and 1 or more other variables in the regression setting.

We have ADL and VAR.

#### ADL

This is an autoregressive model (AR) with lags of another variable, this yields

\begin{equation}
ALD_{\left(p,q\right)}
(\#eq:ADL)
\end{equation}

#### VAR

One is able to construct different linear models, and see how they influence each other. Therefore, it is a good approach to assessing exogeniourity and endogeniourity.


### Decompisition

This is decomposing the data into its composing and the assembling $\hat{y}$ as a composition of e.g., trend and seasonality. Notice that one must be aware of whether y is the product or the sum of the components.


\

## Performance Measurements {#PerformanceMeasurements}

### Error terms

+ Mean error (ME): 

\begin{equation}
ME=\frac{1}{n}\sum_{ }^{ }\left(Y_t-\hat{Y}_t\right)$
(\#eq:ME)
\end{equation} 

+ Mean Absolute Deviation (error): 

\begin{equation}
MAD\left(i.e.\ MAE\right)\ =\ \frac{1}{n}\cdot\sum_{ }^{ }\left|Y_t-\hat{Y}_t\right|
(\#eq:MAD)
\end{equation} 

+ Mean Percentage Error (MPE): 

\begin{equation}
MPE\ =\ \frac{1}{n}\ \sum_{ }^{ }\frac{\left(Y_t-\hat{Y}_t\right)}{Y_t}
(\#eq:MPE)
\end{equation} 

+ Mean Absolute Percentage Error (MAPE): 

\begin{equation}
MAPE\ =\ \frac{1}{n}\ \sum_{ }^{ }\frac{|\left(Y_t-\hat{Y}_t\right)|}{|Y_t|}
(\#eq:MAPE)
\end{equation} 

+ Mean-Squared Error (MSE): 

\begin{equation}
MSE=\frac{1}{n}\sum_{ }^{ }(Y_t-\hat{Y}_t)^2
(\#eq:MSE)
\end{equation}

+ Root Mean-Squared Error: 

\begin{equation}
RMSE=\sqrt{MSE}
(\#eq:MSE)
\end{equation}

### Multicollinearity

__VIF__

\begin{equation}
VIF_j=\frac{1}{1-R_j^2}
(\#eq:VIF)
\end{equation}

Where $j = 1,...,k$

Thus, we see that Rsquare is obtained from regression each IDV against the remaining variables. We can then have the following outputs:

+ VIF = 1, no milticollinearity
+ VIF > 10, indicates multicollinearity

If one gets an indication of multicollinearity, then one should drop one of the correlated variables.


## Statistical tests

The following are tests that we have done throughout the semester. Notice, that code examples are random, so the input may vary, e.g., not the fitted values but residuals can be put in.

**Bresuch-Pagan test (BP)**

+ This is a test for heteroskedasticity
  + H0: Homoskedasticity
  + `bptest(fit)`

**Jarque-Beta test**

+ This is an assessment of normality
  + H0: The data is normally distributed
  + `jarque.bera.test(m.arima$residuals)`
  
**Durbin-Watson test (DW)**

+ This is a test for serial correlation (autocorrelation).
  + H0: No autocorrelation
  + `dwtest(fit)`

**Ljung-Box test**

+ Test for absence or serial correlation, i.e., autocorrelation, accounting for p and q lags.
  + H0: No autocorrelation
  + `Box.test(ARIMAmod$residuals,type = "Ljung-Box",fitdf = p+q)`, must define p and q

**ACF correlogram**

+ Test for autocorrelation. Visual inspection, see if there are spikes over the confidence level
  + `acf()`, `tsdisplay()` will also show it

**Augmented Dickey-Fuller test (ADF)**

+ Test for stationarity
  + H0: Non-stationary series
  + `adf.test(y)`

**Portmanteau test (symptotic)**

+ The Portmanteau statistic for testing the absence of up to the order h serially correlated disturbances in a stable VAR(p).
  + H0: No autocorrelation
  + `serial.test(fit, lags.pt=10, type="PT.asymptotic")`
  + *This was applied with VAR(p) only*

**Phillips-Ouliaris test (PO)**

+ Test for cointegration (follows theh 2 tep EG test)
  + H0: No cointegration
  + `serial.test(var1,lags.pt = 10,type = "PT.asymptotic")`, notice, that the lags.pt. may be adjusted, e.g., take one or two seasonal cycles.

**Diebold-Mariano test (DM)**

+ The tests whether two tests have the same accuracy
  + H0: Forecast a and b have the same accuracy
  + `dm.test(e1 = resid(HW)e2 = resid(arima.bc)[25:2016],alternative = "two.sided")`


**Variance Inflation Factor (VIF)**

+ Test for multicollinearity
  + If larger than 9, then assume multicollinearity
  + `vif(fit)`


## Formulas

- `auto.arima()` from forecast package, automize the orders and drift. With possibility to select ic and selection method (stepwise vs. non stepwise)
  - This may yield an ARIMA(0,0,0)(0,1,0)[12] saying that ARIMA is 0,0,0 order, but the seasonality is of differencing order 1. We see that the frequency is 12, meaning that each period is subtracted with the previous period (found the difference).
- `tsdisplay()` from forecast package, does acf, pacf and residuals, if the timeseries$residuals are made as input
- `diff()` does desired order of differencing
- `Arima()`, practically does the same. Although we are able to call another model, hence other coefficients, thus the coefficients will not be estimated again **That is pretty useful!!!**


## Loops!!!!

### HoltWinters Smoothing - finding optimal frequency

This is develop by anton

```{r}
# y <- read_excel("prob8p92HW.xlsx")
#         #' *set up list of different possible seasonality components*
#         m <- list()
#             m[[1]] <- 2
#             m[[2]] <- 4
#         #' *run loop calculating RMSE for winters exponential smoothing with different frequencies*
#             RMSE <- matrix(0,length(m),1)
#                 for (i in seq(length(m)))
#                 {
#                         yt <- ts(y, frequency = m[[i]])
#                         fit <- HoltWinters(yt
#                                             ,alpha=0.6 #setting alpha
#                                             ,beta=TRUE #trend
#                                             ,gamma=TRUE) #seasonality
#                         for.fit <- forecast(fit)
#                         acc.fit <- accuracy(for.fit)
#                         RMSE[i,1] <- acc.fit [1,2] #putting RMSE for different frequencies in matrix
#                 }
#         #' *finding frequency with smallest RMSE*
#             best.freq <- which.min(RMSE)
#             best.freq <- m[best.freq]
#             best.freq
```





### ARIMA - it is not really that useful, does not have all combinations

Testing different orders

```{r}
# #Insert data as timeseries
# y <- read_excel("Data/Week47/IBMstock.xls") %>% ts(frequency = 52
#                                                    #,start =
#                                                    )
# #Import matrix of differencing combinations
# OrderMatrix <- read_excel("Data/Week47/OrdersMatrix.xlsx")
# 
# {
#   RMSE <- as.matrix(0)
#   boxtest <- as.matrix(0)
# 
#   for (i in seq(from = 1,to = nrow(OrderMatrix),by = 1)) {
# 
#     print(i)
#     p <- as.numeric(OrderMatrix[i,1]) #AR order
#     d <- as.numeric(OrderMatrix[i,2]) #Differencing order
#     q <- as.numeric(OrderMatrix[i,3]) #MA order
#     order <- c(p,d,q)
# 
#     ARIMAmod <- arima(x = y #The time-series
#                       ,order = order
#                       )
# 
#     #Assessing in-samp accuracy RMSE
#     RMSE[i] <- accuracy(object = fitted(ARIMAmod),x = y)[2] #2 for RMSE
# 
#     #Storing hypothesis test of independence
#     boxtest[i] <- Box.test(ARIMAmod$residuals
#                            ,fitdf = p+q)$p.value #Because it is applied to and ARIMA model
# }
# 
#   #The optimal combination based on the highest Box-Pierce test (similar to Ljung-Box)
#   OrderMatrix[which.max(boxtest),]
# }

```









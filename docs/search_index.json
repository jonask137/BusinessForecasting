[["index.html", "Business Forecasting setup", " Business Forecasting Jonas Ringive Korsholm setup All the output formatting is done in the _output.yml file while all bookdown rendering options is done in the _bookdown.yml file.§ library(bookdown) Tips and tricks Referencing This is making a figure with a caption and where we center the figure and set the size. Notice, that the figure is automatically numbered according to the chapter number. plot(pressure,type = &#39;b&#39;,pch = 19) Figure 0.1: A fig Now we are able to make a reference to the chunk. Refer to a chunk: 0.1 Options RMarkdown Reference Guide "],["introduction.html", "1 Introduction", " 1 Introduction "],["curriculum.html", "2 Curriculum", " 2 Curriculum Description of qualifications (Expectations from you): This course will provide you with the ability to: Understand and argue why forecasting is important and discuss different approaches/strategies/principles implemented in the business world. Explain the key difference between qualitative and quantitative methods in forecasting. Reason and argue for which model to use in the face of real-world business situations. Carry out static and dynamic forecasting based on linear regressions and time-series methodology. Evaluate the accuracy of forecast outcomes. Contents: This course is designed to give a solid theoretical and applied background to graduate students in forecasting. Students are expected to have taken Quantitative Research Methods, or an equivalent course that covers regression analysis with a good understanding of the statistical methodology used. The course will not only be a methodology course but equally an applied course in that students will develop skills to approach business life situations critically, evaluate and communicate their findings with ease. The applications that immediately follow the theoretical topics to be taught will cover different business topics including quality control, product demand analysis, marketing and advertising. Tentative Outline of Course Topics: Qualitative Forecasting Methods Quantitative and qualitative forecasting New product forecasting + Executive opinions Sales forces opinions Consumer surveys Delphi method Forecast process, data considerations and model selection Trend, seasonal, cycle and irregular components + Statistical review Correlograms Moving average and exponential smoothing methods Moving average Holt’s and Holt-Winter’s exponential smoothing models + Product demand forecasting Forecasting with static regression methods Bivariate regression model review Forecasting with simple linear trend Serial correlation and heteroskedasticity Time-series decomposition Basic time series decomposition models + Deseasonalizing and seasonal indices Time-series decomposition forecast Applications ARIMA processes and Box-Jenkins methodology Moving average models + Autoregressive models Mixed autoregressive and moving average models Model selection, Box-Jenkins identification process, estimation of ARIMA processes Forecasting seasonal time series Applications Dynamic forecasting of economic and financial time series Nonstationary time series + Cointegration Spurious regression Combining forecast results Some basic theorems on diversification of forecasts + Nelson combination method Granger-Ramanathan combination method Combinations with time-varying weights Applications Forecast evaluation Measures of forecasting accuracy Diebold-Mariano test for significant differences in forecasting accuracies "],["qualitative-forecasting-methods.html", "3 Qualitative Forecasting Methods", " 3 Qualitative Forecasting Methods – HW: Introduction to forecasting – HW: Judgmental Forecasting and Forecast Adjustments – HW: A Review of Basic Statistical Concepts – Armstrong et al. (2000), “Sales Forecasts for Existing Consumer Products and Services: Do Purchase Intentions Contribute to Accuracy?” International Journal of Forecasting, 16(3), 383-397. – Lawrence et al. (2006), “Judgmental forecasting: A review of progress over the last 25 years,” International Journal of Forecasting, 22, 493-518. – Lynn et al. (1999), “Survey of New Product Forecasting Practices in Industrial High Technology and Low Technology Businesses,” Industrial Marketing Manage- ment, 28, 565-571. – Bass, Frank M. (1969). “A new product growth model for consumer durables,” Management Science, 15, 215-227. "],["judgmental-forecasting-and-forecast-adjustments.html", "3.1 Judgmental Forecasting and Forecast Adjustments", " 3.1 Judgmental Forecasting and Forecast Adjustments A forecast on its own does not include the analysts judgements well enough. To include experts insights, one can apply different methods for this, that is called judgmental forecasting. There er different methods for including experts input on the analysis. See below: The Delphi Method: That is an iterative process where experts makes analysis an predictions independently, then the findings are distributed and then they are able to independently make corrections to their output if they feel to. The results are then distributed and they are possible to correct their output The feedback loop ends when they have reached redundancy and they make no more corrections. Thus, the output is different expert views, that you can select between. Scenario writing: you have different experts that write scenarios that are likely to happen in the future, you rank those events after likelyhood. The scenario writing is followed by discussion. Thus they are able to defend and modify predictions. Combining forecasts: This is where you make different predicte analysis perhaps not including the same predictors, then you use the outputs to compute a forecast, that is done either by finding the mean of the forecasts or assigning weights to the different forecasts that now act as predictors. Forecasting and Neural Networks: NN is used to find its own variables, it is particularly good to fill in missing values. Other tools to judgmental decision making: The decision making tree, where you assign costs and probabilities to events. Bayesian statistics, also where you create a tree of probabilities. "],["articles.html", "3.2 Articles", " 3.2 Articles 3.2.1 A new product growth model for consumer durables,” Management Science The Bass Model i.e. the Bass Curve is the model for sales of new products, or more correct a generalization of the sales for a product, hence he created the model for adopters, where it has later been separated into innovators, early adopters, early majority, etc. that is then the product life cycle. Thus the purpose of the Bass Model is to predict sales of a new product, based on questionnaires or early sales The Bass Curve is based on the early adopters and calculates the curve based on their numbers. The model has proven that it only needs very few observations to plot when it is going to peak and at what amount. Basically the model is based on a conditional likelihood with respect to time. In fact, it has shown to predict peak and sales amount based on research and questionnaires prior to sales Then there are some notes on what further research that can be done and some notes on extensions to the Bass Model 3.2.2 Sales Forecasts for Existing Consumer Products and Services: Do Purchase Intentions Contribute to Accuracy? As we know from the Bass Model we can use consumer intentions (questionnaires about whether a person think he will buy it or not) to guestimate the total sales and when it is going to peak, but are we are able to optimize accuracy of forecasts on existing products using consumer intentions? A previous study has shown, that intentions does not improve the model, although the data did not have available the intentions and whether the person made a purchase or not. In this study there is an improvement to the forecast when the historical data is combined with the intentions of the persons. Also they ruled out, the factor that if the data does not have historical data on intentions and sales history. Hence they experienced on their seven data set, that combining historical data and intentions improved the forecast. To be noted is that the manipulated the intentions in different ways to experiment, e.g. moving average, binary (intentions or not) and some other faucets. 3.2.3 Judgmental forecasting A review of progress over the last 25 years, International Journal of Forecasting This is basically just more information on the judgemental forecasting where they tested different set ups, such as groups, bootsstrapping etc. Did not read all of this, it seemed very much like HW9 "],["exploring-patterns-and-forecasting-techniques.html", "4 Exploring Patterns and Forecasting Techniques", " 4 Exploring Patterns and Forecasting Techniques This chapter will elaborate on how one identify patterns in data thus how to account for this. Thus, we are looking into smoothing methods and moving averages Additionally, we will explore how we select forecasting methods Literature: HW: Exploring Data Patterns and an Introduction to Forecasting Techniques HW: Moving Averages and Smoothing Methods Armstrong, J.S. (2001) “Selecting Forecast Methods,” In Principles of Forecasting: A Handbook for Researchers and Practitioners (Ed. J. Scott Armstrong), Kluwer "],["the-forecasting-process.html", "4.1 The forecasting process", " 4.1 The forecasting process The following describe the forecasting process, hence what one must consider before performing the forecast and ultimately using the forecasts, the purpose of the process is to make sure that the forecast is reliable Specify objectives Reason for the forecast Applications based on the forecast Good communication between all those involved Determine what to forecast Based on set objectives, choose key indicator(s) Example: domestic sales, export sales, or even both? Identify time dimension Length and periodicity of the forecast Desired frequency Urgency of the forecast Planning of the forecast Data considerations Available and quantity of the data Internal vs. external data Desired frequency in data (annual, quarterly, monthly) Example: Dollar sales instead of unit sales Model selection The pattern exhibited by the data The quantity of historic data available The length of the forecast horizon Figure 4.1: Model Selection Model evaluation Testing the models on the series to be forecast Checking how each model works ‘in sample’ Measures such as MSE, RMSE, etc. used to rank models Fit (in sample) vs. accuracy (out of sample) Forecast preparation Based on the selected model, obtain the forecast Keep possibly competing models See if their combination yields mode accuracy Presentation of forecast result Clear communication Keep it as simple as possible Visual aids to support the findings Tracking results Comparison of forecasts to actual values Re-specify the selected model(s) over time if necessary Try other model combinations to keep the accuracy level intact Conclusion: One should realize that it is an iterative process, that one must be aware of. "],["data-patterns-and-terminology.html", "4.2 Data Patterns and terminology", " 4.2 Data Patterns and terminology Basically the data is assumed to consist of up to four components, that is: Trend Long-term change in the level of data Positive vs. negative trends Stationary series have no trend Example: Increasing technology leading to increase in productivity Seasonal Repeated regular variation the level of data Example: Number of tourists in Mallorca Cyclical Wavelike upward and downward movements around the long-term trend Longer duration than seasonal fluctuations Example: Business cycles Note, this is very often to identify Irregular Random fluctuations Possibly carrying more dynamics than just deterministic ones Hardest to capture in a forecasting model The four components may look similar to this: Figure 4.2: Components in a timeseries 4.2.1 Terminology \\(Y_t\\): Denotes a time series variable \\(\\hat{Y_t}\\): Denotes the foretasted value of \\(Y_t\\) \\(e_t=Y_t-\\hat{Y_t}\\): Denotes the residual or the forecast error. \\(Y_{t-k}\\): Denotes a time series variable lagged by k periods. 4.2.1.1 Autocorrelation Autocorrelation: is the correlation between a time series and its past (lagged) observations. To identify this, one can merely compare the lagged values as a series for itself, hence comparing actual time series against the lagged time series. This can be written as: \\[r_k=\\frac{\\sum_{t=k+1}^n\\left(Y_{t\\ }-\\hat{Y}\\right)}{\\sum_{t=1}^n\\left(Y_t-\\hat{Y}\\right)^{^2}}\\] Where \\(k = 0,1,2,...\\), hence take on numbers, typically whole numbers, as the result must be measurable. We assess autocorrelation to identify if the data have a trend, seasons, cycles or it is random? If we have seasons, trends or cycles, we must make the model account for this, otherwise one is prone to have a model where it is just implicitly correlated, but that is merely due to the autocorrelation, as it says in the word, it is automatically correlated, but that also implies, that it is not necessarily caused by the data, but rather other factors, often we see macro factors, that have an influence, e.g. an economic book. Autocorrelation can be plotted using an autocorrelation function (ACF) or merely by using a correlogram, which is a k-period plot of the autocorelation, that looks like the following: Figure 4.3: Correlogram Example Where one wants to be within the upper and lower level. Manually testing for autocorrelation One must: Calculate \\(r_k\\) Calculate \\(SE(r_k)\\) Hypothesis: \\(H0 : \\rho=0\\), \\(H0 : \\rho≠0\\) We apply t-test Where: \\[SE\\left(r_k\\right)=\\sqrt{\\left\\{\\frac{1+2\\sum_{i=1}^{k-1}r_i^2}{n}\\right\\}}\\] Although, with normal approximation \\[SE\\left(r_k\\right)=\\frac{1}{\\sqrt{n-k}}\\] and test statistic equal \\[t=\\frac{r_k}{SE(r_k)}\\] Thence one merely must look up the cut off values and assess if there is statistical evidance for autocorrelation or not. Alternative: Ljung-Box Q statistic The Ljung Box Q is to identify if at least one of the components explains the Y. Thence H0 = p1 = p2 = p3 = pm, thence we want to reject this one. If not, then none of the predictors explain the Y, thus they are irregular components. \\[Q\\ =\\ n\\left(n+2\\right)\\sum_{k=1}^m\\frac{r_k^2}{n-k}\\] Where m is the number of lags to be tested. The Q statistic is commonly used for testing correlation in the residuals of a forecast model and the comparison is mate to \\(X^2_{m-q}\\), where q is the number of parameters in the model. 4.2.1.2 Random vs. correlated data Randomness is important for forecast model residuals. One can write simple random model, but we dont want complete randomness. Hence we don’t want patterns in our error, where the previous error can explain the next error. E.g. if the data contain trend or seasons, that we have not accounted for, then the errors will be able to predict the coming errors (can be tested by testing errors (residuals) against the lagged errors (residuals)). \\[Y_t=c+\\epsilon_t\\] Where c is the component and \\(\\epsilon_t\\) is the random error component. That is assumed to be uncorrelated period to period. 4.2.1.3 Stationary vs. non stationary data Stationary series is not trending, where is non stationary series is trending, can both be linear or exponential. The how is it solved? One can merely apply differencing of order k. That is equal to: \\[\\Delta Y_t=Y_t-Y_{t-1}\\] One could also apply growth rates are log differencing instead. "],["DataPatternsAndModelSelection.html", "4.3 Data Patterns and Model Selection", " 4.3 Data Patterns and Model Selection Here are some examples from the lectures Tend, no cycle, no seasonality Holt’s exponential Smoothing Linear regression with trend Trend, seasonality, cycle Winters’ exponential smoothing Linear regression with trend and seasonal adjustments Causal regression Time-series decomposition Non linear trend, no seasonality, no cycle Non linear regression with trend Causal regression Holt’s exponential smoothing Learn more about the methods in section 5.1, where a collection of performance measures can be found in section 5.2 "],["exercises.html", "4.4 Exercises", " 4.4 Exercises This section contain exercises, hence the methods applied on data 4.4.1 p. 92 HW Problem 8 4.4.1.1 Moving Averages df &lt;- read_excel(&quot;Data/Week45/prob8p92HW.xlsx&quot;) yt &lt;- ts(df) #Rename and define as time series ts.plot(yt) #We can plot the We see that there is a trend in the data. We can calculate the five period moving average by: yt5c &lt;- ma(yt #The time series ,order = 5 #Amount of periods to be evaluated ,centre = TRUE #We want the center value of the MA ) yt5c ## Time Series: ## Start = 1 ## End = 8 ## Frequency = 1 ## [,1] ## [1,] NA ## [2,] NA ## [3,] 212.0 ## [4,] 216.0 ## [5,] 219.0 ## [6,] 221.2 ## [7,] NA ## [8,] NA Hence we are able to produce moving averages based on the data. Notice, that the most recent MA is the prediction, hence being \\(\\hat{Y}_{t+1}\\). One could extend this, by adding this value to the time series and then calculate MA for the period hereafter. We see that the output of the table above is somewhat misleading, as the most recent MA predictinos, are not positioned in the end, but instead where the center actually is. This problem is solvable using filter(). See the following chunk k &lt;- 5 #specify the order of the moving average c &lt;- rep (1/k,k) #remember that simple average is a weighted average with equal weights, #you need to specify weights for the filter command to work yt5&lt;- filter(yt, c, sides = 1) ts.plot(yt5)# &quot;Plotting the MA&#39;s Figure 4.4: 5k Moving Average yt5 #The updated vector of MA&#39;s ## Time Series: ## Start = 1 ## End = 8 ## Frequency = 1 ## [,1] ## [1,] NA ## [2,] NA ## [3,] NA ## [4,] NA ## [5,] 212.0 ## [6,] 216.0 ## [7,] 219.0 ## [8,] 221.2 This we see, that scores are moved to the end, so even though it is the center of the MA, it is now presented as recent values. For simple moving averages, one may do it in excel, it may be easier and quicker. 4.4.1.2 Exponential moving averages, normal, Holts and Winters Simple and Holt-Winters exponential smoothing Where; alpha is the smoothing parameter, beta tells you if you should account for a trend or not, gamma is responsible for the presence of a seasonal component in the model fit &lt;- HoltWinters(yt ,alpha = 0.4 ,beta = FALSE ,gamma = FALSE) plot(fit,xlim = c(1,nrow(df))) + grid(col = &quot;lightgrey&quot;) ## integer(0) legend(&quot;topleft&quot;,c(&quot;Observed&quot;,&quot;Fitted&quot;),lty = 1,col = c(1:2)) Hence we see the smoothed values, where the higher alpha, the more will the fitted line track the changes in the observations. We can now plot the forecast values: plot(forecast(fit),xlim = c(1,nrow(df)+10))# + grid(col = &quot;lightgrey&quot;) legend(&quot;topleft&quot;,c(&quot;Observed&quot;,&quot;Forecast&quot;),lty = 1,col = c(&quot;Black&quot;,&quot;Blue&quot;)) Figure 4.5: Forecast Exponential Smoothing One see the confidence intervals of the forecast widening as we get further away from the actual values. Now one may assess the accuracy: accuracy(forecast(fit)) ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 8.001509 8.209102 8.001509 3.673623 3.673623 2.154252 0.1962194 rm(list = ls()) "],["Methods.html", "5 Methods and Performance Measurement ", " 5 Methods and Performance Measurement "],["ForecastMethods.html", "5.1 Forecasting Methods", " 5.1 Forecasting Methods Naive forecasts This is merely the current period is assumed to be the best predictor for the future, hence it can be written as: \\[\\begin{equation} \\hat{Y}_{t+1}=Y_t \\tag{5.1} \\end{equation}\\] where, \\(Y_t\\) = the last period, hence \\(\\hat{Y}_{t+1}\\) = the following period. Therefore, the error can merely be written as: \\(e_t=Y_{t+1}-\\hat{Y}_{t+1}\\), being the actual amount compared with the foretasted value. One can make several iterations to account for trending, the growth rate, or seasonal data. Those being: \\(\\hat{Y}_{t+1}=Y_t+(Y_t-Y_{t-1})\\), to account for trending data (non stationary data) \\(\\hat{Y}_{t+1}=Y_t * \\frac{Y_t}{Y_{t-1}}\\), to account for the growth rate, notice that it only assess the growth rate to the prior period. \\(\\hat{Y}_{t+1}=Y_{t-3}+\\frac{Y_t-Y_{t-4}}{4}\\), to account for quarterly trending data, the periods can naturally be changed by changing the formula, e.g. to 12. but notice, that this is just replicating previous periods, hence also previous seasons 5.1.1 Using Averages We have the following: Simple averages, which merely takes the average of all observations. Moving averages, which account for the given time frame. This can be extended by, Double moving averages, often seen when you need the center value of a period consisting of an even number of periods, where there is no actual median value, thus one can extend the MA with a double MA. 5.1.1.1 Simple Averages One may assume that it is sufficient to apply the average of all observations, to predict the next period, hence we can say: \\[\\begin{equation} \\hat{Y}_{t+1}=\\frac{1}{n}\\sum^t_{i=1}Y_i \\tag{5.2} \\end{equation}\\] This is appropriate if the data has shown historical stability, thus without seasons, trends and etc. 5.1.1.2 Moving Average (MA) One may apply a moving average instead, accounting for k periods, also one could extend this by adding weights. For practical purposes only a k period MA is show: \\[\\begin{equation} \\hat{Y}_{t+1}=\\frac{Y_t+Y_{t-1}+...+Y_{t-k}}{k} \\tag{5.3} \\end{equation}\\] this may be applied to remove seasonal effect, either by k=4 or 12 if the data is respectively quarterly or monthly 5.1.1.3 Double Moving Average This is simply doing moving averages twice, hence it is an extension of equation (5.3) As mentioned, often seen when one wants the median when using an even number of periods, e.g. 12 months, hence double MA can be applied. \\[\\begin{equation} M_t=\\hat{Y}_{t+1}=\\frac{Y_t+Y_{t-1}+...+Y_{t-k+1}}{k} \\tag{5.4} \\end{equation}\\] \\[\\begin{equation} M&#39;_t=\\frac{M_t+M_{t-1}+...+M_{t-k+1}}{k} \\tag{5.5} \\end{equation}\\] \\[\\begin{equation} a_t=M_t+\\left(M_t-M_t\\right) \\end{equation}\\] \\[\\begin{equation} b_t=\\frac{2}{k-1}\\left(M_t-M&#39;_t\\right) \\end{equation}\\] Thence we are able to say: \\[\\begin{equation} \\hat{Y}_{t+p}=a_t+b_t*p \\tag{5.6} \\end{equation}\\] 5.1.2 Linear regressions Linear regression with a trend: that is normal linear regression, where the trend is added as a counter, which will account for the trend, given it is linear. 5.1.3 Non linear regressions Non linear regression with trend Causal regression 5.1.4 Smoothing methods 5.1.4.1 Exponential smoothing This is exponentially weighted moving average of all historical values, meaning that the most recent value will be assigned the most weight. Hence we merely add different weights to past periods, thus there is no specific way to adjust for trend and seasonality, which is a limitation of exponential smoothing, it can be written as: \\[\\begin{equation} \\hat{Y}_{t+1}=\\alpha Y_t+\\left(1-\\alpha\\right)\\hat{Y}_t \\end{equation}\\] thus: \\[\\begin{equation} =\\hat{Y}_t+\\alpha(Y_t-\\hat{Y}_t) \\end{equation}\\] Where \\(\\alpha\\) = the smoothing constant, thus is can be between 0 and 1. The higher alpha the largest weight to the most recent observation. Then how to choose the smoothing parameter \\(\\alpha\\)? For stable predictions, choose a high alpha For sensitive predictions, choose low alpha Test different alpha values and compare the models using the performance measures in section 5.2. 5.1.4.2 Holt’s exponential smoothing, Put words to this 5.1.4.3 Winters’ exponential smoothing, Put words to this 5.1.4.4 Moving Averages, see section 5.1.1.2 5.1.5 ARIMA Decomposition of the time series AR: MA: ARMA: ARIMA: "],["PerformanceMeasurements.html", "5.2 Performance Measurements", " 5.2 Performance Measurements Mean error (ME): \\[\\begin{equation} ME=\\frac{1}{n}\\sum_{ }^{ }\\left(Y_t-\\hat{Y}_t\\right)$ \\tag{5.7} \\end{equation}\\] Mean Absolute Deviation (error): \\[\\begin{equation} MAD\\left(i.e.\\ MAE\\right)\\ =\\ \\frac{1}{n}\\cdot\\sum_{ }^{ }\\left|Y_t-\\hat{Y}_t\\right| \\tag{5.8} \\end{equation}\\] Mean Percentage Error (MPE): \\[\\begin{equation} MPE\\ =\\ \\frac{1}{n}\\ \\sum_{ }^{ }\\frac{\\left(Y_t-\\hat{Y}_t\\right)}{Y_t} \\tag{5.9} \\end{equation}\\] Mean Absolute Percentage Error (MAPE): \\[\\begin{equation} MAPE\\ =\\ \\frac{1}{n}\\ \\sum_{ }^{ }\\frac{|\\left(Y_t-\\hat{Y}_t\\right)|}{|Y_t|} \\tag{5.10} \\end{equation}\\] Mean-Squared Error (MSE): \\[\\begin{equation} MSE=\\frac{1}{n}\\sum_{ }^{ }(Y_t-\\hat{Y}_t)^2 \\tag{5.11} \\end{equation}\\] Root Mean-Squared Error: \\[\\begin{equation} RMSE=\\sqrt{MSE} \\tag{5.11} \\end{equation}\\] "]]

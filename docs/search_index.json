[["index.html", "Business Forecasting setup", " Business Forecasting Jonas Ringive Korsholm setup All the output formatting is done in the _output.yml file while all bookdown rendering options is done in the _bookdown.yml file.§ library(bookdown) Tips and tricks Referencing This is making a figure with a caption and where we center the figure and set the size. Notice, that the figure is automatically numbered according to the chapter number. plot(pressure,type = &#39;b&#39;,pch = 19) Figure 0.1: A fig Now we are able to make a reference to the chunk. Refer to a chunk: 0.1 Options RMarkdown Reference Guide "],["introduction.html", "1 Introduction", " 1 Introduction "],["curriculum.html", "2 Curriculum", " 2 Curriculum Description of qualifications (Expectations from you): This course will provide you with the ability to: Understand and argue why forecasting is important and discuss different approaches/strategies/principles implemented in the business world. Explain the key difference between qualitative and quantitative methods in forecasting. Reason and argue for which model to use in the face of real-world business situations. Carry out static and dynamic forecasting based on linear regressions and time-series methodology. Evaluate the accuracy of forecast outcomes. Contents: This course is designed to give a solid theoretical and applied background to graduate students in forecasting. Students are expected to have taken Quantitative Research Methods, or an equivalent course that covers regression analysis with a good understanding of the statistical methodology used. The course will not only be a methodology course but equally an applied course in that students will develop skills to approach business life situations critically, evaluate and communicate their findings with ease. The applications that immediately follow the theoretical topics to be taught will cover different business topics including quality control, product demand analysis, marketing and advertising. Tentative Outline of Course Topics: Qualitative Forecasting Methods Quantitative and qualitative forecasting New product forecasting + Executive opinions Sales forces opinions Consumer surveys Delphi method Forecast process, data considerations and model selection Trend, seasonal, cycle and irregular components + Statistical review Correlograms Moving average and exponential smoothing methods Moving average Holt’s and Holt-Winter’s exponential smoothing models + Product demand forecasting Forecasting with static regression methods Bivariate regression model review Forecasting with simple linear trend Serial correlation and heteroskedasticity Time-series decomposition Basic time series decomposition models + Deseasonalizing and seasonal indices Time-series decomposition forecast Applications ARIMA processes and Box-Jenkins methodology Moving average models + Autoregressive models Mixed autoregressive and moving average models Model selection, Box-Jenkins identification process, estimation of ARIMA processes Forecasting seasonal time series Applications Dynamic forecasting of economic and financial time series Nonstationary time series + Cointegration Spurious regression Combining forecast results Some basic theorems on diversification of forecasts + Nelson combination method Granger-Ramanathan combination method Combinations with time-varying weights Applications Forecast evaluation Measures of forecasting accuracy Diebold-Mariano test for significant differences in forecasting accuracies "],["qualitative-forecasting-methods.html", "3 Qualitative Forecasting Methods", " 3 Qualitative Forecasting Methods – HW: Introduction to forecasting – HW: Judgmental Forecasting and Forecast Adjustments – HW: A Review of Basic Statistical Concepts – Armstrong et al. (2000), “Sales Forecasts for Existing Consumer Products and Services: Do Purchase Intentions Contribute to Accuracy?” International Journal of Forecasting, 16(3), 383-397. – Lawrence et al. (2006), “Judgmental forecasting: A review of progress over the last 25 years,” International Journal of Forecasting, 22, 493-518. – Lynn et al. (1999), “Survey of New Product Forecasting Practices in Industrial High Technology and Low Technology Businesses,” Industrial Marketing Manage- ment, 28, 565-571. – Bass, Frank M. (1969). “A new product growth model for consumer durables,” Management Science, 15, 215-227. "],["judgmental-forecasting-and-forecast-adjustments.html", "3.1 Judgmental Forecasting and Forecast Adjustments", " 3.1 Judgmental Forecasting and Forecast Adjustments A forecast on its own does not include the analysts judgements well enough. To include experts insights, one can apply different methods for this, that is called judgmental forecasting. There er different methods for including experts input on the analysis. See below: The Delphi Method: That is an iterative process where experts makes analysis an predictions independently, then the findings are distributed and then they are able to independently make corrections to their output if they feel to. The results are then distributed and they are possible to correct their output The feedback loop ends when they have reached redundancy and they make no more corrections. Thus, the output is different expert views, that you can select between. Scenario writing: you have different experts that write scenarios that are likely to happen in the future, you rank those events after likelyhood. The scenario writing is followed by discussion. Thus they are able to defend and modify predictions. Combining forecasts: This is where you make different predicte analysis perhaps not including the same predictors, then you use the outputs to compute a forecast, that is done either by finding the mean of the forecasts or assigning weights to the different forecasts that now act as predictors. Forecasting and Neural Networks: NN is used to find its own variables, it is particularly good to fill in missing values. Other tools to judgmental decision making: The decision making tree, where you assign costs and probabilities to events. Bayesian statistics, also where you create a tree of probabilities. "],["articles.html", "3.2 Articles", " 3.2 Articles 3.2.1 A new product growth model for consumer durables,” Management Science The Bass Model i.e. the Bass Curve is the model for sales of new products, or more correct a generalization of the sales for a product, hence he created the model for adopters, where it has later been separated into innovators, early adopters, early majority, etc. that is then the product life cycle. Thus the purpose of the Bass Model is to predict sales of a new product, based on questionnaires or early sales The Bass Curve is based on the early adopters and calculates the curve based on their numbers. The model has proven that it only needs very few observations to plot when it is going to peak and at what amount. Basically the model is based on a conditional likelihood with respect to time. In fact, it has shown to predict peak and sales amount based on research and questionnaires prior to sales Then there are some notes on what further research that can be done and some notes on extensions to the Bass Model 3.2.2 Sales Forecasts for Existing Consumer Products and Services: Do Purchase Intentions Contribute to Accuracy? As we know from the Bass Model we can use consumer intentions (questionnaires about whether a person think he will buy it or not) to guestimate the total sales and when it is going to peak, but are we are able to optimize accuracy of forecasts on existing products using consumer intentions? A previous study has shown, that intentions does not improve the model, although the data did not have available the intentions and whether the person made a purchase or not. In this study there is an improvement to the forecast when the historical data is combined with the intentions of the persons. Also they ruled out, the factor that if the data does not have historical data on intentions and sales history. Hence they experienced on their seven data set, that combining historical data and intentions improved the forecast. To be noted is that the manipulated the intentions in different ways to experiment, e.g. moving average, binary (intentions or not) and some other faucets. 3.2.3 Judgmental forecasting A review of progress over the last 25 years, International Journal of Forecasting This is basically just more information on the judgemental forecasting where they tested different set ups, such as groups, bootsstrapping etc. Did not read all of this, it seemed very much like HW9 "],["exploring-patterns-and-forecasting-techniques.html", "4 Exploring Patterns and Forecasting Techniques", " 4 Exploring Patterns and Forecasting Techniques This chapter will elaborate on how one identify patterns in data thus how to account for this. Thus, we are looking into smoothing methods and moving averages Additionally, we will explore how we select forecasting methods Literature: HW: Exploring Data Patterns and an Introduction to Forecasting Techniques HW: Moving Averages and Smoothing Methods Armstrong, J.S. (2001) “Selecting Forecast Methods,” In Principles of Forecasting: A Handbook for Researchers and Practitioners (Ed. J. Scott Armstrong), Kluwer "],["the-forecasting-process.html", "4.1 The forecasting process", " 4.1 The forecasting process The following describe the forecasting process, hence what one must consider before performing the forecast and ultimately using the forecasts, the purpose of the process is to make sure that the forecast is reliable Specify objectives Reason for the forecast Applications based on the forecast Good communication between all those involved Determine what to forecast Based on set objectives, choose key indicator(s) Example: domestic sales, export sales, or even both? Identify time dimension Length and periodicity of the forecast Desired frequency Urgency of the forecast Planning of the forecast Data considerations Available and quantity of the data Internal vs. external data Desired frequency in data (annual, quarterly, monthly) Example: Dollar sales instead of unit sales Model selection The pattern exhibited by the data The quantity of historic data available The length of the forecast horizon Figure 4.1: Model Selection Model evaluation Testing the models on the series to be forecast Checking how each model works ‘in sample’ Measures such as MSE, RMSE, etc. used to rank models Fit (in sample) vs. accuracy (out of sample) Forecast preparation Based on the selected model, obtain the forecast Keep possibly competing models See if their combination yields mode accuracy Presentation of forecast result Clear communication Keep it as simple as possible Visual aids to support the findings Tracking results Comparison of forecasts to actual values Re-specify the selected model(s) over time if necessary Try other model combinations to keep the accuracy level intact Conclusion: One should realize that it is an iterative process, that one must be aware of. "],["data-patterns-and-terminology.html", "4.2 Data Patterns and terminology", " 4.2 Data Patterns and terminology Basically the data is assumed to consist of up to four components, that is: Trend Long-term change in the level of data Positive vs. negative trends Stationary series have no trend Example: Increasing technology leading to increase in productivity Seasonal Repeated regular variation the level of data Example: Number of tourists in Mallorca Cyclical Wavelike upward and downward movements around the long-term trend Longer duration than seasonal fluctuations Example: Business cycles Note, this is very often to identify Irregular Random fluctuations Possibly carrying more dynamics than just deterministic ones Hardest to capture in a forecasting model The four components may look similar to this: Figure 4.2: Components in a timeseries 4.2.1 Terminology \\(Y_t\\): Denotes a time series variable \\(\\hat{Y_t}\\): Denotes the foretasted value of \\(Y_t\\) \\(e_t=Y_t-\\hat{Y_t}\\): Denotes the residual or the forecast error. \\(Y_{t-k}\\): Denotes a time series variable lagged by k periods. 4.2.1.1 Autocorrelation Autocorrelation: is the correlation between a time series and its past (lagged) observations. To identify this, one can merely compare the lagged values as a series for itself, hence comparing actual time series against the lagged time series. This can be written as: \\[r_k=\\frac{\\sum_{t=k+1}^n\\left(Y_{t\\ }-\\hat{Y}\\right)}{\\sum_{t=1}^n\\left(Y_t-\\hat{Y}\\right)^{^2}}\\] Where \\(k = 0,1,2,...\\), hence take on numbers, typically whole numbers, as the result must be measurable. We assess autocorrelation to identify if the data have a trend, seasons, cycles or it is random? If we have seasons, trends or cycles, we must make the model account for this, otherwise one is prone to have a model where it is just implicitly correlated, but that is merely due to the autocorrelation, as it says in the word, it is automatically correlated, but that also implies, that it is not necessarily caused by the data, but rather other factors, often we see macro factors, that have an influence, e.g. an economic book. Autocorrelation can be plotted using an autocorrelation function (ACF) or merely by using a correlogram, which is a k-period plot of the autocorelation, that looks like the following: Figure 4.3: Correlogram Example Where one wants to be within the upper and lower level. Manually testing for autocorrelation One must: Calculate \\(r_k\\) Calculate \\(SE(r_k)\\) Hypothesis: \\(H0 : \\rho=0\\), \\(H0 : \\rho≠0\\) We apply t-test Where: \\[SE\\left(r_k\\right)=\\sqrt{\\left\\{\\frac{1+2\\sum_{i=1}^{k-1}r_i^2}{n}\\right\\}}\\] Although, with normal approximation \\[SE\\left(r_k\\right)=\\frac{1}{\\sqrt{n-k}}\\] and test statistic equal \\[t=\\frac{r_k}{SE(r_k)}\\] Thence one merely must look up the cut off values and assess if there is statistical evidance for autocorrelation or not. Alternative: Ljung-Box Q statistic The Ljung Box Q is to identify if at least one of the components explains the Y. Thence H0 = p1 = p2 = p3 = pm, thence we want to reject this one. If not, then none of the predictors explain the Y, thus they are irregular components. \\[Q\\ =\\ n\\left(n+2\\right)\\sum_{k=1}^m\\frac{r_k^2}{n-k}\\] Where m is the number of lags to be tested. The Q statistic is commonly used for testing correlation in the residuals of a forecast model and the comparison is mate to \\(X^2_{m-q}\\), where q is the number of parameters in the model. 4.2.1.2 Random vs. correlated data Randomness is important for forecast model residuals. One can write simple random model, but we dont want complete randomness. Hence we don’t want patterns in our error, where the previous error can explain the next error. E.g. if the data contain trend or seasons, that we have not accounted for, then the errors will be able to predict the coming errors (can be tested by testing errors (residuals) against the lagged errors (residuals)). \\[Y_t=c+\\epsilon_t\\] Where c is the component and \\(\\epsilon_t\\) is the random error component. That is assumed to be uncorrelated period to period. 4.2.1.3 Stationary vs. non stationary data Stationary series is not trending, where is non stationary series is trending, can both be linear or exponential. The how is it solved? One can merely apply differencing of order k. That is equal to: \\[\\Delta Y_t=Y_t-Y_{t-1}\\] One could also apply growth rates are log differencing instead. "],["DataPatternsAndModelSelection.html", "4.3 Data Patterns and Model Selection", " 4.3 Data Patterns and Model Selection Here are some examples from the lectures Tend, no cycle, no seasonality Holt’s exponential Smoothing Linear regression with trend Trend, seasonality, cycle Winters’ exponential smoothing Linear regression with trend and seasonal adjustments Causal regression Time-series decomposition Non linear trend, no seasonality, no cycle Non linear regression with trend Causal regression Holt’s exponential smoothing Learn more about the methods in section 6.1, where a collection of performance measures can be found in section 6.2 "],["exercises.html", "4.4 Exercises", " 4.4 Exercises This section contain exercises, hence the methods applied on data 4.4.1 p. 92 HW Problem 8 4.4.1.1 Moving Averages df &lt;- read_excel(&quot;Data/Week45/prob8p92HW.xlsx&quot;) yt &lt;- ts(df) #Rename and define as time series ts.plot(yt) #We can plot the We see that there is a trend in the data. We can calculate the five period moving average by: yt5c &lt;- ma(yt #The time series ,order = 5 #Amount of periods to be evaluated ,centre = TRUE #We want the center value of the MA ) yt5c ## Time Series: ## Start = 1 ## End = 8 ## Frequency = 1 ## [,1] ## [1,] NA ## [2,] NA ## [3,] 212.0 ## [4,] 216.0 ## [5,] 219.0 ## [6,] 221.2 ## [7,] NA ## [8,] NA Hence we are able to produce moving averages based on the data. Notice, that the most recent MA is the prediction, hence being \\(\\hat{Y}_{t+1}\\). One could extend this, by adding this value to the time series and then calculate MA for the period hereafter. We see that the output of the table above is somewhat misleading, as the most recent MA predictinos, are not positioned in the end, but instead where the center actually is. This problem is solvable using filter(). See the following chunk k &lt;- 5 #specify the order of the moving average c &lt;- rep (1/k,k) #remember that simple average is a weighted average with equal weights, #you need to specify weights for the filter command to work yt5&lt;- filter(yt, c, sides = 1) ts.plot(yt5)# &quot;Plotting the MA&#39;s Figure 4.4: 5k Moving Average yt5 #The updated vector of MA&#39;s ## Time Series: ## Start = 1 ## End = 8 ## Frequency = 1 ## [,1] ## [1,] NA ## [2,] NA ## [3,] NA ## [4,] NA ## [5,] 212.0 ## [6,] 216.0 ## [7,] 219.0 ## [8,] 221.2 This we see, that scores are moved to the end, so even though it is the center of the MA, it is now presented as recent values. For simple moving averages, one may do it in excel, it may be easier and quicker. 4.4.1.2 Exponential moving averages + Holts and Winters Simple exponential smoothing Where; alpha is the smoothing parameter, beta tells you if you should account for a trend or not, gamma is responsible for the presence of a seasonal component in the model fit &lt;- HoltWinters(yt ,alpha = 0.4 ,beta = FALSE ,gamma = FALSE) plot(fit,xlim = c(1,nrow(df))) + grid(col = &quot;lightgrey&quot;) ## integer(0) legend(&quot;topleft&quot;,c(&quot;Observed&quot;,&quot;Fitted&quot;),lty = 1,col = c(1:2)) Figure 4.5: Exponential Smoothing Hence we see the smoothed values, where the higher alpha, the more will the fitted line track the changes in the observations. We can now plot the forecast values: plot(forecast(fit),xlim = c(1,nrow(df)+10))# + grid(col = &quot;lightgrey&quot;) legend(&quot;topleft&quot;,c(&quot;Observed&quot;,&quot;Forecast&quot;),lty = 1,col = c(&quot;Black&quot;,&quot;Blue&quot;)) Figure 4.6: Forecast Exponential Smoothing One see the confidence intervals of the forecast widening as we get further away from the actual values. Now one may assess the accuracy: accuracy(forecast(fit)) ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 8.001509 8.209102 8.001509 3.673623 3.673623 2.154252 0.1962194 One see an RMSE of 8.2. Hence one could compare it with an exponential smoothing, which is more sensitive to the observations. fit0.6 &lt;- HoltWinters(yt ,alpha = 0.6 #Changed ,beta = FALSE ,gamma = FALSE) accuracy(forecast(fit0.6)) ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 5.860023 6.373698 5.860023 2.700235 2.700235 1.577698 0.264614 Where we see an RMSE of 6.37, hence lower than the initial test. Holt’s exponential smoothing exponential smoothing when a trend component is present: beta = TRUE {fit3 &lt;- HoltWinters(yt ,alpha = 0.6 ,beta = TRUE ,gamma = FALSE) plot(forecast(fit3)) accuracy(forecast(fit3))} Figure 4.7: Holt’s Exponential Smoothing ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set -1.990133 4.369207 3.451467 -0.9324372 1.581919 0.929241 0.4352197 We see that the RMSE is even lower (4.37). Which is expected, as Holt’s exponential smoothing accounts for trend. Winter’s Exponential Smoothing Which accounts for trend and seasonality in order to make it work one needs to define the frequency of your seasonal component, when specifying the ts data {yt &lt;- ts(df ,frequency = 4) # let&#39;s assume we suspect a quarterly pattern fit4 &lt;- HoltWinters(yt, alpha=0.6, beta=TRUE, gamma=TRUE) plot(forecast(fit4)) accuracy(forecast(fit4))} # experiment with seasonality frequency to see if you can get any lower in MSE ## ME RMSE MAE MPE MAPE MASE ## Training set -0.310075 3.973148 3.34445 -0.1227802 1.50834 0.2730163 ## ACF1 ## Training set 0.009251985 We see that the RMSE is even lower (3.97), hence Winters Exponential Smoothing appear to be the best model for prediction. 4.4.2 p. 93 HW Problem 9-10 Basically just another example of the exercise above. 4.4.3 CO2 and Sales Data Not done again. Do if time allows. 4.4.4 Case 6 oo. 108-111 HW Not done again. Do if time allows. "],["simple-and-multiple-linear-regression.html", "5 Simple and Multiple Linear Regression", " 5 Simple and Multiple Linear Regression Lectures 5th lecture - simple linear regression 6th lecture - multiple linear regression Literature HW: Simple Linear Regression "],["simple-linear-regression.html", "5.1 Simple Linear Regression", " 5.1 Simple Linear Regression I will not go much in details with what simple linaer regression is. One can calculate the beta values by the following \\[\\begin{equation} b_0=\\ \\overline{Y}-\\ b_1\\overline{X} \\tag{5.1} \\end{equation}\\] \\[\\begin{equation} b_1=\\frac{\\sum_{ }^{ }\\left(X-\\overline{X}\\right)\\left(Y-\\overline{Y}\\right)}{\\sum_{ }^{ }\\left(X-\\overline{X}\\right)^{^2}} \\tag{5.2} \\end{equation}\\] Where point forecast, hence \\(\\hat{Y}\\) is merely the sum of the linear equation, hence \\(\\hat{Y}=b_0+b_1X^*\\), where \\(X^*\\) is the specific X values. Thence one can estimate the standard error by: \\[\\begin{equation} s_{yx}=\\sqrt{\\frac{\\sum_{ }^{ }\\left(Y-\\overline{Y}\\right)^{^2}}{n-2}} \\tag{5.3} \\end{equation}\\] Equations (5.3) can also be written otherwise, see the slides for that. The residuals can be broken down to the following \\[\\begin{equation} \\sum_{ }^{ }\\left(Y-\\overline{Y}\\right)^{^2}=\\sum_{ }^{ }\\left(\\hat{Y}-\\ \\overline{Y}\\right)^{^2}+\\sum_{ }^{ }\\left(Y-\\ \\hat{Y}\\right)^{^2} \\tag{5.4} \\end{equation}\\] Which consist of the following three elements. \\[\\begin{equation} SST = SSR + SSE \\tag{5.5} \\end{equation}\\] The residuals can then be applied for a goodness of fit assessment, where one can identify R squared- \\[\\begin{equation} R^2=\\frac{SSR}{SST} \\tag{5.5} \\end{equation}\\] So what can the linear regression then be used for? Inference Prediction Notice, that inference can only be done when the model is adequate, hence the assumptions actually being met. 5.1.1 Assumptions We have the following assumptions for a linear model: The underlying relationship between dependent and independent variable is actually linear Independent residuals Homoskedastic residuals (show constant variance) Identically distributed (In general, normal distribution is assumed) Hence how is the assumptions tested? Some can be done before analysis and others after the model is applied, hence it can be described by the following: Before the model is applied: The underlying relationship between dependent and independent variable is actually linear After the model is applied (doing diagnostics): Independent residuals Homoskedastic residuals (show constant variance) Identically distributed (In general, normal distribution is assumed) Now lets dive into the data Serial correlation and Heteroskedasticity Notice that autocorrelation = serial correlation Serial correlation is where the observations are trailing each other, where heteroskedasticity is where the variance is changing over timer: 5.1.1.1 Serial correlation (checking for independent ): Figure 5.1: Serial Correlation Example Notice, that one should also test for autocorrelation in the errors, that can be done with a Durbin-Watson statistic: \\[\\begin{equation} DW\\ =\\frac{\\sum_{ }^{ }\\left(e_t-e_{t-1}\\right)^{^2}}{\\sum_{ }^{ }e_t^{^2}} \\tag{5.6} \\end{equation}\\] 0 &lt; DW &lt; 4, where if DW = 2 it indicates no serial correlation (this is the ideal), generally if 1.5 &lt; DW &lt; 2.5 is widely used as an acceptable level. If DW &gt; 2, it indicates negative serial correlation and if DW &lt; 2, it indicates that there is positive serial correlation. One could also use correlation testing by checking correlogram of residuals or testing residuals against lagged residuals Heteroskedasticity: Figure 5.2: Heteroskedasticity Example If one observe serial correlation (auto correlation) and heteroskedasticity in the residuals, then the model cannot be used. "],["multiple-linear-regression.html", "5.2 Multiple Linear Regression", " 5.2 Multiple Linear Regression "],["Methods.html", "6 Methods and Performance Measurement ", " 6 Methods and Performance Measurement "],["ForecastMethods.html", "6.1 Forecasting Methods", " 6.1 Forecasting Methods Naive forecasts This is merely the current period is assumed to be the best predictor for the future, hence it can be written as: \\[\\begin{equation} \\hat{Y}_{t+1}=Y_t \\tag{6.1} \\end{equation}\\] where, \\(Y_t\\) = the last period, hence \\(\\hat{Y}_{t+1}\\) = the following period. Therefore, the error can merely be written as: \\(e_t=Y_{t+1}-\\hat{Y}_{t+1}\\), being the actual amount compared with the foretasted value. One can make several iterations to account for trending, the growth rate, or seasonal data. Those being: \\(\\hat{Y}_{t+1}=Y_t+(Y_t-Y_{t-1})\\), to account for trending data (non stationary data) \\(\\hat{Y}_{t+1}=Y_t * \\frac{Y_t}{Y_{t-1}}\\), to account for the growth rate, notice that it only assess the growth rate to the prior period. \\(\\hat{Y}_{t+1}=Y_{t-3}+\\frac{Y_t-Y_{t-4}}{4}\\), to account for quarterly trending data, the periods can naturally be changed by changing the formula, e.g. to 12. but notice, that this is just replicating previous periods, hence also previous seasons 6.1.1 Using Averages We have the following: Simple averages, which merely takes the average of all observations. Moving averages, which account for the given time frame. This can be extended by, Double moving averages, often seen when you need the center value of a period consisting of an even number of periods, where there is no actual median value, thus one can extend the MA with a double MA. 6.1.1.1 Simple Averages One may assume that it is sufficient to apply the average of all observations, to predict the next period, hence we can say: \\[\\begin{equation} \\hat{Y}_{t+1}=\\frac{1}{n}\\sum^t_{i=1}Y_i \\tag{6.2} \\end{equation}\\] This is appropriate if the data has shown historical stability, thus without seasons, trends and etc. 6.1.1.2 Moving Average (MA) One may apply a moving average instead, accounting for k periods, also one could extend this by adding weights. For practical purposes only a k period MA is show: \\[\\begin{equation} \\hat{Y}_{t+1}=\\frac{Y_t+Y_{t-1}+...+Y_{t-k}}{k} \\tag{6.3} \\end{equation}\\] this may be applied to remove seasonal effect, either by k=4 or 12 if the data is respectively quarterly or monthly 6.1.1.3 Double Moving Average This is simply doing moving averages twice, hence it is an extension of equation (6.3) As mentioned, often seen when one wants the median when using an even number of periods, e.g. 12 months, hence double MA can be applied. \\[\\begin{equation} M_t=\\hat{Y}_{t+1}=\\frac{Y_t+Y_{t-1}+...+Y_{t-k+1}}{k} \\tag{6.4} \\end{equation}\\] \\[\\begin{equation} M&#39;_t=\\frac{M_t+M_{t-1}+...+M_{t-k+1}}{k} \\tag{6.5} \\end{equation}\\] \\[\\begin{equation} a_t=M_t+\\left(M_t-M_t\\right) \\end{equation}\\] \\[\\begin{equation} b_t=\\frac{2}{k-1}\\left(M_t-M&#39;_t\\right) \\end{equation}\\] Thence we are able to say: \\[\\begin{equation} \\hat{Y}_{t+p}=a_t+b_t*p \\tag{6.6} \\end{equation}\\] 6.1.2 Linear regressions Linear regression with a trend: that is normal linear regression, where the trend is added as a counter, which will account for the trend, given it is linear. 6.1.3 Non linear regressions Non linear regression with trend Causal regression 6.1.4 Smoothing methods 6.1.4.1 Exponential smoothing This is exponentially weighted moving average of all historical values, meaning that the most recent value will be assigned the most weight. Hence we merely add different weights to past periods, thus there is no specific way to adjust for trend and seasonality, which is a limitation of exponential smoothing, it can be written as: \\[\\begin{equation} \\hat{Y}_{t+1}=\\alpha Y_t+\\left(1-\\alpha\\right)\\hat{Y}_t \\end{equation}\\] thus: \\[\\begin{equation} =\\hat{Y}_t+\\alpha(Y_t-\\hat{Y}_t) \\end{equation}\\] Where \\(\\alpha\\) = the smoothing constant, thus is can be between 0 and 1. The higher alpha the largest weight to the most recent observation. Then how to choose the smoothing parameter \\(\\alpha\\)? For stable predictions, choose a high alpha For sensitive predictions, choose low alpha Test different alpha values and compare the models using the performance measures in section 6.2. 6.1.4.2 Holt’s exponential smoothing Exponential smoothing method with adjustment for trend, hence we introduce a new tuning parameter, hence we have \\(\\alpha\\) and \\(\\beta\\) \\(\\alpha\\) = Weight to the most recent observations \\(\\beta\\) = adjustment for trend. R will automatically set this, when beta = TRUE Hence the smoothing now consists of two elements: The level estimate \\[\\begin{equation} L_t=\\alpha Y_t+\\left(1-\\alpha\\right)\\left(L_{t-1}+T_{t-1}\\right) \\tag{6.7} \\end{equation}\\] The trend estimate \\[\\begin{equation} T_t=\\beta\\left(L_t-L_{t-1}\\right)+\\left(1-\\beta\\right)T_{t-1} \\tag{6.8} \\end{equation}\\] Thus, the forecasting of p periods into the future, can be explained by: \\[\\begin{equation} \\hat{Y}_{t+p}=L_t+pT_t \\tag{6.9} \\end{equation}\\] To apply: use HoltWinters() and select parameters that lowers the performance measurements. When one assigns large weights the model will become more sensitive to changes in the observed data. One can either set the initial value to 0 or take the average of the first few observations. If \\(\\alpha = \\beta\\), then we have the Brown’s double exponential smoothing model. If \\(\\beta\\) = 0, then we merely have a simple exponential smoothing. 6.1.4.3 Winters’ exponential smoothing Exponential smoothing method with adjustment for trend and seasonality, hence we introduce two new tuning parameters, hence we have \\(\\alpha\\), \\(\\beta\\) (as in Holt’s) and \\(\\gamma\\) Hence the smoothing now consists of three elements: The level estimate \\[\\begin{equation} L_t=\\alpha\\frac{Y_t}{S_{t-s}}+\\left(1-\\alpha\\right)\\left(L_{t-1}+T_{t-1}\\right) \\tag{6.10} \\end{equation}\\] The trend estimate \\[\\begin{equation} T_t=\\beta\\left(L_t-L_{t-1}\\right)+\\left(1-\\beta\\right)T_{t-1} \\tag{6.11} \\end{equation}\\] The seasonality estimate \\[\\begin{equation} S_t=\\gamma\\frac{Y_t}{L_t}+\\left(1-\\gamma\\right)S_{t-s} \\tag{6.12} \\end{equation}\\] Thus, the forecasting of p periods into the future, can be explained by: \\[\\begin{equation} \\hat{Y}_{t+p}=\\left(L_t+pT_t\\right)S_{t-s+p} \\tag{6.13} \\end{equation}\\] To apply: use HoltWinters() and select parameters that lowers the performance measurements. When one assigns large weights the model will become more sensitive to changes in the observed data. One can either set the initial value to 0 or take the average of the first few observations. if \\(\\beta = \\gamma = 0\\) the model is merely simple exponential smoothing. 6.1.4.4 Moving Averages, see section 6.1.1.2 6.1.5 ARIMA Decomposition of the time series AR: MA: ARMA: ARIMA: "],["PerformanceMeasurements.html", "6.2 Performance Measurements", " 6.2 Performance Measurements Mean error (ME): \\[\\begin{equation} ME=\\frac{1}{n}\\sum_{ }^{ }\\left(Y_t-\\hat{Y}_t\\right)$ \\tag{6.14} \\end{equation}\\] Mean Absolute Deviation (error): \\[\\begin{equation} MAD\\left(i.e.\\ MAE\\right)\\ =\\ \\frac{1}{n}\\cdot\\sum_{ }^{ }\\left|Y_t-\\hat{Y}_t\\right| \\tag{6.15} \\end{equation}\\] Mean Percentage Error (MPE): \\[\\begin{equation} MPE\\ =\\ \\frac{1}{n}\\ \\sum_{ }^{ }\\frac{\\left(Y_t-\\hat{Y}_t\\right)}{Y_t} \\tag{6.16} \\end{equation}\\] Mean Absolute Percentage Error (MAPE): \\[\\begin{equation} MAPE\\ =\\ \\frac{1}{n}\\ \\sum_{ }^{ }\\frac{|\\left(Y_t-\\hat{Y}_t\\right)|}{|Y_t|} \\tag{6.17} \\end{equation}\\] Mean-Squared Error (MSE): \\[\\begin{equation} MSE=\\frac{1}{n}\\sum_{ }^{ }(Y_t-\\hat{Y}_t)^2 \\tag{6.18} \\end{equation}\\] Root Mean-Squared Error: \\[\\begin{equation} RMSE=\\sqrt{MSE} \\tag{6.18} \\end{equation}\\] "]]

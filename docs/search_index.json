[["index.html", "Business Forecasting setup", " Business Forecasting Jonas Ringive Korsholm setup All the output formatting is done in the _output.yml file while all bookdown rendering options is done in the _bookdown.yml file.§ library(bookdown) Tips and tricks Referencing This is making a figure with a caption and where we center the figure and set the size. Notice, that the figure is automatically numbered according to the chapter number. plot(pressure,type = &#39;b&#39;,pch = 19) Figure 0.1: A fig Now we are able to make a reference to the chunk. Refer to a chunk: 0.1 Options RMarkdown Reference Guide "],["introduction.html", "1 Introduction", " 1 Introduction "],["curriculum.html", "2 Curriculum", " 2 Curriculum Description of qualifications (Expectations from you): This course will provide you with the ability to: Understand and argue why forecasting is important and discuss different approaches/strategies/principles implemented in the business world. Explain the key difference between qualitative and quantitative methods in forecasting. Reason and argue for which model to use in the face of real-world business situations. Carry out static and dynamic forecasting based on linear regressions and time-series methodology. Evaluate the accuracy of forecast outcomes. Contents: This course is designed to give a solid theoretical and applied background to graduate students in forecasting. Students are expected to have taken Quantitative Research Methods, or an equivalent course that covers regression analysis with a good understanding of the statistical methodology used. The course will not only be a methodology course but equally an applied course in that students will develop skills to approach business life situations critically, evaluate and communicate their findings with ease. The applications that immediately follow the theoretical topics to be taught will cover different business topics including quality control, product demand analysis, marketing and advertising. Tentative Outline of Course Topics: Qualitative Forecasting Methods Quantitative and qualitative forecasting New product forecasting + Executive opinions Sales forces opinions Consumer surveys Delphi method Forecast process, data considerations and model selection Trend, seasonal, cycle and irregular components + Statistical review Correlograms Moving average and exponential smoothing methods Moving average Holt’s and Holt-Winter’s exponential smoothing models + Product demand forecasting Forecasting with static regression methods Bivariate regression model review Forecasting with simple linear trend Serial correlation and heteroskedasticity Time-series decomposition Basic time series decomposition models + Deseasonalizing and seasonal indices Time-series decomposition forecast Applications ARIMA processes and Box-Jenkins methodology Moving average models + Autoregressive models Mixed autoregressive and moving average models Model selection, Box-Jenkins identification process, estimation of ARIMA processes Forecasting seasonal time series Applications Dynamic forecasting of economic and financial time series Nonstationary time series + Cointegration Spurious regression Combining forecast results Some basic theorems on diversification of forecasts + Nelson combination method Granger-Ramanathan combination method Combinations with time-varying weights Applications Forecast evaluation Measures of forecasting accuracy Diebold-Mariano test for significant differences in forecasting accuracies "],["qualitative-forecasting-methods.html", "3 Qualitative Forecasting Methods", " 3 Qualitative Forecasting Methods – HW: Introduction to forecasting – HW: Judgmental Forecasting and Forecast Adjustments – HW: A Review of Basic Statistical Concepts – Armstrong et al. (2000), “Sales Forecasts for Existing Consumer Products and Services: Do Purchase Intentions Contribute to Accuracy?” International Journal of Forecasting, 16(3), 383-397. – Lawrence et al. (2006), “Judgmental forecasting: A review of progress over the last 25 years,” International Journal of Forecasting, 22, 493-518. – Lynn et al. (1999), “Survey of New Product Forecasting Practices in Industrial High Technology and Low Technology Businesses,” Industrial Marketing Manage- ment, 28, 565-571. – Bass, Frank M. (1969). “A new product growth model for consumer durables,” Management Science, 15, 215-227. "],["judgmental-forecasting-and-forecast-adjustments.html", "3.1 Judgmental Forecasting and Forecast Adjustments", " 3.1 Judgmental Forecasting and Forecast Adjustments A forecast on its own does not include the analysts judgements well enough. To include experts insights, one can apply different methods for this, that is called judgmental forecasting. There er different methods for including experts input on the analysis. See below: The Delphi Method: That is an iterative process where experts makes analysis an predictions independently, then the findings are distributed and then they are able to independently make corrections to their output if they feel to. The results are then distributed and they are possible to correct their output The feedback loop ends when they have reached redundancy and they make no more corrections. Thus, the output is different expert views, that you can select between. Scenario writing: you have different experts that write scenarios that are likely to happen in the future, you rank those events after likelyhood. The scenario writing is followed by discussion. Thus they are able to defend and modify predictions. Combining forecasts: This is where you make different predicte analysis perhaps not including the same predictors, then you use the outputs to compute a forecast, that is done either by finding the mean of the forecasts or assigning weights to the different forecasts that now act as predictors. Forecasting and Neural Networks: NN is used to find its own variables, it is particularly good to fill in missing values. Other tools to judgmental decision making: The decision making tree, where you assign costs and probabilities to events. Bayesian statistics, also where you create a tree of probabilities. "],["articles.html", "3.2 Articles", " 3.2 Articles 3.2.1 A new product growth model for consumer durables,” Management Science The Bass Model i.e. the Bass Curve is the model for sales of new products, or more correct a generalization of the sales for a product, hence he created the model for adopters, where it has later been separated into innovators, early adopters, early majority, etc. that is then the product life cycle. Thus the purpose of the Bass Model is to predict sales of a new product, based on questionnaires or early sales The Bass Curve is based on the early adopters and calculates the curve based on their numbers. The model has proven that it only needs very few observations to plot when it is going to peak and at what amount. Basically the model is based on a conditional likelihood with respect to time. In fact, it has shown to predict peak and sales amount based on research and questionnaires prior to sales Then there are some notes on what further research that can be done and some notes on extensions to the Bass Model 3.2.2 Sales Forecasts for Existing Consumer Products and Services: Do Purchase Intentions Contribute to Accuracy? As we know from the Bass Model we can use consumer intentions (questionnaires about whether a person think he will buy it or not) to guestimate the total sales and when it is going to peak, but are we are able to optimize accuracy of forecasts on existing products using consumer intentions? A previous study has shown, that intentions does not improve the model, although the data did not have available the intentions and whether the person made a purchase or not. In this study there is an improvement to the forecast when the historical data is combined with the intentions of the persons. Also they ruled out, the factor that if the data does not have historical data on intentions and sales history. Hence they experienced on their seven data set, that combining historical data and intentions improved the forecast. To be noted is that the manipulated the intentions in different ways to experiment, e.g. moving average, binary (intentions or not) and some other faucets. 3.2.3 Judgmental forecasting A review of progress over the last 25 years, International Journal of Forecasting This is basically just more information on the judgemental forecasting where they tested different set ups, such as groups, bootsstrapping etc. Did not read all of this, it seemed very much like HW9 "],["exploring-patterns-and-forecasting-techniques.html", "4 Exploring Patterns and Forecasting Techniques", " 4 Exploring Patterns and Forecasting Techniques This chapter will elaborate on how one identify patterns in data thus how to account for this. Thus, we are looking into smoothing methods and moving averages Additionally, we will explore how we select forecasting methods Literature: HW: Exploring Data Patterns and an Introduction to Forecasting Techniques HW: Moving Averages and Smoothing Methods Armstrong, J.S. (2001) “Selecting Forecast Methods,” In Principles of Forecasting: A Handbook for Researchers and Practitioners (Ed. J. Scott Armstrong), Kluwer "],["the-forecasting-process.html", "4.1 The forecasting process", " 4.1 The forecasting process The following describe the forecasting process, hence what one must consider before performing the forecast and ultimately using the forecasts, the purpose of the process is to make sure that the forecast is reliable Specify objectives Reason for the forecast Applications based on the forecast Good communication between all those involved Determine what to forecast Based on set objectives, choose key indicator(s) Example: domestic sales, export sales, or even both? Identify time dimension Length and periodicity of the forecast Desired frequency Urgency of the forecast Planning of the forecast Data considerations Available and quantity of the data Internal vs. external data Desired frequency in data (annual, quarterly, monthly) Example: Dollar sales instead of unit sales Model selection The pattern exhibited by the data The quantity of historic data available The length of the forecast horizon Figure 4.1: Model Selection Model evaluation Testing the models on the series to be forecast Checking how each model works ‘in sample’ Measures such as MSE, RMSE, etc. used to rank models Fit (in sample) vs. accuracy (out of sample) Forecast preparation Based on the selected model, obtain the forecast Keep possibly competing models See if their combination yields mode accuracy Presentation of forecast result Clear communication Keep it as simple as possible Visual aids to support the findings Tracking results Comparison of forecasts to actual values Re-specify the selected model(s) over time if necessary Try other model combinations to keep the accuracy level intact Conclusion: One should realize that it is an iterative process, that one must be aware of. "],["data-patterns-and-terminology.html", "4.2 Data Patterns and terminology", " 4.2 Data Patterns and terminology Basically the data is assumed to consist of up to four components, that is: Trend Long-term change in the level of data Positive vs. negative trends Stationary series have no trend Example: Increasing technology leading to increase in productivity Seasonal Repeated regular variation the level of data Example: Number of tourists in Mallorca Cyclical Wavelike upward and downward movements around the long-term trend Longer duration than seasonal fluctuations Example: Business cycles Note, this is very often to identify Irregular Random fluctuations Possibly carrying more dynamics than just deterministic ones Hardest to capture in a forecasting model The four components may look similar to this: Figure 4.2: Components in a timeseries 4.2.1 Terminology \\(Y_t\\): Denotes a time series variable \\(\\hat{Y_t}\\): Denotes the foretasted value of \\(Y_t\\) \\(e_t=Y_t-\\hat{Y_t}\\): Denotes the residual or the forecast error. \\(Y_{t-k}\\): Denotes a time series variable lagged by k periods. 4.2.1.1 Autocorrelation Autocorrelation: is the correlation between a time series and its past (lagged) observations. To identify this, one can merely compare the lagged values as a series for itself, hence comparing actual time series against the lagged time series. This can be written as: \\[r_k=\\frac{\\sum_{t=k+1}^n\\left(Y_{t\\ }-\\hat{Y}\\right)}{\\sum_{t=1}^n\\left(Y_t-\\hat{Y}\\right)^{^2}}\\] Where \\(k = 0,1,2,...\\), hence take on numbers, typically whole numbers, as the result must be measurable. We assess autocorrelation to identify if the data have a trend, seasons, cycles or it is random? If we have seasons, trends or cycles, we must make the model account for this, otherwise one is prone to have a model where it is just implicitly correlated, but that is merely due to the autocorrelation, as it says in the word, it is automatically correlated, but that also implies, that it is not necessarily caused by the data, but rather other factors, often we see macro factors, that have an influence, e.g. an economic book. Autocorrelation can be plotted using an autocorrelation function (ACF) or merely by using a correlogram, which is a k-period plot of the autocorelation, that looks like the following: Figure 4.3: Correlogram Example Where one wants to be within the upper and lower level. Manually testing for autocorrelation One must: Calculate \\(r_k\\) Calculate \\(SE(r_k)\\) Hypothesis: \\(H0 : \\rho=0\\), \\(H0 : \\rho≠0\\) We apply t-test Where: \\[SE\\left(r_k\\right)=\\sqrt{\\left\\{\\frac{1+2\\sum_{i=1}^{k-1}r_i^2}{n}\\right\\}}\\] Although, with normal approximation \\[SE\\left(r_k\\right)=\\frac{1}{\\sqrt{n-k}}\\] and test statistic equal \\[t=\\frac{r_k}{SE(r_k)}\\] Thence one merely must look up the cut off values and assess if there is statistical evidance for autocorrelation or not. Alternative: Ljung-Box Q statistic The Ljung Box Q is to identify if at least one of the components explains the Y. Thence H0 = p1 = p2 = p3 = pm, thence we want to reject this one. If not, then none of the predictors explain the Y, thus they are irregular components. \\[Q\\ =\\ n\\left(n+2\\right)\\sum_{k=1}^m\\frac{r_k^2}{n-k}\\] Where m is the number of lags to be tested. The Q statistic is commonly used for testing correlation in the residuals of a forecast model and the comparison is mate to \\(X^2_{m-q}\\), where q is the number of parameters in the model. 4.2.1.2 Random vs. correlated data Randomness is important for forecast model residuals. One can write simple random model, but we dont want complete randomness. Hence we don’t want patterns in our error, where the previous error can explain the next error. E.g. if the data contain trend or seasons, that we have not accounted for, then the errors will be able to predict the coming errors (can be tested by testing errors (residuals) against the lagged errors (residuals)). \\[Y_t=c+\\epsilon_t\\] Where c is the component and \\(\\epsilon_t\\) is the random error component. That is assumed to be uncorrelated period to period. 4.2.1.3 Stationary vs. non stationary data Stationary series is not trending, where is non stationary series is trending, can both be linear or exponential. The how is it solved? One can merely apply differencing of order k. That is equal to: \\[\\Delta Y_t=Y_t-Y_{t-1}\\] One could also apply growth rates are log differencing instead. "],["DataPatternsAndModelSelection.html", "4.3 Data Patterns and Model Selection", " 4.3 Data Patterns and Model Selection Here are some examples from the lectures Tend, no cycle, no seasonality Holt’s exponential Smoothing Linear regression with trend Trend, seasonality, cycle Winters’ exponential smoothing Linear regression with trend and seasonal adjustments Causal regression Time-series decomposition Non linear trend, no seasonality, no cycle Non linear regression with trend Causal regression Holt’s exponential smoothing Learn more about the methods in section 9.1, where a collection of performance measures can be found in section 9.2 "],["exercises.html", "4.4 Exercises", " 4.4 Exercises This section contain exercises, hence the methods applied on data 4.4.1 p. 92 HW Problem 8 4.4.1.1 Moving Averages df &lt;- read_excel(&quot;Data/Week45/prob8p92HW.xlsx&quot;) yt &lt;- ts(df) #Rename and define as time series ts.plot(yt) #We can plot the We see that there is a trend in the data. We can calculate the five period moving average by: yt5c &lt;- ma(yt #The time series ,order = 5 #Amount of periods to be evaluated ,centre = TRUE #We want the center value of the MA ) yt5c ## Time Series: ## Start = 1 ## End = 8 ## Frequency = 1 ## [,1] ## [1,] NA ## [2,] NA ## [3,] 212.0 ## [4,] 216.0 ## [5,] 219.0 ## [6,] 221.2 ## [7,] NA ## [8,] NA Hence we are able to produce moving averages based on the data. Notice, that the most recent MA is the prediction, hence being \\(\\hat{Y}_{t+1}\\). One could extend this, by adding this value to the time series and then calculate MA for the period hereafter. We see that the output of the table above is somewhat misleading, as the most recent MA predictinos, are not positioned in the end, but instead where the center actually is. This problem is solvable using filter(). See the following chunk k &lt;- 5 #specify the order of the moving average c &lt;- rep (1/k,k) #remember that simple average is a weighted average with equal weights, #you need to specify weights for the filter command to work yt5&lt;- filter(yt, c, sides = 1) ts.plot(yt5)# &quot;Plotting the MA&#39;s Figure 4.4: 5k Moving Average yt5 #The updated vector of MA&#39;s ## Time Series: ## Start = 1 ## End = 8 ## Frequency = 1 ## [,1] ## [1,] NA ## [2,] NA ## [3,] NA ## [4,] NA ## [5,] 212.0 ## [6,] 216.0 ## [7,] 219.0 ## [8,] 221.2 This we see, that scores are moved to the end, so even though it is the center of the MA, it is now presented as recent values. For simple moving averages, one may do it in excel, it may be easier and quicker. 4.4.1.2 Exponential moving averages + Holts and Winters Simple exponential smoothing Where; alpha is the smoothing parameter, beta tells you if you should account for a trend or not, gamma is responsible for the presence of a seasonal component in the model fit &lt;- HoltWinters(yt ,alpha = 0.4 ,beta = FALSE ,gamma = FALSE) plot(fit,xlim = c(1,nrow(df))) + grid(col = &quot;lightgrey&quot;) ## integer(0) legend(&quot;topleft&quot;,c(&quot;Observed&quot;,&quot;Fitted&quot;),lty = 1,col = c(1:2)) Figure 4.5: Exponential Smoothing Hence we see the smoothed values, where the higher alpha, the more will the fitted line track the changes in the observations. We can now plot the forecast values: plot(forecast(fit),xlim = c(1,nrow(df)+10))# + grid(col = &quot;lightgrey&quot;) legend(&quot;topleft&quot;,c(&quot;Observed&quot;,&quot;Forecast&quot;),lty = 1,col = c(&quot;Black&quot;,&quot;Blue&quot;)) Figure 4.6: Forecast Exponential Smoothing One see the confidence intervals of the forecast widening as we get further away from the actual values. Now one may assess the accuracy: accuracy(forecast(fit)) ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 8.001509 8.209102 8.001509 3.673623 3.673623 2.154252 0.1962194 One see an RMSE of 8.2. Hence one could compare it with an exponential smoothing, which is more sensitive to the observations. fit0.6 &lt;- HoltWinters(yt ,alpha = 0.6 #Changed ,beta = FALSE ,gamma = FALSE) accuracy(forecast(fit0.6)) ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 5.860023 6.373698 5.860023 2.700235 2.700235 1.577698 0.264614 Where we see an RMSE of 6.37, hence lower than the initial test. Holt’s exponential smoothing exponential smoothing when a trend component is present: beta = TRUE {fit3 &lt;- HoltWinters(yt ,alpha = 0.6 ,beta = TRUE ,gamma = FALSE) plot(forecast(fit3)) accuracy(forecast(fit3))} Figure 4.7: Holt’s Exponential Smoothing ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set -1.990133 4.369207 3.451467 -0.9324372 1.581919 0.929241 0.4352197 We see that the RMSE is even lower (4.37). Which is expected, as Holt’s exponential smoothing accounts for trend. Winter’s Exponential Smoothing Which accounts for trend and seasonality in order to make it work one needs to define the frequency of your seasonal component, when specifying the ts data {yt &lt;- ts(df ,frequency = 4) # let&#39;s assume we suspect a quarterly pattern fit4 &lt;- HoltWinters(yt, alpha=0.6, beta=TRUE, gamma=TRUE) plot(forecast(fit4)) accuracy(forecast(fit4))} # experiment with seasonality frequency to see if you can get any lower in MSE ## ME RMSE MAE MPE MAPE MASE ## Training set -0.310075 3.973148 3.34445 -0.1227802 1.50834 0.2730163 ## ACF1 ## Training set 0.009251985 We see that the RMSE is even lower (3.97), hence Winters Exponential Smoothing appear to be the best model for prediction. 4.4.2 p. 93 HW Problem 9-10 Basically just another example of the exercise above. 4.4.3 CO2 and Sales Data Not done again. Do if time allows. 4.4.4 Case 6 oo. 108-111 HW Not done again. Do if time allows. "],["simple-and-multiple-linear-regression.html", "5 Simple and Multiple Linear Regression", " 5 Simple and Multiple Linear Regression This chapter elaborates on how linear regression may be applied to forecast data and also how we may get rid of trends when applying linear regression and assesing the assumptions for the model. Lastly, the chapter elaborates on how to chose a model Lectures 5th lecture - simple linear regression 6th lecture - multiple linear regression Literature HW: Simple Linear Regression "],["simple-linear-regression.html", "5.1 Simple Linear Regression", " 5.1 Simple Linear Regression I will not go much in details with what simple linaer regression is. One can calculate the beta values by the following \\[\\begin{equation} b_0=\\ \\overline{Y}-\\ b_1\\overline{X} \\tag{5.1} \\end{equation}\\] \\[\\begin{equation} b_1=\\frac{\\sum_{ }^{ }\\left(X-\\overline{X}\\right)\\left(Y-\\overline{Y}\\right)}{\\sum_{ }^{ }\\left(X-\\overline{X}\\right)^{^2}} \\tag{5.2} \\end{equation}\\] Where point forecast, hence \\(\\hat{Y}\\) is merely the sum of the linear equation, hence \\(\\hat{Y}=b_0+b_1X^*\\), where \\(X^*\\) is the specific X values. Thence one can estimate the standard error by: \\[\\begin{equation} s_{yx}=\\sqrt{\\frac{\\sum_{ }^{ }\\left(Y-\\overline{Y}\\right)^{^2}}{n-2}} \\tag{5.3} \\end{equation}\\] Equations (5.3) can also be written otherwise, see the slides for that. The residuals can be broken down to the following \\[\\begin{equation} \\sum_{ }^{ }\\left(Y-\\overline{Y}\\right)^{^2}=\\sum_{ }^{ }\\left(\\hat{Y}-\\ \\overline{Y}\\right)^{^2}+\\sum_{ }^{ }\\left(Y-\\ \\hat{Y}\\right)^{^2} \\tag{5.4} \\end{equation}\\] Which consist of the following three elements. \\[\\begin{equation} SST = SSR + SSE \\tag{5.5} \\end{equation}\\] The residuals can then be applied for a goodness of fit assessment, where one can identify R squared- \\[\\begin{equation} R^2=\\frac{SSR}{SST} \\tag{5.5} \\end{equation}\\] So what can the linear regression then be used for? Inference Prediction Notice, that inference can only be done when the model is adequate, hence the assumptions actually being met. 5.1.1 Assumptions We have the following assumptions for a linear model: The underlying relationship between dependent and independent variable is actually linear Independent residuals Homoskedastic residuals (show constant variance) Identically distributed (In general, normal distribution is assumed) Hence how is the assumptions tested? Some can be done before analysis and others after the model is applied, hence it can be described by the following: Before the model is applied: The underlying relationship between dependent and independent variable is actually linear After the model is applied (doing diagnostics): Independent residuals Homoskedastic residuals (show constant variance) Identically distributed (In general, normal distribution is assumed) Now lets dive into the data Serial correlation and Heteroskedasticity Notice that autocorrelation = serial correlation Serial correlation is where the observations are trailing each other, where heteroskedasticity is where the variance is changing over timer: 5.1.1.1 Serial correlation (checking for independent residuals): We must make sure that the residuals does not have a clear pattern, as that means that some variables has been omitted. This can be assessed for example by: Visual inspection Durbin Watson test, see equation (5.6) Correlogram Statistical test for relationship between residuals and lagged residuals Figure 5.1: Serial Correlation Example Notice, that one should also test for autocorrelation in the errors, that can be done with a Durbin-Watson statistic: \\[\\begin{equation} DW\\ =\\frac{\\sum_{ }^{ }\\left(e_t-e_{t-1}\\right)^{^2}}{\\sum_{ }^{ }e_t^{^2}} \\tag{5.6} \\end{equation}\\] 0 &lt; DW &lt; 4, where if DW = 2 it indicates no serial correlation (this is the ideal), generally if 1.5 &lt; DW &lt; 2.5 is widely used as an acceptable level. If DW &gt; 2, it indicates negative serial correlation and if DW &lt; 2, it indicates that there is positive serial correlation. One could also use correlation testing by checking correlogram of residuals or testing residuals against lagged residuals Solution Try with lagging the variables 5.1.1.2 Heteroskedasticity: We want the variance to have a constant variance. This can be checked visually, where we dont want to see a funnel shape, as in the visualization below. Figure 5.2: Heteroskedasticity Example This can also be tested with a Breusch-Pagan Test, where the null hypothesis is that all errors are equal, hence null hypothesis, is that the errors are homoskedastic. Solution Try lagging the variables Try applying differences in the observations If one observe heteroskedasticity and can’t get rid of it, then one can apply a generalized least squares method. Although this introduce a set of new assumptions. 5.1.2 Forecasting with a linear trend If the data contain a linear trend, then we are able to make the model account for this trend. Hence we are able to detrend the data, by including a counter as a variable. Another solution may be to using the differences in the observations, e.g., using growth rates. This is important, as if one don’t detrend the data, then the model will merely describe the trend and not the actual values behind the trend, thus you want the model to account for the trend 5.1.3 Exercises 5.1.3.1 Problems 5 pp. 209 df &lt;- read_excel(&quot;Data/Week46/prob5p209.xlsx&quot;) plot(x = df$Age,y = df$MaintenanceCost,) + grid(col = &quot;lightgrey&quot;) + abline(reg = lm(MaintenanceCost ~ Age,data = df)) ## integer(0) coef(object = lm(MaintenanceCost ~ Age,data = df)) ## (Intercept) Age ## 208.20335 70.91813 If age is the dependent variable We see that the intercept is - 1.78, indicating that if there are no maintenance costs, then the age of the vehicle is -1.78 years old, which is naturally not possible, hence we see that the linear relathionship indicates that there will always be maintenance costs. If maintenance cost is the dependent variable We see that the intercept is 208, hence there will be a begin maintenance of 208, which will always be there, and then for each year, it is expected to increase with about 71. We can then test to see if the relationship is significant from a statistic point of view. lm(Age ~ .,data = df) %&gt;% summary() ## ## Call: ## lm(formula = Age ~ ., data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.6658 -1.0498 -0.7737 1.0125 2.0119 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.789563 1.268264 -1.411 0.201098 ## MaintenanceCost 0.012398 0.001737 7.139 0.000187 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.476 on 7 degrees of freedom ## Multiple R-squared: 0.8792, Adjusted R-squared: 0.862 ## F-statistic: 50.96 on 1 and 7 DF, p-value: 0.0001872 We see that there is statistical evidence to say that the relationship is linear. Also the coefficient of determination is 86% hence 86 of the variance is explained by the x variable. Now we can do the diagnostics: plot(lm(Age ~ .,data = df)) Before the model is applied: The underlying relationship between dependent and independent variable is actually linear, This we must assume After the model is applied (doing diagnostics): Independent residuals We dont say any indication that the residuals are not independent. Homoskedastic residuals (show constant variance) The variance appear to be constant Identically distributed (In general, normal distribution is assumed) This we must assume rm(list = ls()) 5.1.3.2 Problem 11 p. 212 HW 5.1.3.3 Cases 2 HW Notice that the X is deviations from 65 degrees, as 65 degrees is the ideal for the production, hence when one read 10 degrees, then it is in fact 75 degrees or 55. Notice that the deviation is in absolut values. df &lt;- read_excel(&quot;Data/Week46/Case2p222.xlsx&quot;) #head(df) #To see the first observations in each Y &lt;- df$Y X &lt;- df$X scatter.smooth(x=X, y=Y, main=&quot;Y ~ X&quot;) Figure 5.3: Plotting the model cor(Y, X) ## [1] -0.8010968 We see that the correlation is negative, hence -0.8 linMod &lt;- lm(Y ~ X) #To omit intercept when necessary, the formula can be written for example as lm(cost ~ age - 1) summary(linMod) ## ## Call: ## lm(formula = Y ~ X) ## ## Residuals: ## Min 1Q Median 3Q Max ## -91.38 -43.83 -12.64 44.48 122.25 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 552.040 22.851 24.158 &lt; 2e-16 *** ## X -8.911 1.453 -6.133 4.37e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 59.41 on 21 degrees of freedom ## Multiple R-squared: 0.6418, Adjusted R-squared: 0.6247 ## F-statistic: 37.62 on 1 and 21 DF, p-value: 4.374e-06 We see an R square of 62.47 indicating that the linear relationship is not describing the relationship in the sample data very well, which the illustration above also show quite well. plot(resid(linMod)) Figure 5.4: Residuals Case 2 Now we can assess if the data show autocorrelation. That can be done by using the acf() acf(resid(linMod)) # white noise residuals? Figure 5.5: Correlogram The data appear to show white noise, although there appear to be some pattern, which may show seasons in the data. We can test to see if the residuals actually show constant variance, of the variance is not constant (heteroskedasticity) bptest(linMod) # Breusch-Pagan test H_0: variance is constant. ## ## studentized Breusch-Pagan test ## ## data: linMod ## BP = 1.069, df = 1, p-value = 0.3012 Since the p value is not significant, there is not enough evidance to reject the null hypothesis, hence we may assume that the residuals show constant variance. { AIC(linMod) %&gt;% print() BIC(linMod) %&gt;% print()} ## [1] 257.0604 ## [1] 260.4669 We see the different information criteria, but we need other models to assess what is good and what is bad. Q1 How many units would your forecast for a day in which the high temperatire is 80 degrees a &lt;- data.frame(X=24) #65+24=89 degrees predict(linMod, a) %&gt;% print() ## 1 ## 338.1778 Hence we may expect 338 units to be produced on a day with 89 degrees. Q2 When the degrees is 41, hence also a deviation of 24 degrees. Q3 Is the forecasting tool effective? We saw earlier that 80% of the variance is explained, although there is probably room for optimization rm(list = ls()) 5.1.3.4 Case 3 from HW df &lt;- read_excel(&quot;Data/Week46/Case2p222.xlsx&quot;) #head(df) #To see the first observations in each Y &lt;- df$Y X &lt;- df$X plot(X,Y) cor(Y, X) ## [1] -0.8010968 We see that the correlation is negative, hence -0.8 cor.test(Y,X) ## ## Pearson&#39;s product-moment correlation ## ## data: Y and X ## t = -6.1335, df = 21, p-value = 4.374e-06 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.9121082 -0.5806250 ## sample estimates: ## cor ## -0.8010968 5.1.3.5 Detrending thorugh regression: CO2 CO2levels &lt;- read_excel(&quot;Data/Week46/CO2levels(1).xlsx&quot;) y &lt;- ts(CO2levels, frequency = 12) #Monthly series, so we specify the frequency=12. trend &lt;- seq(1:length(y)) #Creating the linear trend, simply a counter plot(y) Figure 5.6: CO2 data We see that there is a trend and cycles. This we want to get rid of, by enabling the model to account for that. fit &lt;- lm(y ~ trend) summary(fit) ## ## Call: ## lm(formula = y ~ trend) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.392 -1.866 0.199 2.190 3.677 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.293e+02 3.462e-01 951.40 &lt;2e-16 *** ## trend 1.210e-01 3.707e-03 32.64 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.186 on 159 degrees of freedom ## Multiple R-squared: 0.8701, Adjusted R-squared: 0.8693 ## F-statistic: 1065 on 1 and 159 DF, p-value: &lt; 2.2e-16 ystar &lt;- resid(fit) #ystar is the detrended series, which is simply the residuals from the previous regression. plot(ystar) + lines(ystar, col=&quot;red&quot;) Figure 5.7: Detrended series ## integer(0) Now wee see that the data show constant variance and appear to be stationary around a mean of 0. But did we get rid of the seasonality? To compare, one can print the detrended and the initial data. par(mfrow = c(2,1)) acf(y, 50) acf(x = ystar,lag.max = 50) #We can specify how many lags we want to plot. Here I just chose 50. Figure 5.8: Correlogram comparison detrended Hence we see that we got rid of the trend, but still see that there is great seasonality in the data. Alternative to detrending –&gt; differencing An alternative to detrending with the trend variable, is to using differencing dy &lt;- diff(x = y,lag = 1) plot(dy) # Figure 5.9: Detrending using differencing acf(dy, 50) Figure 5.10: Detrending using differencing "],["multiple-linear-regression.html", "5.2 Multiple Linear Regression", " 5.2 Multiple Linear Regression Simply regression with more than one independent variable. The diagnostics tools are the same as before, significance testing, R square, DW stats, residuals diagnostics. 5.2.1 Multicollinearity Although in MLR one must be aware of multicollinearity, meaning that do we see a strong relationship between independent variables, hence are they explaining the same? To assess for multicollinearity one can apply VIF, which is the following: \\[\\begin{equation} VIF_j=\\frac{1}{1-R^2_j} \\tag{5.7} \\end{equation}\\] Where \\(j = 1,...,k\\) Thus, we see that Rsquare is obtained from regression each IDV against the remaining variables. We can then have the following outputs: VIF = 1, no milticollinearity VIF &gt; 10, indicates multicollinearity If one gets an indication of multicollinearity, then one should drop one of the correlated variables. 5.2.2 Serial correlation and omitted variables When doing regression, we may observe that the IDVs are correlated with the error term, meaning that the errors are not randomly distributed, hence serial correlation in the error terms. For serial correlation in the error terms, we are able to make use of the Durbin-Watson, see (5.6). 5.2.3 Selection criteria We cannot use R square anymore, as it will never really penalize when we are adding variables. Hence one should use&gt; AIC BIC Depending on whether one is interested in the best model for prediction or the true model. "],["time-series-decomposition-and-regression-with-time-series-data.html", "6 Time-Series Decomposition and Regression with Time-Series Data", " 6 Time-Series Decomposition and Regression with Time-Series Data This chapter identifies what elements a time series can be broken into, hence it elaborates on the trend, seasonality, cycles and irregular movements Litterature HW: Time Series and Their Components HW: Regression with Time Series Data "],["arima-models-and-box-jenkins-methodology.html", "7 ARIMA Models and Box-Jenkins Methodology", " 7 ARIMA Models and Box-Jenkins Methodology Explanation here Litterature HW: The Box-Jenkins (ARIMA) Methodology "],["communicating-technical-findings.html", "8 Communicating Technical Findings", " 8 Communicating Technical Findings This chapter is about efficient report writing Litteratur Link to article: https://www.wgtn.ac.nz/learning-teaching/support/approach/steps-to-teaching-success/resources/WSBG-report-writing-guide-2017.pdf Lectures Lecture 6 The following path contain a folder with a template for reports: Business Forecasting/Week 46 Simple Linear Regression/BusinessReportRMD "],["Methods.html", "9 Methods and Performance Measurement ", " 9 Methods and Performance Measurement "],["ForecastMethods.html", "9.1 Forecasting Methods", " 9.1 Forecasting Methods Naive forecasts This is merely the current period is assumed to be the best predictor for the future, hence it can be written as: \\[\\begin{equation} \\hat{Y}_{t+1}=Y_t \\tag{9.1} \\end{equation}\\] where, \\(Y_t\\) = the last period, hence \\(\\hat{Y}_{t+1}\\) = the following period. Therefore, the error can merely be written as: \\(e_t=Y_{t+1}-\\hat{Y}_{t+1}\\), being the actual amount compared with the foretasted value. One can make several iterations to account for trending, the growth rate, or seasonal data. Those being: \\(\\hat{Y}_{t+1}=Y_t+(Y_t-Y_{t-1})\\), to account for trending data (non stationary data) \\(\\hat{Y}_{t+1}=Y_t * \\frac{Y_t}{Y_{t-1}}\\), to account for the growth rate, notice that it only assess the growth rate to the prior period. \\(\\hat{Y}_{t+1}=Y_{t-3}+\\frac{Y_t-Y_{t-4}}{4}\\), to account for quarterly trending data, the periods can naturally be changed by changing the formula, e.g. to 12. but notice, that this is just replicating previous periods, hence also previous seasons 9.1.1 Using Averages We have the following: Simple averages, which merely takes the average of all observations. Moving averages, which account for the given time frame. This can be extended by, Double moving averages, often seen when you need the center value of a period consisting of an even number of periods, where there is no actual median value, thus one can extend the MA with a double MA. 9.1.1.1 Simple Averages One may assume that it is sufficient to apply the average of all observations, to predict the next period, hence we can say: \\[\\begin{equation} \\hat{Y}_{t+1}=\\frac{1}{n}\\sum^t_{i=1}Y_i \\tag{9.2} \\end{equation}\\] This is appropriate if the data has shown historical stability, thus without seasons, trends and etc. 9.1.1.2 Moving Average (MA) One may apply a moving average instead, accounting for k periods, also one could extend this by adding weights. For practical purposes only a k period MA is show: \\[\\begin{equation} \\hat{Y}_{t+1}=\\frac{Y_t+Y_{t-1}+...+Y_{t-k}}{k} \\tag{9.3} \\end{equation}\\] this may be applied to remove seasonal effect, either by k=4 or 12 if the data is respectively quarterly or monthly 9.1.1.3 Double Moving Average This is simply doing moving averages twice, hence it is an extension of equation (9.3) As mentioned, often seen when one wants the median when using an even number of periods, e.g. 12 months, hence double MA can be applied. \\[\\begin{equation} M_t=\\hat{Y}_{t+1}=\\frac{Y_t+Y_{t-1}+...+Y_{t-k+1}}{k} \\tag{9.4} \\end{equation}\\] \\[\\begin{equation} M&#39;_t=\\frac{M_t+M_{t-1}+...+M_{t-k+1}}{k} \\tag{9.5} \\end{equation}\\] \\[\\begin{equation} a_t=M_t+\\left(M_t-M_t\\right) \\end{equation}\\] \\[\\begin{equation} b_t=\\frac{2}{k-1}\\left(M_t-M&#39;_t\\right) \\end{equation}\\] Thence we are able to say: \\[\\begin{equation} \\hat{Y}_{t+p}=a_t+b_t*p \\tag{9.6} \\end{equation}\\] 9.1.2 Linear regressions Linear regression with a trend: that is normal linear regression, where the trend is added as a counter, which will account for the trend, given it is linear. 9.1.3 Non linear regressions Non linear regression with trend Causal regression 9.1.4 Smoothing methods 9.1.4.1 Exponential smoothing This is exponentially weighted moving average of all historical values, meaning that the most recent value will be assigned the most weight. Hence we merely add different weights to past periods, thus there is no specific way to adjust for trend and seasonality, which is a limitation of exponential smoothing, it can be written as: \\[\\begin{equation} \\hat{Y}_{t+1}=\\alpha Y_t+\\left(1-\\alpha\\right)\\hat{Y}_t \\end{equation}\\] thus: \\[\\begin{equation} =\\hat{Y}_t+\\alpha(Y_t-\\hat{Y}_t) \\end{equation}\\] Where \\(\\alpha\\) = the smoothing constant, thus is can be between 0 and 1. The higher alpha the largest weight to the most recent observation. Then how to choose the smoothing parameter \\(\\alpha\\)? For stable predictions, choose a high alpha For sensitive predictions, choose low alpha Test different alpha values and compare the models using the performance measures in section 9.2. 9.1.4.2 Holt’s exponential smoothing Exponential smoothing method with adjustment for trend, hence we introduce a new tuning parameter, hence we have \\(\\alpha\\) and \\(\\beta\\) \\(\\alpha\\) = Weight to the most recent observations \\(\\beta\\) = adjustment for trend. R will automatically set this, when beta = TRUE Hence the smoothing now consists of two elements: The level estimate \\[\\begin{equation} L_t=\\alpha Y_t+\\left(1-\\alpha\\right)\\left(L_{t-1}+T_{t-1}\\right) \\tag{9.7} \\end{equation}\\] The trend estimate \\[\\begin{equation} T_t=\\beta\\left(L_t-L_{t-1}\\right)+\\left(1-\\beta\\right)T_{t-1} \\tag{9.8} \\end{equation}\\] Thus, the forecasting of p periods into the future, can be explained by: \\[\\begin{equation} \\hat{Y}_{t+p}=L_t+pT_t \\tag{9.9} \\end{equation}\\] To apply: use HoltWinters() and select parameters that lowers the performance measurements. When one assigns large weights the model will become more sensitive to changes in the observed data. One can either set the initial value to 0 or take the average of the first few observations. If \\(\\alpha = \\beta\\), then we have the Brown’s double exponential smoothing model. If \\(\\beta\\) = 0, then we merely have a simple exponential smoothing. 9.1.4.3 Winters’ exponential smoothing Exponential smoothing method with adjustment for trend and seasonality, hence we introduce two new tuning parameters, hence we have \\(\\alpha\\), \\(\\beta\\) (as in Holt’s) and \\(\\gamma\\) Hence the smoothing now consists of three elements: The level estimate \\[\\begin{equation} L_t=\\alpha\\frac{Y_t}{S_{t-s}}+\\left(1-\\alpha\\right)\\left(L_{t-1}+T_{t-1}\\right) \\tag{9.10} \\end{equation}\\] The trend estimate \\[\\begin{equation} T_t=\\beta\\left(L_t-L_{t-1}\\right)+\\left(1-\\beta\\right)T_{t-1} \\tag{9.11} \\end{equation}\\] The seasonality estimate \\[\\begin{equation} S_t=\\gamma\\frac{Y_t}{L_t}+\\left(1-\\gamma\\right)S_{t-s} \\tag{9.12} \\end{equation}\\] Thus, the forecasting of p periods into the future, can be explained by: \\[\\begin{equation} \\hat{Y}_{t+p}=\\left(L_t+pT_t\\right)S_{t-s+p} \\tag{9.13} \\end{equation}\\] To apply: use HoltWinters() and select parameters that lowers the performance measurements. When one assigns large weights the model will become more sensitive to changes in the observed data. One can either set the initial value to 0 or take the average of the first few observations. if \\(\\beta = \\gamma = 0\\) the model is merely simple exponential smoothing. 9.1.4.4 Moving Averages, see section 9.1.1.2 9.1.5 ARIMA Decomposition of the time series AR: MA: ARMA: ARIMA: "],["PerformanceMeasurements.html", "9.2 Performance Measurements", " 9.2 Performance Measurements Mean error (ME): \\[\\begin{equation} ME=\\frac{1}{n}\\sum_{ }^{ }\\left(Y_t-\\hat{Y}_t\\right)$ \\tag{9.14} \\end{equation}\\] Mean Absolute Deviation (error): \\[\\begin{equation} MAD\\left(i.e.\\ MAE\\right)\\ =\\ \\frac{1}{n}\\cdot\\sum_{ }^{ }\\left|Y_t-\\hat{Y}_t\\right| \\tag{9.15} \\end{equation}\\] Mean Percentage Error (MPE): \\[\\begin{equation} MPE\\ =\\ \\frac{1}{n}\\ \\sum_{ }^{ }\\frac{\\left(Y_t-\\hat{Y}_t\\right)}{Y_t} \\tag{9.16} \\end{equation}\\] Mean Absolute Percentage Error (MAPE): \\[\\begin{equation} MAPE\\ =\\ \\frac{1}{n}\\ \\sum_{ }^{ }\\frac{|\\left(Y_t-\\hat{Y}_t\\right)|}{|Y_t|} \\tag{9.17} \\end{equation}\\] Mean-Squared Error (MSE): \\[\\begin{equation} MSE=\\frac{1}{n}\\sum_{ }^{ }(Y_t-\\hat{Y}_t)^2 \\tag{9.18} \\end{equation}\\] Root Mean-Squared Error: \\[\\begin{equation} RMSE=\\sqrt{MSE} \\tag{9.18} \\end{equation}\\] "]]

[["index.html", "Business Forecasting setup", " Business Forecasting setup All the output formatting is done in the _output.yml file while all bookdown rendering options is done in the _bookdown.yml file.§ library(bookdown) Tips and tricks Referencing This is making a figure with a caption and where we center the figure and set the size. Notice, that the figure is automatically numbered according to the chapter number. plot(pressure,type = &#39;b&#39;,pch = 19) Figure 0.1: A fig Now we are able to make a reference to the chunk. Refer to a chunk: 0.1 Options RMarkdown Reference Guide "],["introduction.html", "1 Introduction", " 1 Introduction "],["curriculum.html", "2 Curriculum", " 2 Curriculum Description of qualifications (Expectations from you): This course will provide you with the ability to: Understand and argue why forecasting is important and discuss different approaches/strategies/principles implemented in the business world. Explain the key difference between qualitative and quantitative methods in forecasting. Reason and argue for which model to use in the face of real-world business situations. Carry out static and dynamic forecasting based on linear regressions and time-series methodology. Evaluate the accuracy of forecast outcomes. Contents: This course is designed to give a solid theoretical and applied background to graduate students in forecasting. Students are expected to have taken Quantitative Research Methods, or an equivalent course that covers regression analysis with a good understanding of the statistical methodology used. The course will not only be a methodology course but equally an applied course in that students will develop skills to approach business life situations critically, evaluate and communicate their findings with ease. The applications that immediately follow the theoretical topics to be taught will cover different business topics including quality control, product demand analysis, marketing and advertising. Tentative Outline of Course Topics: Qualitative Forecasting Methods Quantitative and qualitative forecasting New product forecasting + Executive opinions Sales forces opinions Consumer surveys Delphi method Forecast process, data considerations and model selection Trend, seasonal, cycle and irregular components + Statistical review Correlograms Moving average and exponential smoothing methods Moving average Holt’s and Holt-Winter’s exponential smoothing models + Product demand forecasting Forecasting with static regression methods Bivariate regression model review Forecasting with simple linear trend Serial correlation and heteroskedasticity Time-series decomposition Basic time series decomposition models + Deseasonalizing and seasonal indices Time-series decomposition forecast Applications ARIMA processes and Box-Jenkins methodology Moving average models + Autoregressive models Mixed autoregressive and moving average models Model selection, Box-Jenkins identification process, estimation of ARIMA processes Forecasting seasonal time series Applications Dynamic forecasting of economic and financial time series Nonstationary time series + Cointegration Spurious regression Combining forecast results Some basic theorems on diversification of forecasts + Nelson combination method Granger-Ramanathan combination method Combinations with time-varying weights Applications Forecast evaluation Measures of forecasting accuracy Diebold-Mariano test for significant differences in forecasting accuracies "],["qualitative-forecasting-methods.html", "3 Qualitative Forecasting Methods", " 3 Qualitative Forecasting Methods – HW: Introduction to forecasting – HW: Judgmental Forecasting and Forecast Adjustments – HW: A Review of Basic Statistical Concepts – Armstrong et al. (2000), “Sales Forecasts for Existing Consumer Products and Services: Do Purchase Intentions Contribute to Accuracy?”, International Journal of Forecasting, 16(3), 383-397. – Lawrence et al. (2006), “Judgmental forecasting: A review of progress over the last 25 years”, International Journal of Forecasting, 22, 493-518. – Lynn et al. (1999), “Survey of New Product Forecasting Practices in Industrial High Technology and Low Technology Businesses,” Industrial Marketing Manage- ment, 28, 565-571. – Bass, Frank M. (1969). “A new product growth model for consumer durables,” Management Science, 15, 215-227. "],["judgmental-forecasting-and-forecast-adjustments.html", "3.1 Judgmental Forecasting and Forecast Adjustments", " 3.1 Judgmental Forecasting and Forecast Adjustments A forecast on its own does not include the analysts judgements well enough. To include experts insights, one can apply different methods for this, that is called judgmental forecasting. There er different methods for including experts input on the analysis. See below: The Delphi Method: That is an iterative process where experts makes analysis an predictions independently, then the findings are distributed and then they are able to independently make corrections to their output if they feel to. The results are then distributed and they are possible to correct their output The feedback loop ends when they have reached redundancy and they make no more corrections. Thus, the output is different expert views, that you can select between. Scenario writing: you have different experts that write scenarios that are likely to happen in the future, you rank those events after likelyhood. The scenario writing is followed by discussion. Thus they are able to defend and modify predictions. Combining forecasts: This is where you make different predicte analysis perhaps not including the same predictors, then you use the outputs to compute a forecast, that is done either by finding the mean of the forecasts or assigning weights to the different forecasts that now act as predictors. Forecasting and Neural Networks: NN is used to find its own variables, it is particularly good to fill in missing values. Other tools to judgmental decision making: The decision making tree, where you assign costs and probabilities to events. Bayesian statistics, also where you create a tree of probabilities. "],["articles.html", "3.2 Articles", " 3.2 Articles 3.2.1 A new product growth model for consumer durables,” Management Science The Bass Model i.e. the Bass Curve is the model for sales of new products, or more correct a generalization of the sales for a product, hence he created the model for adopters, where it has later been separated into innovators, early adopters, early majority, etc. that is then the product life cycle. Thus the purpose of the Bass Model is to predict sales of a new product, based on questionnaires or early sales The Bass Curve is based on the early adopters and calculates the curve based on their numbers. The model has proven that it only needs very few observations to plot when it is going to peak and at what amount. Basically the model is based on a conditional likelihood with respect to time. In fact, it has shown to predict peak and sales amount based on research and questionnaires prior to sales Then there are some notes on what further research that can be done and some notes on extensions to the Bass Model 3.2.2 Sales Forecasts for Existing Consumer Products and Services: Do Purchase Intentions Contribute to Accuracy? As we know from the Bass Model we can use consumer intentions (questionnaires about whether a person think he will buy it or not) to guestimate the total sales and when it is going to peak, but are we are able to optimize accuracy of forecasts on existing products using consumer intentions? A previous study has shown, that intentions does not improve the model, although the data did not have available the intentions and whether the person made a purchase or not. In this study there is an improvement to the forecast when the historical data is combined with the intentions of the persons. Also they ruled out, the factor that if the data does not have historical data on intentions and sales history. Hence they experienced on their seven data set, that combining historical data and intentions improved the forecast. To be noted is that the manipulated the intentions in different ways to experiment, e.g. moving average, binary (intentions or not) and some other faucets. 3.2.3 Judgmental forecasting A review of progress over the last 25 years, International Journal of Forecasting This is basically just more information on the judgemental forecasting where they tested different set ups, such as groups, bootsstrapping etc. Did not read all of this, it seemed very much like HW9 "],["exploring-patterns-and-forecasting-techniques.html", "4 Exploring Patterns and Forecasting Techniques", " 4 Exploring Patterns and Forecasting Techniques This chapter will elaborate on how one identify patterns in data thus how to account for this. Thus, we are looking into smoothing methods and moving averages Additionally, we will explore how we select forecasting methods Literature: HW: Exploring Data Patterns and an Introduction to Forecasting Techniques HW: Moving Averages and Smoothing Methods Armstrong, J.S. (2001) “Selecting Forecast Methods”, In Principles of Forecasting: A Handbook for Researchers and Practitioners (Ed. J. Scott Armstrong), Kluwer "],["the-forecasting-process.html", "4.1 The forecasting process", " 4.1 The forecasting process The following describe the forecasting process, hence what one must consider before performing the forecast and ultimately using the forecasts, the purpose of the process is to make sure that the forecast is reliable Specify objectives Reason for the forecast Applications based on the forecast Good communication between all those involved Determine what to forecast Based on set objectives, choose key indicator(s) Example: domestic sales, export sales, or even both? Identify time dimension Length and periodicity of the forecast Desired frequency Urgency of the forecast Planning of the forecast Data considerations Available and quantity of the data Internal vs. external data Desired frequency in data (annual, quarterly, monthly) Example: Dollar sales instead of unit sales Model selection The pattern exhibited by the data The quantity of historic data available The length of the forecast horizon Figure 4.1: Model Selection Model evaluation Testing the models on the series to be forecast Checking how each model works ‘in sample’ Measures such as MSE, RMSE, etc. used to rank models Fit (in sample) vs. accuracy (out of sample) Forecast preparation Based on the selected model, obtain the forecast Keep possibly competing models See if their combination yields mode accuracy Presentation of forecast result Clear communication Keep it as simple as possible Visual aids to support the findings Tracking results Comparison of forecasts to actual values Re-specify the selected model(s) over time if necessary Try other model combinations to keep the accuracy level intact Conclusion: One should realize that it is an iterative process, that one must be aware of. "],["data-patterns-and-terminology.html", "4.2 Data Patterns and terminology", " 4.2 Data Patterns and terminology Basically the data is assumed to consist of up to four components, that is: Trend Long-term change in the level of data Positive vs. negative trends Stationary series have no trend Example: Increasing technology leading to increase in productivity Seasonal Repeated regular variation the level of data Example: Number of tourists in Mallorca Cyclical Wavelike upward and downward movements around the long-term trend Longer duration than seasonal fluctuations Example: Business cycles Note, this is very often to identify Irregular Random fluctuations Possibly carrying more dynamics than just deterministic ones Hardest to capture in a forecasting model The four components may look similar to this: Figure 4.2: Components in a timeseries 4.2.1 Terminology \\(Y_t\\): Denotes a time series variable \\(\\hat{Y_t}\\): Denotes the foretasted value of \\(Y_t\\) \\(e_t=Y_t-\\hat{Y_t}\\): Denotes the residual or the forecast error. \\(Y_{t-k}\\): Denotes a time series variable lagged by k periods. 4.2.1.1 Autocorrelation Autocorrelation: is the correlation between a time series and its past (lagged) observations. To identify this, one can merely compare the lagged values as a series for itself, hence comparing actual time series against the lagged time series. This can be written as: \\[r_k=\\frac{\\sum_{t=k+1}^n\\left(Y_{t\\ }-\\hat{Y}\\right)}{\\sum_{t=1}^n\\left(Y_t-\\hat{Y}\\right)^{^2}}\\] Where \\(k = 0,1,2,...\\), hence take on numbers, typically whole numbers, as the result must be measurable. We assess autocorrelation to identify if the data have a trend, seasons, cycles or it is random? If we have seasons, trends or cycles, we must make the model account for this, otherwise one is prone to have a model where it is just implicitly correlated, but that is merely due to the autocorrelation, as it says in the word, it is automatically correlated, but that also implies, that it is not necessarily caused by the data, but rather other factors, often we see macro factors, that have an influence, e.g. an economic book. Autocorrelation can be plotted using an autocorrelation function (ACF) or merely by using a correlogram, which is a k-period plot of the autocorelation, that looks like the following: Figure 4.3: Correlogram Example Where one wants to be within the upper and lower level. Manually testing for autocorrelation One must: Calculate \\(r_k\\) Calculate \\(SE(r_k)\\) Hypothesis: \\(H0 : \\rho=0\\), \\(H0 : \\rho≠0\\) We apply t-test Where: \\[SE\\left(r_k\\right)=\\sqrt{\\left\\{\\frac{1+2\\sum_{i=1}^{k-1}r_i^2}{n}\\right\\}}\\] Although, with normal approximation \\[SE\\left(r_k\\right)=\\frac{1}{\\sqrt{n-k}}\\] and test statistic equal \\[t=\\frac{r_k}{SE(r_k)}\\] Thence one merely must look up the cut off values and assess if there is statistical evidance for autocorrelation or not. Alternative: Ljung-Box Q statistic The Ljung Box Q is to identify if at least one of the components explains the Y. Thence H0 = p1 = p2 = p3 = pm, thence we want to reject this one. If not, then none of the predictors explain the Y, thus they are irregular components. \\[Q\\ =\\ n\\left(n+2\\right)\\sum_{k=1}^m\\frac{r_k^2}{n-k}\\] Where m is the number of lags to be tested. The Q statistic is commonly used for testing correlation in the residuals of a forecast model and the comparison is mate to \\(X^2_{m-q}\\), where q is the number of parameters in the model. 4.2.1.2 Random vs. correlated data Randomness is important for forecast model residuals. One can write simple random model, but we dont want complete randomness. Hence we don’t want patterns in our error, where the previous error can explain the next error. E.g. if the data contain trend or seasons, that we have not accounted for, then the errors will be able to predict the coming errors (can be tested by testing errors (residuals) against the lagged errors (residuals)). \\[Y_t=c+\\epsilon_t\\] Where c is the component and \\(\\epsilon_t\\) is the random error component. That is assumed to be uncorrelated period to period. 4.2.1.3 Stationary vs. non stationary data Stationary series is not trending, where is non stationary series is trending, can both be linear or exponential. The how is it solved? One can merely apply differencing of order k. That is equal to: \\[\\Delta Y_t=Y_t-Y_{t-1}\\] One could also apply growth rates are log differencing instead. "],["DataPatternsAndModelSelection.html", "4.3 Data Patterns and Model Selection", " 4.3 Data Patterns and Model Selection Here are some examples from the lectures Tend, no cycle, no seasonality Holt’s exponential Smoothing Linear regression with trend Trend, seasonality, cycle Winters’ exponential smoothing Linear regression with trend and seasonal adjustments Causal regression Time-series decomposition Non linear trend, no seasonality, no cycle Non linear regression with trend Causal regression Holt’s exponential smoothing Learn more about the methods in section 12.1, where a collection of performance measures can be found in section 12.2 The book (page 30 - 35) suggest the following application: Stationary date Naive method Simple Moving Averages Moving averages ARMA Data with a trend Moving Averages Holt’s exponential smoothing Simple regression ARIMA Seasonal data Classical decomposition Winter’s exponential smoothing Multiple regression ARIMA Data with Cycle factors Classical decomposition Mutliple regression ARIMA 4.3.1 Forecast Methods and Horizons Short term Regression Means Moving Averages Classical decomposition Trend projection ARIMA Intermediate term Regression Means Moving Averages Classical decomposition Trend projection ARIMA Long term Regression Classical decomposition Notice, that only regression in this constellation is an appropriate model for long term forecasts. The reasoning is that you are going to predict something in the future, and e.g., models based on stationary data, will tend towards the mean, that the data is centered around. Also if the forecast is based on other lagged variables, then you will need to forecast those variables, to be able to execute the forecasting model. Hence one must make sure that all of the models used for forecasting meets the assumptions and requirements, as they are prone to no be very accurate. Although one may apply long term predictions to construct an idea of how it may look, and then as you get close to what have previously been long term predictions, then you must constant iterate the model, hence throughout the intermediate and short term periods to capture the immediate variance. "],["exercises.html", "4.4 Exercises", " 4.4 Exercises This section contain exercises, hence the methods applied on data 4.4.1 p. 92 HW Problem 8 4.4.1.1 Moving Averages df &lt;- read_excel(&quot;Data/Week45/prob8p92HW.xlsx&quot;) yt &lt;- ts(df) #Rename and define as time series ts.plot(yt) #We can plot the We see that there is a trend in the data. We can calculate the five period moving average by: yt5c &lt;- ma(yt #The time series ,order = 5 #Amount of periods to be evaluated ,centre = TRUE #We want the center value of the MA ) yt5c ## Time Series: ## Start = 1 ## End = 8 ## Frequency = 1 ## [,1] ## [1,] NA ## [2,] NA ## [3,] 212.0 ## [4,] 216.0 ## [5,] 219.0 ## [6,] 221.2 ## [7,] NA ## [8,] NA Hence we are able to produce moving averages based on the data. Notice, that the most recent MA is the prediction, hence being \\(\\hat{Y}_{t+1}\\). One could extend this, by adding this value to the time series and then calculate MA for the period hereafter. We see that the output of the table above is somewhat misleading, as the most recent MA predictinos, are not positioned in the end, but instead where the center actually is. This problem is solvable using filter(). See the following chunk k &lt;- 5 #specify the order of the moving average c &lt;- rep (1/k,k) #remember that simple average is a weighted average with equal weights, #you need to specify weights for the filter command to work yt5&lt;- filter(yt, c, sides = 1) ts.plot(yt5)# &quot;Plotting the MA&#39;s Figure 4.4: 5k Moving Average yt5 #The updated vector of MA&#39;s ## Time Series: ## Start = 1 ## End = 8 ## Frequency = 1 ## [,1] ## [1,] NA ## [2,] NA ## [3,] NA ## [4,] NA ## [5,] 212.0 ## [6,] 216.0 ## [7,] 219.0 ## [8,] 221.2 This we see, that scores are moved to the end, so even though it is the center of the MA, it is now presented as recent values. For simple moving averages, one may do it in excel, it may be easier and quicker. 4.4.1.2 Exponential moving averages + Holts and Winters Simple exponential smoothing Where; alpha is the smoothing parameter, beta tells you if you should account for a trend or not, gamma is responsible for the presence of a seasonal component in the model fit &lt;- HoltWinters(yt ,alpha = 0.4 ,beta = FALSE ,gamma = FALSE) plot(fit,xlim = c(1,nrow(df))) + grid(col = &quot;lightgrey&quot;) ## integer(0) legend(&quot;topleft&quot;,c(&quot;Observed&quot;,&quot;Fitted&quot;),lty = 1,col = c(1:2)) Figure 4.5: Exponential Smoothing Hence we see the smoothed values, where the higher alpha, the more will the fitted line track the changes in the observations. We can now plot the forecast values: plot(forecast(fit),xlim = c(1,nrow(df)+10))# + grid(col = &quot;lightgrey&quot;) legend(&quot;topleft&quot;,c(&quot;Observed&quot;,&quot;Forecast&quot;),lty = 1,col = c(&quot;Black&quot;,&quot;Blue&quot;)) Figure 4.6: Forecast Exponential Smoothing One see the confidence intervals of the forecast widening as we get further away from the actual values. Now one may assess the accuracy: accuracy(forecast(fit)) ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 8.001509 8.209102 8.001509 3.673623 3.673623 2.154252 0.1962194 One see an RMSE of 8.2. Hence one could compare it with an exponential smoothing, which is more sensitive to the observations. fit0.6 &lt;- HoltWinters(yt ,alpha = 0.6 #Changed ,beta = FALSE ,gamma = FALSE) accuracy(forecast(fit0.6)) ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 5.860023 6.373698 5.860023 2.700235 2.700235 1.577698 0.264614 Where we see an RMSE of 6.37, hence lower than the initial test. Holt’s exponential smoothing exponential smoothing when a trend component is present: beta = TRUE {fit3 &lt;- HoltWinters(yt ,alpha = 0.6 ,beta = TRUE ,gamma = FALSE) plot(forecast(fit3)) accuracy(forecast(fit3))} Figure 4.7: Holt’s Exponential Smoothing ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set -1.990133 4.369207 3.451467 -0.9324372 1.581919 0.929241 0.4352197 We see that the RMSE is even lower (4.37). Which is expected, as Holt’s exponential smoothing accounts for trend. Winter’s Exponential Smoothing Which accounts for trend and seasonality in order to make it work one needs to define the frequency of your seasonal component, when specifying the ts data {yt &lt;- ts(df ,frequency = 4) # let&#39;s assume we suspect a quarterly pattern fit4 &lt;- HoltWinters(yt, alpha=0.6, beta=TRUE, gamma=TRUE) plot(forecast(fit4)) accuracy(forecast(fit4))} # experiment with seasonality frequency to see if you can get any lower in MSE ## ME RMSE MAE MPE MAPE MASE ## Training set -0.310075 3.973148 3.34445 -0.1227802 1.50834 0.2730163 ## ACF1 ## Training set 0.009251985 We see that the RMSE is even lower (3.97), hence Winters Exponential Smoothing appear to be the best model for prediction. 4.4.2 p. 93 HW Problem 9-10 Basically just another example of the exercise above. 4.4.3 CO2 and Sales Data Not done again. Do if time allows. 4.4.4 Case 6 oo. 108-111 HW Not done again. Do if time allows. "],["simple-and-multiple-linear-regression.html", "5 Simple and Multiple Linear Regression", " 5 Simple and Multiple Linear Regression This chapter elaborates on how linear regression may be applied to forecast data and also how we may get rid of trends when applying linear regression and assesing the assumptions for the model. Lastly, the chapter elaborates on how to chose a model Lectures 5th lecture - simple linear regression 6th lecture - multiple linear regression Literature HW: Simple Linear Regression How to write business reports - https://www.victoria.ac.nz/vbs/teaching/resources/VBS-Report-Writing-Guide-2017.pdf "],["simple-linear-regression.html", "5.1 Simple Linear Regression", " 5.1 Simple Linear Regression I will not go much in details with what simple linaer regression is. One can calculate the beta values by the following \\[\\begin{equation} b_0=\\ \\overline{Y}-\\ b_1\\overline{X} \\tag{5.1} \\end{equation}\\] \\[\\begin{equation} b_1=\\frac{\\sum_{ }^{ }\\left(X-\\overline{X}\\right)\\left(Y-\\overline{Y}\\right)}{\\sum_{ }^{ }\\left(X-\\overline{X}\\right)^{^2}} \\tag{5.2} \\end{equation}\\] Where point forecast, hence \\(\\hat{Y}\\) is merely the sum of the linear equation, hence \\(\\hat{Y}=b_0+b_1X^*\\), where \\(X^*\\) is the specific X values. Thence one can estimate the standard error by: \\[\\begin{equation} s_{yx}=\\sqrt{\\frac{\\sum_{ }^{ }\\left(Y-\\overline{Y}\\right)^{^2}}{n-2}} \\tag{5.3} \\end{equation}\\] Equations (5.3) can also be written otherwise, see the slides for that. The residuals can be broken down to the following \\[\\begin{equation} \\sum_{ }^{ }\\left(Y-\\overline{Y}\\right)^{^2}=\\sum_{ }^{ }\\left(\\hat{Y}-\\ \\overline{Y}\\right)^{^2}+\\sum_{ }^{ }\\left(Y-\\ \\hat{Y}\\right)^{^2} \\tag{5.4} \\end{equation}\\] Which consist of the following three elements. \\[\\begin{equation} SST = SSR + SSE \\tag{5.5} \\end{equation}\\] The residuals can then be applied for a goodness of fit assessment, where one can identify R squared- \\[\\begin{equation} R^2=\\frac{SSR}{SST} \\tag{5.5} \\end{equation}\\] So what can the linear regression then be used for? Inference Prediction Notice, that inference can only be done when the model is adequate, hence the assumptions actually being met. 5.1.1 Assumptions We have the following assumptions for a linear model: The underlying relationship between dependent and independent variable is actually linear Independent residuals Homoskedastic residuals (show constant variance) Identically distributed (In general, normal distribution is assumed) Hence how is the assumptions tested? Some can be done before analysis and others after the model is applied, hence it can be described by the following: Before the model is applied: The underlying relationship between dependent and independent variable is actually linear After the model is applied (doing diagnostics): Independent residuals Homoskedastic residuals (show constant variance) Identically distributed (In general, normal distribution is assumed) Now lets dive into the data Serial correlation and Heteroskedasticity Notice that autocorrelation = serial correlation Serial correlation is where the observations are trailing each other, where heteroskedasticity is where the variance is changing over timer: 5.1.1.1 Serial correlation (checking for independent residuals): We must make sure that the residuals does not have a clear pattern, as that means that some variables has been omitted. This can be assessed for example by: Visual inspection Durbin Watson test, see equation (5.6) Correlogram Statistical test for relationship between residuals and lagged residuals Figure 5.1: Serial Correlation Example Notice, that one should also test for autocorrelation in the errors, that can be done with a Durbin-Watson statistic: \\[\\begin{equation} DW\\ =\\frac{\\sum_{ }^{ }\\left(e_t-e_{t-1}\\right)^{^2}}{\\sum_{ }^{ }e_t^{^2}} \\tag{5.6} \\end{equation}\\] 0 &lt; DW &lt; 4, where if DW = 2 it indicates no serial correlation (this is the ideal), generally if 1.5 &lt; DW &lt; 2.5 is widely used as an acceptable level. If DW &gt; 2, it indicates negative serial correlation and if DW &lt; 2, it indicates that there is positive serial correlation. One could also use correlation testing by checking correlogram of residuals or testing residuals against lagged residuals Solution Try with lagging the variables 5.1.1.2 Heteroskedasticity: We want the variance to have a constant variance. This can be checked visually, where we dont want to see a funnel shape, as in the visualization below. Figure 5.2: Heteroskedasticity Example This can also be tested with a Breusch-Pagan Test, where the null hypothesis is that all errors are equal, hence null hypothesis, is that the errors are homoskedastic. Solution Try lagging the variables Try applying differences in the observations If one observe heteroskedasticity and can’t get rid of it, then one can apply a generalized least squares method. Although this introduce a set of new assumptions. 5.1.2 Forecasting with a linear trend If the data contain a linear trend, then we are able to make the model account for this trend. Hence we are able to detrend the data, by including a counter as a variable. Another solution may be to using the differences in the observations, e.g., using growth rates. This is important, as if one don’t detrend the data, then the model will merely describe the trend and not the actual values behind the trend, thus you want the model to account for the trend 5.1.3 Exercises 5.1.3.1 Problems 5 pp. 209 df &lt;- read_excel(&quot;Data/Week46/prob5p209.xlsx&quot;) plot(x = df$Age,y = df$MaintenanceCost,) + grid(col = &quot;lightgrey&quot;) + abline(reg = lm(MaintenanceCost ~ Age,data = df)) ## integer(0) coef(object = lm(MaintenanceCost ~ Age,data = df)) ## (Intercept) Age ## 208.20335 70.91813 If age is the dependent variable We see that the intercept is - 1.78, indicating that if there are no maintenance costs, then the age of the vehicle is -1.78 years old, which is naturally not possible, hence we see that the linear relathionship indicates that there will always be maintenance costs. If maintenance cost is the dependent variable We see that the intercept is 208, hence there will be a begin maintenance of 208, which will always be there, and then for each year, it is expected to increase with about 71. We can then test to see if the relationship is significant from a statistic point of view. lm(Age ~ .,data = df) %&gt;% summary() ## ## Call: ## lm(formula = Age ~ ., data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.6658 -1.0498 -0.7737 1.0125 2.0119 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.789563 1.268264 -1.411 0.201098 ## MaintenanceCost 0.012398 0.001737 7.139 0.000187 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.476 on 7 degrees of freedom ## Multiple R-squared: 0.8792, Adjusted R-squared: 0.862 ## F-statistic: 50.96 on 1 and 7 DF, p-value: 0.0001872 We see that there is statistical evidence to say that the relationship is linear. Also the coefficient of determination is 86% hence 86 of the variance is explained by the x variable. Now we can do the diagnostics: plot(lm(Age ~ .,data = df)) Before the model is applied: The underlying relationship between dependent and independent variable is actually linear, This we must assume After the model is applied (doing diagnostics): Independent residuals We dont say any indication that the residuals are not independent. Homoskedastic residuals (show constant variance) The variance appear to be constant Identically distributed (In general, normal distribution is assumed) This we must assume rm(list = ls()) 5.1.3.2 Problem 11 p. 212 HW 5.1.3.3 Cases 2 HW Notice that the X is deviations from 65 degrees, as 65 degrees is the ideal for the production, hence when one read 10 degrees, then it is in fact 75 degrees or 55. Notice that the deviation is in absolut values. df &lt;- read_excel(&quot;Data/Week46/Case2p222.xlsx&quot;) #head(df) #To see the first observations in each Y &lt;- df$Y X &lt;- df$X scatter.smooth(x=X, y=Y, main=&quot;Y ~ X&quot;) Figure 5.3: Plotting the model cor(Y, X) ## [1] -0.8010968 We see that the correlation is negative, hence -0.8 linMod &lt;- lm(Y ~ X) #To omit intercept when necessary, the formula can be written for example as lm(cost ~ age - 1) summary(linMod) ## ## Call: ## lm(formula = Y ~ X) ## ## Residuals: ## Min 1Q Median 3Q Max ## -91.38 -43.83 -12.64 44.48 122.25 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 552.040 22.851 24.158 &lt; 2e-16 *** ## X -8.911 1.453 -6.133 4.37e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 59.41 on 21 degrees of freedom ## Multiple R-squared: 0.6418, Adjusted R-squared: 0.6247 ## F-statistic: 37.62 on 1 and 21 DF, p-value: 4.374e-06 We see an R square of 62.47 indicating that the linear relationship is not describing the relationship in the sample data very well, which the illustration above also show quite well. plot(resid(linMod)) Figure 5.4: Residuals Case 2 Now we can assess if the data show autocorrelation. That can be done by using the acf() acf(resid(linMod)) # white noise residuals? Figure 5.5: Correlogram The data appear to show white noise, although there appear to be some pattern, which may show seasons in the data. We can test to see if the residuals actually show constant variance, of the variance is not constant (heteroskedasticity) bptest(linMod) # Breusch-Pagan test H_0: variance is constant. ## ## studentized Breusch-Pagan test ## ## data: linMod ## BP = 1.069, df = 1, p-value = 0.3012 Since the p value is not significant, there is not enough evidance to reject the null hypothesis, hence we may assume that the residuals show constant variance. { AIC(linMod) %&gt;% print() BIC(linMod) %&gt;% print()} ## [1] 257.0604 ## [1] 260.4669 We see the different information criteria, but we need other models to assess what is good and what is bad. Q1 How many units would your forecast for a day in which the high temperatire is 80 degrees a &lt;- data.frame(X=24) #65+24=89 degrees predict(linMod, a) %&gt;% print() ## 1 ## 338.1778 Hence we may expect 338 units to be produced on a day with 89 degrees. Q2 When the degrees is 41, hence also a deviation of 24 degrees. Q3 Is the forecasting tool effective? We saw earlier that 80% of the variance is explained, although there is probably room for optimization rm(list = ls()) 5.1.3.4 Case 3 from HW df &lt;- read_excel(&quot;Data/Week46/Case2p222.xlsx&quot;) #head(df) #To see the first observations in each Y &lt;- df$Y X &lt;- df$X plot(X,Y) cor(Y, X) ## [1] -0.8010968 We see that the correlation is negative, hence -0.8 cor.test(Y,X) ## ## Pearson&#39;s product-moment correlation ## ## data: Y and X ## t = -6.1335, df = 21, p-value = 4.374e-06 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.9121082 -0.5806250 ## sample estimates: ## cor ## -0.8010968 5.1.3.5 Detrending thorugh regression: CO2 CO2levels &lt;- read_excel(&quot;Data/Week46/CO2levels(1).xlsx&quot;) y &lt;- ts(CO2levels, frequency = 12) #Monthly series, so we specify the frequency=12. trend &lt;- seq(1:length(y)) #Creating the linear trend, simply a counter plot(y) Figure 5.6: CO2 data We see that there is a trend and cycles. This we want to get rid of, by enabling the model to account for that. fit &lt;- lm(y ~ trend) summary(fit) ## ## Call: ## lm(formula = y ~ trend) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.392 -1.866 0.199 2.190 3.677 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.293e+02 3.462e-01 951.40 &lt;2e-16 *** ## trend 1.210e-01 3.707e-03 32.64 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.186 on 159 degrees of freedom ## Multiple R-squared: 0.8701, Adjusted R-squared: 0.8693 ## F-statistic: 1065 on 1 and 159 DF, p-value: &lt; 2.2e-16 ystar &lt;- resid(fit) #ystar is the detrended series, which is simply the residuals from the previous regression. plot(ystar) + lines(ystar, col=&quot;red&quot;) Figure 5.7: Detrended series ## integer(0) Now wee see that the data show constant variance and appear to be stationary around a mean of 0. But did we get rid of the seasonality? To compare, one can print the detrended and the initial data. par(mfrow = c(2,1)) acf(y, 50) acf(x = ystar,lag.max = 50) #We can specify how many lags we want to plot. Here I just chose 50. Figure 5.8: Correlogram comparison detrended Hence we see that we got rid of the trend, but still see that there is great seasonality in the data. Alternative to detrending –&gt; differencing An alternative to detrending with the trend variable, is to using differencing dy &lt;- diff(x = y,lag = 1) plot(dy) # Figure 5.9: Detrending using differencing acf(dy, 50) Figure 5.10: Detrending using differencing "],["multiple-linear-regression.html", "5.2 Multiple Linear Regression", " 5.2 Multiple Linear Regression Simply regression with more than one independent variable. The diagnostics tools are the same as before, significance testing, R square, DW stats, residuals diagnostics. 5.2.1 Multicollinearity Although in MLR one must be aware of multicollinearity, meaning that do we see a strong relationship between independent variables, hence are they explaining the same? To assess for multicollinearity one can apply VIF, which is the following: \\[\\begin{equation} VIF_j=\\frac{1}{1-R_j^2} \\tag{5.7} \\end{equation}\\] Where \\(j = 1,...,k\\) Thus, we see that Rsquare is obtained from regression each IDV against the remaining variables. We can then have the following outputs: VIF = 1, no milticollinearity VIF &gt; 10, indicates multicollinearity If one gets an indication of multicollinearity, then one should drop one of the correlated variables. 5.2.2 Serial correlation and omitted variables When doing regression, we may observe that the IDVs are correlated with the error term, meaning that the errors are not randomly distributed, hence serial correlation in the error terms. For serial correlation in the error terms, we are able to make use of the Durbin-Watson, see (5.6). 5.2.3 Selection criteria We cannot use R square anymore, as it will never really penalize when we are adding variables. Hence one should use&gt; AIC BIC Depending on whether one is interested in the best model for prediction or the true model. "],["time-series-decomposition-and-regression-with-time-series-data.html", "6 Time-Series Decomposition and Regression with Time-Series Data", " 6 Time-Series Decomposition and Regression with Time-Series Data This chapter identifies what elements a time series can be broken into, hence it elaborates on the trend, seasonality, cycles and irregular movements Literature HW: Time Series and Their Components HW: Regression with Time Series Data "],["TSComponents.html", "6.1 Time Series and Their Components (HW)", " 6.1 Time Series and Their Components (HW) Basically a Time Series is variables that are collected over time. The variables are highly likely to have autocorrelation Autocorellation: Variables are automatically dependant on each other over time, and the mere aspect of these synergies (patterns) where one will often be able to prove correlation between the variables One approach to assessing time series is by decomposing the patterns by finding the components hereof, these are: Trend(T): if it is linear, then it can be explained by \\(\\hat{T}_t=\\beta_0+\\beta_1t\\), hence we apply the linear function, hence what in statistics is \\(\\hat{y}=\\hat{T}\\) in time series Cyclical(T or C)Note, often included in practice as the trend, as it can be difficult to extinguish Seasonal(S) Irregular(random)(I) - Notice! We dont want to use this for assembling the model, as it is random These are also called deterministic variables The purpose of decomposing the time series data, can be either for exploration or prediction. Hence you can estimate the coefficients of the components by breaking down the data. Although the typical purpose of time series is exploration of the data and assess if there are seasons, trends etc. and perhaps to pinpoint whether you are above or below the season/trend/cycle. If the Y observations is the sum of the components, then we have additive model, if they are the product of the components, then it is called multiplicative model Time series is typically an additive model, if the variance is more or less the same, it is a multiplicative model if the variance increases with time Note, one can transform a multiplicative model to an additive model by taking the logarithm 6.1.1 Additional on trend Quadratic trend e.g. where we have curvature \\[\\hat{T}_t=\\beta_0+\\beta_1t+\\beta_1t^2\\] Exponential trend e.g. exponentially growing population \\[\\hat{T}_t=\\beta_0*\\beta_2^t\\] NOTE, one may transform this into a logistic trend instead, as continuous exponential trend is not typical 6.1.2 Additional on seasonal pattern One can manually rule out seasonality by adding seasonal index (that is hard coding the expected index in the respective periods) One must rule out other factors before doing this! Seasonally adjusted data For additive \\[Y_t - S_t = T_t + I_t\\] For multiplicative \\[\\frac{Y_t}{S_t} = T_t * I_t\\] One does often take out seasonality to better compare data and also create short term forecasts. 6.1.3 Cyclical and Irregular Variations One can often rule out (or at least smooth out) irregularities by taking the moving average "],["methods-of-decomposing.html", "6.2 Methods of decomposing", " 6.2 Methods of decomposing There are three approaches to decomposing (probably more). Decomposing, with the following approach: Deseasonalizing using seasonal dummies Detrending using a trend variable Decomposing with the following: Deseasonalizing using MA (moving averages not ARMA). Where you apply the season, e.g., quarterly, then 4 periods b.Detrending using a trend variable Using decompose() a. Note: this use moving averages, see the documentation "],["regression-with-time-series-data.html", "6.3 Regression with time series data", " 6.3 Regression with time series data One of the assumptions for regression models, is that the errors are independent (uncorrelated), THAT IS RARELY THE CASE WITH TIME SERIES. Hence one must be very precautions. "],["the-success-criteria-and-process.html", "6.4 The success criteria and process", " 6.4 The success criteria and process The following elaborates on success criteria and the process 6.4.1 Success Criteria Ultimately we want to be able to answer the following: Do we have trend? Do we have cyclical movements? Do we have seasons? Do we have autocorellation (elaborated in section ??)? If yes: If RHO = 1, then we can take first differences If RHO &lt;&gt; 1, then we can do the generalized differences., thus implies the following: Do an OLS and get the residuals Use the residuals in the following equation \\(e_t=\\rho e_{t-1}+\\sigma_t\\), using OLS as estimated rho (\\(\\hat{\\rho}\\)) If rho = 0, then 0 autocorrelation 6.4.2 The Process Deseasonalizing and detrending based on moving averages and accounting for cyclical moves Note, if you do not have seasonality, then jump to section 2. trend etc. 6.4.2.1 Desaesonalizing Remove the short-term fluctuations If we have even number of periods, one must center the data. Whith odd period numbers, you can merely center with the period in the middle. The procedure with even number of periods is the following: 1. Find the MA_t with equation (6.1) and (6.2) \\[\\begin{equation} MA_t = \\frac{(Y_{t-2}+Y_{t-1}+Y_{t}+Y_{t+1}+)}{4} \\tag{6.1} \\end{equation}\\] and \\[\\begin{equation} MA_{t+1} = \\frac{(Y_{t-1}+Y_{t}+Y_{t+1}+Y_{t+2}+)}{4} \\tag{6.2} \\end{equation}\\] Note, that the MA for each is centered in the center and rounded up to the coming period 2. Then find the centered MA Then do the average of the two periods, which will find the actual center: \\[\\begin{equation} CMA_t=\\frac{(MA_t+MA_{t+1})}{2} \\tag{6.3} \\end{equation}\\] Notice, that the example is with quarterly MA, hence the MAs are divided with 4, this could have been monthly and then onw would divide by 12. \\(CMA_t\\) is representing the depersonalized data. One can find the seasonal factor by saying \\[\\begin{equation} SF_t=\\frac{Y_t}{CMA_t} \\tag{6.4} \\end{equation}\\] This explains whether one as above or below the expect season level. CONCLUSION: IF \\(SF_t &gt; 1\\), then Y is greater than the quarterly (or what other period is used) average or, \\(SF_t &lt; 1\\), then the Y is less than the quarterly average Alternative, deseasonalizing data can be done by dividing the raw data with some seasonal index, that is adding dummy variables for the periods. Although by using the index, one assume, that the same seasonality is the same as preivous periods. 6.4.2.2 Long-term trend Long-term trend, this is estimated from the deseasonalized data. This is estimated using simple linear regression. Basically the detrended data consists of the residuals between the actual data and the estimated data by using the trend variable (the counter 1 to n). Task 1 We must find out if the trend is linear or quadratic. Linear: \\(C\\hat{M}A_t=f(t)=\\beta_0 + \\beta_1t\\) Quadratic: \\(C\\hat{M}A_t=f(t)=\\beta_0 + \\beta_1t+\\beta_2t^2\\) Where \\(t\\) is the time indicator and 1 = the first observation and increases by 1 thereafter. Now we have obtained the centered moving-average trend \\[\\begin{equation} CMAT = C\\hat{M}A \\tag{6.5} \\end{equation}\\] 6.4.2.3 Cyclical Component Cyclical component, one can compare the CMA with the CMAT to find the cyclical factor. Thus, the cyclical factor is: \\[\\begin{equation} CF = \\frac{CMA}{CMAT} (\\#CF) \\end{equation}\\] If CF &gt; 1, the deseasonalised value is above the long-term trend of the data. If the opposite, then below. 6.4.2.4 Time-Series decomposition forecast Now we can do the reverse procedure, using the factors, that we have just found. The reverse procedure is assembling the predicted Y based on the factors that have just been found. \\[\\begin{equation} \\hat{Y}=CMAT*SI*CF*I \\tag{6.6} \\end{equation}\\] Where, CMAT = T, S is the SF, CF is the CF and I is the irregular component (this is assumed to be 1 given its random nature, if one expects a boom or shock, this can be modeled with) 6.4.3 Autocorellation ALWAYS ALWAYS CHECK FOR AUTOCORRELATION. What to do? You are missing some variable, find the missing variable(s). In practice this can be very difficult. Do differencing Use autoregressive model approach, where you are using lagged variables as variables to predict the coming period We are going to talk about ARIMA (The box and jenkins methodology, more about this in section 7) If our residuals have autocorelation, it means that there is some relationship in the model, that our model does not account four. In worst case, we can end up ‘proving’ some relationships between variables, that are in fact not true, but it is rather autocorellation that is proving the model, and the not the relationship between the variables and the dependent variables. "],["exercises-2.html", "6.5 Exercises", " 6.5 Exercises The section contain several exercises covering: Decomposing and forecasting without cycles Decomposing and forecasting with cycles Time Series regression with other variables Fitting trend variable Decomposing using OLS, decompose and CMA 6.5.1 Decomposition + Forecast Exercise We use the Alomega Foodstore sales data to show the following: How to make a decomposition of a time series, using decompose() How to use decompose() to retrieve the different components and store these How to inteprete the components How to forecast using the components y &lt;- read_excel(&quot;Data/Week47/Alomegafoodstores.xlsx&quot;) y &lt;- y$Sales y &lt;- ts(data = y #The data ,frequency = 12 #Data is monthly ,end = c(2006,12) ) options(scipen = 999) tsdisplay(y) It appears as if we have seasonality, but a trend is dificult to say. It appears as if the data is additive, as the variance appear to be more or less the same over time. 6.5.1.1 a. Decomposing the time series Now we can decompose the elements using decompose(), where moving averages are applied to deseasonalize the data. But just for the sake of it, the multuplicative decomposition is made as well. y.decom.add &lt;- decompose(x = y #The ts ,type = &quot;additive&quot; #The composition ) y.decom.mult &lt;- decompose(x = y #The ts ,type = &quot;multiplicative&quot; #The composition ) plot(y.decom.add) 6.5.1.2 b. Retrieving components Recall that we have the following components: Trend (T), Tr in this example, as t is the transpose function Seasons(S) Irregular(I) - NOTICE: We don’t want to use this for forecasting, as it is random, hence it can be left out from the assembly yhat We see no cycles, and assume that they are not there. Tr &lt;- y.decom.add$trend S &lt;- y.decom.add$seasonal I &lt;- y.decom.add$random 6.5.1.3 c. Interpreting the resutls For the sake of it, the additive and multiplicative decompositions are plotted. plot(y.decom.add) plot(y.decom.mult) We see that the decomposition finds a trend and also it appears as there are some reoccurring patterns in the seasons. But notice that the seasonal component is different, now it is a factor, which is relative to the trend. 6.5.1.4 d. Forecasting using the components 6.5.1.4.1 Fitting the model We see that we are able to construct the in sample data 1:1 if the composition is created with all three components. Let us forecast only using the trend and season component. We see that we area able to plot the following: yhat&lt;-Tr+S #Notice, that I is left out, as it is a random component plot(y, type=&quot;l&quot;) lines(Tr,col=&quot;green&quot;) lines(yhat,col=&quot;red&quot;) We can assess the accuracy of the composition of the trend and seasons. accuracy(yhat,y) ## ME RMSE MAE MPE MAPE ACF1 Theil&#39;s U ## Test set -3913.472 56659.88 43104.6 -2.781556 11.18807 -0.09662767 0.2250085 We see that the mean percentage error is -2,78 percent, hence it appears as there is a tendency for the model to be underestimating y. 6.5.1.4.2 Forecasting using the composition Now we can apply the model in a forecasting setting to forecast h horizons. forecast.decom.add &lt;- forecast(object = yhat,h = 18) t(t(forecast.decom.add$mean)) ## [,1] ## [1,] 446496.7 ## [2,] 320351.2 ## [3,] 541732.4 ## [4,] 380127.7 ## [5,] 361224.6 ## [6,] 262271.3 ## [7,] 635257.0 ## [8,] 317322.8 ## [9,] 598609.9 ## [10,] 362402.8 ## [11,] 526939.7 ## [12,] 414048.1 ## [13,] 464803.6 ## [14,] 338658.2 ## [15,] 560039.4 ## [16,] 398434.7 ## [17,] 379531.6 ## [18,] 280578.3 18 months are chosen, as the function forecast from the middle of 2006, where it is was not able to fit the components, as it applies CMA on the months. Using 18 months, we get forecasts for the last half a year and there after a whole year We can also plot the forecast plot(forecast.decom.add) We can assess the accuracy of the first 6 forecasted periods, as they are known y.test &lt;- y[(length(y)-5):length(y)] accuracy(object = forecast.decom.add$mean[1:6],x = y.test) ## ME RMSE MAE MPE MAPE ## Test set 36881.52 90588.21 60504.58 3.564348 13.03319 We see that the accuracy of these periods are 3.56% (MPE). rm(list = ls()) 6.5.2 Decomposition with cycles + Forecasting Process: y &lt;- read_excel(&quot;Data/Week47/Alomegafoodstores.xlsx&quot;) y &lt;- y$Sales y &lt;- ts(data = y #The data ,frequency = 12 #Data is monthly ,end = c(2006,12) ) 6.5.2.1 1. Finding CMA (deseasonalizing) CMA &lt;- ma(x = y #The data ,order = 12 #When the same period occurs ,centre = TRUE) #As we have an even period We have now deseasonalized the data, as the moving average has removed the effect of the seasons. 6.5.2.2 2. Finding SF That is, identifying whether we are above or below the deseasonalized data. SF&lt;-y/CMA plot(SF, type=&quot;l&quot;) abline(h = 1 #Plotted, as when one is on 1, then you are neither above or below ,col = &quot;red&quot;) 6.5.2.3 3. Identifying Trend We introduce a trend variable to take out the effect of the trend trend &lt;- seq(1:length(y)) #Making counter linmod &lt;- lm(CMA ~ trend ,na.action = &quot;na.exclude&quot;) Notice that we don’t care about the model, we just need the fitted values for identifying the cycle factor. 6.5.2.4 4. Finding detrended and deseasonalized data In step 1 we deseasonalized the data, based on the fitted model, we have detrended the CMAT &lt;- linmod$fitted.values 6.5.2.5 5. Cycle identifying factor We the deseasonalized and detrended data, we are able to idenitify if the deseasonalized data is above or below the seasons. CF &lt;- na.exclude(CMA) / CMAT ts.plot(CF) abline(h = 1,col = &quot;red&quot;) Figure 6.1: Cycle Factor We want to look for wavelike patterns. We don’t see any, hence it is fair to assume, that we don’t have any cycles. If we had, what to do? When multiplicative: One can apply the cycle factor in the regression When additive: One can apply the difference (between CMA and CMAT) in the regression. End of exercise: rm(list = ls()) 6.5.3 Time series regression This example show how one can apply other variables in a regression Alomegasales &lt;- read_excel(&quot;Data/Week47/Alomegafoodstores.xlsx&quot;) y &lt;- read_excel(&quot;Data/Week47/Alomegafoodstores.xlsx&quot;) y &lt;- y$Sales y &lt;- ts(data = y #The data ,frequency = 12 #Data is monthly ,end = c(2006,12) ) #fetch explanatory variables paper&lt;-ts(Alomegasales$Paper, frequency=12) tv&lt;-ts(Alomegasales$TV, frequency=12) month &lt;- seasonaldummy(y) #creates seasonal dummies, if such not available in the data #Regression model for sales reg &lt;-lm(y~paper+tv+month) summary(reg) ## ## Call: ## lm(formula = y ~ paper + tv + month) ## ## Residuals: ## Min 1Q Median 3Q Max ## -88551 -23745 -951 21590 107195 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 184393.15345 23401.84900 7.879 0.000000003560 *** ## paper 0.36319 0.06854 5.299 0.000007029523 *** ## tv 0.31530 0.03638 8.667 0.000000000398 *** ## monthJan 200847.13731 39150.70001 5.130 0.000011646830 *** ## monthFeb 55491.47382 32399.34749 1.713 0.095868 . ## monthMar 199556.46868 34147.06238 5.844 0.000001372948 *** ## monthApr 100151.42985 32387.77669 3.092 0.003952 ** ## monthMay 190293.03783 32822.15893 5.798 0.000001577161 *** ## monthJun 135441.04479 32580.72319 4.157 0.000206 *** ## monthJul 156608.64859 32699.04733 4.789 0.000032157745 *** ## monthAug 51585.64542 32419.67959 1.591 0.120825 ## monthSep 183619.40275 36521.78560 5.028 0.000015816441 *** ## monthOct 109096.18894 32438.67351 3.363 0.001919 ** ## monthNov 96205.98997 32416.66110 2.968 0.005461 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 45800 on 34 degrees of freedom ## Multiple R-squared: 0.9083, Adjusted R-squared: 0.8732 ## F-statistic: 25.9 on 13 and 34 DF, p-value: 0.00000000000007305 We see the model summary above. Now we can make the regression #obtain residuals from the model to perform model quality checks res&lt;-reg$residuals plot(res) We see that the residuals appear homoskedastic. We can check them for normality #check for normality hist(res) By visual inspection, they appear to be normally distributed. library(tseries) jarque.bera.test(res) #Jarque-Bera test the null of normality ## ## Jarque Bera Test ## ## data: res ## X-squared = 1.0947, df = 2, p-value = 0.5785 H0: Normality Cannot reject. Now we can check for multicollinearity #testing for multicollinearity library(car) vif(reg) ## GVIF Df GVIF^(1/(2*Df)) ## paper 2.054002 1 1.433179 ## tv 1.559810 1 1.248924 ## month 2.581443 11 1.044049 We see that they are all ≈ 2 or less, hence no problem We can check the residuals for autocorrelation (serial correlation). We dont want this #serial correlation #acf(res) plot(acf(res,plot=F)[1:20])#alternative command if you want to skip the 0 lag Despite a few significant spikes, it appears as if it is not autocorrelated. This can be tested with the Durbin Watson test. #Durbin-Watson serial correlation test library(lmtest) dwtest(reg) ## ## Durbin-Watson test ## ## data: reg ## DW = 2.2742, p-value = 0.8231 ## alternative hypothesis: true autocorrelation is greater than 0 H0: No autocorrelation We cannot reject on the five percent level 6.5.4 Alomega Food Stores, case 6 p. 166 + case 7 p. 348 (Another example) y &lt;- read_excel(&quot;Data/Week47/Alomegafoodstores.xlsx&quot;) str(y) ## tibble [48 × 21] (S3: tbl_df/tbl/data.frame) ## $ Sales : num [1:48] 425075 315305 432101 357191 347874 ... ## $ Paper : num [1:48] 75253 15036 134440 119740 135590 ... ## $ TV : num [1:48] 114433 63599 64988 66842 39626 ... ## $ Month : num [1:48] 1 2 3 4 5 6 7 8 9 10 ... ## $ Dum1 : num [1:48] 1 0 0 0 0 0 0 0 0 0 ... ## $ Dum2 : num [1:48] 0 1 0 0 0 0 0 0 0 0 ... ## $ Dum3 : num [1:48] 0 0 1 0 0 0 0 0 0 0 ... ## $ Dum4 : num [1:48] 0 0 0 1 0 0 0 0 0 0 ... ## $ Dum5 : num [1:48] 0 0 0 0 1 0 0 0 0 0 ... ## $ Dum6 : num [1:48] 0 0 0 0 0 1 0 0 0 0 ... ## $ Dum7 : num [1:48] 0 0 0 0 0 0 1 0 0 0 ... ## $ Dum8 : num [1:48] 0 0 0 0 0 0 0 1 0 0 ... ## $ Dum9 : num [1:48] 0 0 0 0 0 0 0 0 1 0 ... ## $ Dum10 : num [1:48] 0 0 0 0 0 0 0 0 0 1 ... ## $ Dum11 : num [1:48] 0 0 0 0 0 0 0 0 0 0 ... ## $ Paper1: num [1:48] 220 75253 15036 134440 119740 ... ## $ Paper2: num [1:48] 5 220 75253 15036 134440 ... ## $ TV1 : num [1:48] 88218 114433 63599 64988 66842 ... ## $ TV2 : num [1:48] 76001 88218 114423 63599 64988 ... ## $ M01-48: num [1:48] 1 2 3 4 5 6 7 8 9 10 ... ## $ Compet: num [1:48] 1 2 2 1 2 3 3 2 3 3 ... We see that there are 21 variables, where sales is the DV and all others are IDVs, consisting of continous and factors. Now we are interested in constructing a time series, which consist of the dependent variable. That is done in the following: y &lt;- ts(data = y$Sales #The dependent variable ,end = c(2006,12) #The end date ,frequency = 12 #The frequency, 12 as we are working with months ) options(scipen = 999) plot(y) Figure 6.2: Time series Alomega Food Now we want to address if there is: Trend: visually it does not look like. But it will be tested by testing the time series against a trend variable Seasons: Cycles: THis is difficult to say, also as we only have data for four years Trend #Creating trend variable trend &lt;- seq(from = 1,to = length(y),by = 1) #Creating a linear model with the trend variable lm.fit &lt;- lm(y ~ trend) summary(lm.fit) ## ## Call: ## lm(formula = y ~ trend) ## ## Residuals: ## Min 1Q Median 3Q Max ## -200465 -84502 -13613 71381 333683 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 350840 37726 9.300 0.00000000000389 *** ## trend 1335 1340 0.996 0.325 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 128600 on 46 degrees of freedom ## Multiple R-squared: 0.0211, Adjusted R-squared: -0.0001811 ## F-statistic: 0.9915 on 1 and 46 DF, p-value: 0.3246 We see that the trend variable appear to be non significant, hence there is not enough evidence to reject the null hypothesis, being that there is no relationship between the dependent- and independent variable. accuracy(object = lm.fit #The fitted values from the linear model ,y = y #The actual value ) ## ME RMSE MAE MPE MAPE MASE ## Training set -0.000000000002425023 125940.3 99341.6 -10.69473 28.49834 0.992719 We see the RMSE of 125.940, this can also be plotted to see the fitted values against the residuals, where the mean absolute error is just below 100.000 units. Notice, that this test in done in sample. plot(x = lm.fit$fitted.values,y = lm.fit$residuals,main = &quot;Resduals plot&quot;) + grid(col = &quot;lightgrey&quot;) + abline(h = 0,lty = 3,col = &quot;blue&quot;) Figure 6.3: Residuals plot ## integer(0) 6.5.4.1 What might Jackson Tilson say about the forecasts? With mean average percentage error of 28%, the forecast may not be super good. Although it was shown, that there is statistical evidence for the model being significant. Hence, we must try to convince him, that it does in fact contribute with some knowledge. What other forecast methods could be used? E.g., smoothing or moving averages. 6.5.4.2 P.348 Case 7 (Alomega Food Stores) This is an extension of the case above. It has questions regarding what other things she might do. How is Julie sure that it is the right predictors We see on page 348, that some of the variables has a p-vale that is above the 5 percent level, hence there is a greater risk, that there is no statistical evidence for a relationship between the individual variables and the dependent variable. Also changes in VIF for the variables, although none are close to 10 and according to the rule of thumb, there is no indication of multicollinearity How would you sell the model Focus on how it prepares them for future changes, planning staffing, procurement, etc. How may the model indicate future advertizing? We see that it makes sense to spend money on all three types of advertisements. What conditions to be aware of? Autocorrelation in the error terms? Multicollinearity Other models that could be used? ARIMA Smoothing rm(list = ls()) 6.5.5 Monthly Sales Data Loading the data and storing only the y variable as a timeseries df &lt;- read_excel(&quot;Data/Week47/Salesdataseasonal.xlsx&quot;) y &lt;- ts(df$Sales ,frequency = 12 #Monthly data ,start = c(1979,1) #start year 1971 month 1 ) #Plotting for visual interpretation plot(y) Figure 6.4: Visual interpretation Goal: Seasonal dummies Trend variable Hereafter different approaches to decomposing data is presented OLS using seasonal dummies and trend variable Decomposition using decompose() Using CMA (centered moving averages) a. finding seasonal dummies Finding seasonal dummies using a trend #Creting matrix of dummies for each month month &lt;- seasonaldummy(x = y) This can be shown with the following table Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 We see that a matrix has been created identifying what month each period belongs to. b. finding the trend variable In addition hereof, we must create a trend variable to account for the trend: trend &lt;- seq(1,to = length(y),by = 1) 1. OLS using seasonal dummies and trend variable #Constructing linear model lmmod &lt;- lm(y ~ month + trend) summary(lmmod) ## ## Call: ## lm(formula = y ~ month + trend) ## ## Residuals: ## Min 1Q Median 3Q Max ## -41.782 -17.094 -3.591 15.140 91.349 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 84.01705 8.51294 9.869 &lt; 0.0000000000000002 *** ## monthJan 5.78164 10.54471 0.548 0.5844 ## monthFeb 42.89804 10.54352 4.069 0.00008250119327 *** ## monthMar 79.84777 10.54259 7.574 0.00000000000656 *** ## monthApr 76.88084 10.54192 7.293 0.00000000002882 *** ## monthMay 25.49724 10.54153 2.419 0.0170 * ## monthJun -13.05303 10.54139 -1.238 0.2179 ## monthJul -49.51997 10.54153 -4.698 0.00000673141501 *** ## monthAug -23.23690 10.54192 -2.204 0.0293 * ## monthSep -17.94010 10.76929 -1.666 0.0982 . ## monthOct -28.56613 10.76864 -2.653 0.0090 ** ## monthNov 5.80784 10.76825 0.539 0.5906 ## trend 2.71693 0.05288 51.383 &lt; 0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 25.25 on 127 degrees of freedom ## Multiple R-squared: 0.9586, Adjusted R-squared: 0.9547 ## F-statistic: 245 on 12 and 127 DF, p-value: &lt; 0.00000000000000022 What can we deduct from this? January, June, September and November are not significant on the 5% level in the full model. Hence the dummies may not explain the sales for these months There is overwhelming evidence for trend being present in the data Lets now look into the fitted values and plot {#Plotting fitted values plot(lmmod$fitted.values,main = &#39;Fitted values&#39;,ylab = &#39;Sales&#39;) + lines(lmmod$fitted.values,col = &#39;red&#39;) + grid(col = &#39;lightgrey&#39;) #Plotting residuals plot(lmmod$residuals,main = &#39;Residuals&#39;,ylab = &#39;Residuals&#39;) + lines(lmmod$residuals,col = &#39;green&#39;) + grid(col = &#39;lightgrey&#39;)} ## integer(0) 2. Decomposition using decompose() The following is able to decompose the series based on assuming additivity and multiplicative relationship. Although as we see that the variance is increasing over time, then one may expect that the multiplicative approach is more appropriate. #Additive decompSalesAdd &lt;- decompose(x = y #The timeseries ,type = &quot;additive&quot;) #for additive decomposition #Plotting plot(decompSalesAdd) Figure 6.5: Additive decomposition #Multiplicative decompSalesMult &lt;- decompose(x = y #The timeseries ,type = &quot;multiplicative&quot;) #for multiplicative decomposition #Plotting plot(decompSalesMult) 3. CMA see process in 6.4.2 Find the MA_t with equation (6.1) and (6.2) = Deseasonalizing data #Finding centered MA CMA &lt;- ma(x = y #The time series ,order = 12 #We want the average of 12 periods ,centre = TRUE) #As order is equal, then we want to make a centered MA plot(CMA) Figure 6.6: CMA We see that the centered moving averages have removed the seasons from the data. But there is clearly still a trend. We are now able to estimate the trend using the deseasonalized data with with trend variable. Just as we would deseasonalize any other data!! Then find the centered MA (CMAT) = detrending data #Detrending data linmod &lt;- lm(CMA ~ trend ,na.action = &quot;na.exclude&quot;) #We want to remove rows with NAs # NOTE: we dont really care about the model, we only want the fitted values # as they represent the estimated linear trend #Estimated trend CMAT &lt;- linmod$fitted.values #Extracting trend estimates and saving in an object plot(CMAT) Figure 6.7: Cycle With the deseasonalized data and the trend estimates, we are able to assess whether we are above or below the trend in all of deseasonalized data. The cycle #Identifying the cycle Cycle &lt;- na.exclude(CMA) / CMAT ts.plot(Cycle) abline(h=1, col = &quot;red&quot;) Figure 6.8: Cycle Now we see the whether we are above or below the trend in the respective periods, e.g., period 60 appear to be far below the trend level indicating it is a the lowest of the seasons. "],["ARIMA.html", "7 ARIMA Models + ADL and Box-Jenkins Methodology", " 7 ARIMA Models + ADL and Box-Jenkins Methodology Considering a single time series \\(Y_t\\), the interest is in determining the relationship between \\(Y_t\\) and its past. Literature HW: The Box-Jenkins (ARIMA) Methodology ADL: [https://www.econometrics-with-r.org/14-5-apatadlm.html Additional on ADL: http://faculty.washington.edu/ezivot/econ584/stck_watson_var.pdf Note, just the intuitions behind are important, not the over-technical details or derivations. go ARIMA or ADL or VAR?? ARIMA does univariate assessment (only one variable) –&gt; \\(ARIMA_{(p,d,q)}\\) ADL does multivariate assessment while also doing AR on the dependent variable, hence we are able to assess effects from another lagged variables. Note, this only takes on one other variable –&gt; \\(ADL_{(p)}\\) So, what to do if we have more variables we want to include? Then we do VAR (Vector AuroRegressive mdoel) with With VAR, we are able to see relationship between several variables, and do forecast for each of the variables with p order lags, both from the other variables and also the variable itself –&gt; \\(VAR_{(p)}\\) "],["ARIMAΗ2.html", "7.1 ARIMA", " 7.1 ARIMA Basically it is a framework for adding AR and or MA into the regression model, where: AR: A time-series is predicted using its own history, can be explained by: \\(Y_t=β_0+β_1Y_{t-1}+ε_1\\) MA: Model predicts based on current and past shocks to the series. An MA(q) model: \\(Y_t=\\mu+\\epsilon_t-\\omega_1\\ \\epsilon_{e-1}-...-\\omega_q\\ \\epsilon_{e-q}\\) It is important to notice, that it is not by default an advantage of including both methods, but one should select the appropriate method depending on the looks correlogram ARIMA is specifically good for short term forecasts as it utilize previous observations to forecast future values. It is able to represent both stationary and nonstationary data. Often, one encounter difficulties making correct model specifications, meaning meeting the assumptions. This is crucial for predicting reliable forecasts Key concepts ARMA vs. ARIMA: ARMA is an approach to stationary data. If the data is nonstationary, one must make the data stationary by differencing. leading to ARIMA. Hence: 1. ARMA: When we have stationarity 2. ARIMA: When we have non-stationarity Random walk: this is merely when you have an 0,1,0 ARIMA, hence only differencing. Thus no AR and MA Drift: that is a constant that can be added to the equation. It will make the model tend upwards or downwards. Imagine having an ARIMA 0,1,0 with a drift. Then you will only have the most recent \\(y_{t-1}\\) and then you will have the constant, which will force the forecast upwards if positive and vise verca. The notations for ARIMA are: \\[\\begin{equation} ARIMA_{(p,d,q)} \\tag{7.1} \\end{equation}\\] Where: p = the order of AR d = the order of differencing (integration) (NOTE: if this is 0, then the model is reduced to an ARMA model) if we only have differencing, then we call this a random walk this is I, as it is also called Integration order q = the order of MA, hence if q = 2, then MA for \\(t_{-1}\\) and \\(t_{-2}\\) is included in the regression model. 7.1.1 Elaborating on AR models AR = Autoregressive AR models are appropriate with stationary data This makes sense, as if the data is non stationary (where the variance is constant) This can generalized with AR(p), where p = the order of AR, meaning how many prior periods to \\(Y_t\\) to be included. Then how do we select an apporpriate number of lags? The autocorrelation function (ACF): selection criteria: the ACF should decline to zero exponentially fast The partial autocorelation function (PACF) See the full equation on page 360. Process: Select order of p Calculate coefficients for each lagged period Forecast using the coefficients Evaluate constantly if the coefficients are still applicable Assumptions Data is stationary. If not, then one must deal with that 7.1.2 Elaborating on MA models MA = Moving average IMPORTANT NOTE: this has nothing to do with regular Moving Average, as with using past periods This approach applies use residuals (between actual values and fitted values/or forecasted values) multiplied with a coefficient to forecast the coming period. As with AR, we are able to include x amount of previous periods. Thus, we are able to describe MA with: Y_t=+_t-1{t-1}-2{t-2}-…-q{t-q} \\tag{7.2} \\end{equation} where: \\(Y_t\\) = the forecast for time period t. \\(\\mu\\) = just a constant that is applied in the calculation \\(\\epsilon_t\\) = the error term as in any other regression \\(\\omega\\) = the coefficients that we are to calculate for each period NOTICE: these can be interpret as wheights put on each period. But it does NOT need to summarize to 1, it can be above and below \\(\\epsilon_{1-q}\\) = The error (residual) for each period Process: Select order of 1 Calculate coefficients for each lagged period Forecast using the coefficients Evaluate constantly if the coefficients are still applicable Then how do we select an apporpriate number of lags to be included? The autocorrelation function (ACF): selection criteria: the ACF should decline to zero exponentially fast The partial autocorelation function (PACF) 7.1.3 Elaborating on Integration models This corresponds with the I in the ARIMA. If we have a model that only contain I and not AR and MA, then we have a random walk. Meaning that we are left with: \\[\\begin{equation} y_t=y_{t-1}+\\ \\epsilon_t \\tag{7.3} \\end{equation}\\] We see that \\(y_t\\) can be defined by the previous observation, hence \\(y_{t-1}\\) + some randomness, which is explained by the \\(e_t\\), for period t. Naturally we are able to have a drift and a trend in the random walk, that would generate the following: RW with a drift: \\[\\begin{equation} y_t= \\beta_0+y_{t-1}+\\ \\epsilon_t \\tag{7.4} \\end{equation}\\] Hence we see that the drift is added with a constant, that can be compared with the interception in a normal regression. Hence, if \\(y_{t-1}\\) is 0 and the error is 0, then you will have the constant, which will always be there. And the RW with a drift and a trend: \\[\\begin{equation} y_t= \\beta_0+\\beta_1t+y_{t-1}+\\ \\epsilon_t \\tag{7.5} \\end{equation}\\] Hence we add a coefficient that as multiplied with the time period. "],["model-building-process.html", "7.2 Model Building Process", " 7.2 Model Building Process The Box-Jenkins model-building strategy. It has th following steps Model Identification Assessment for stationarity - can be done with ACF correlogram. If found, then use difference, hence ARIMA Identify what form of model to be used, e.g., MA, ARMA, ARIMA, or AR. This is done by assessing the time-series’ ACF (see examples on page 357 - 359) Model Specification: This is estimating model parameters Model Checking Make model diagnostics: Residuals to be random, no autocorelation left in them. Can also be checked with Ljung-Box Q stats No heteroskedasticity No spike in the ACF, must be within \\(±2/\\sqrt{n}\\) confidence interval from 0. If several model. Choose the one with the lowest AIC or BIC, depending on the goal. if that concludes indecisive result, then choose the simplest model (principle of parsimony) Forecasting with the Model "],["advantages-and-disadvantages-for-arima-models.html", "7.3 Advantages and disadvantages for ARIMA models", " 7.3 Advantages and disadvantages for ARIMA models Advantages: Box-Jenkins is a stable tool to get a model for short term accurate forecasts The model is flexible, but dont be fooled by complexity Formal testing procedures are available, such as AIC, BIC etc. Disadvantages: Large amount of data is needed Each time new data arrives, the model must be estimated again. That is because the parameters follow the most recent information. Construction is based on trial and error "],["adl.html", "7.4 ADL", " 7.4 ADL Basically ADL is doing and Autoregressive (AR) model and then we add another variable (this could be anything) which is lagged. Notice that we can model with the order of lagged variables to be included in the model. As the model is based on AR models, we must have stationarity in the data, just as in normal AR models. Hence we have the following equation: \\[\\begin{equation} ADL_{(p,q)} \\tag{7.6} \\end{equation}\\] Where p = the order of AR q = the number of lags for the added variable Procedure: Create the AR model Find the other relevant variable. Transform it to a timeseries using, ts() Include the lags in the model using lag(&lt;ts.object&gt;,&lt;number of lags&gt;) Example of how it may look: # GDPGR_ADL22 &lt;- dynlm(GDPGrowth_ts ~ #The dependant variable # L(GDPGrowth_ts) #The AR(1),The dependant variable lagged # + L(GDPGrowth_ts, 2) #The AR(2),The dependant variable lagged # + L(TSpread_ts) #Another TS lagged # + L(TSpread_ts, 2) #Another TS lagged two periods # ,start = c(1962, 1), end = c(2012, 4)) L() = lag(), if number of lags are not #TB3MS &lt;- xts(USMacroSWQ$TB3MS, USMacroSWQ$Date)[&quot;1960::2012&quot;] 7.4.1 Vector autoregressive (VAR) In the approach in ADL we only work with one other variable. Although with VAR models, we are able to include multiple independent variables. This can be written as: \\[\\begin{equation} VAR(p) \\tag{7.7} \\end{equation}\\] Where: p = p lags of the variable, one can apply VARselection(), this will yield p lags, notice, that it will be the same for all variables. See an example in section 7.6.2.2 7.4.1.1 Selection criteria One may use AIC, BIC or make out of sample assessment to select the best model, hence identifying what lags to be included in the model. "],["exercises-arima.html", "7.5 Exercises - ARIMA", " 7.5 Exercises - ARIMA 7.5.1 IBM stock, problem 12 p 405 Qa The data in Table P-12 are weekly prices for IBM stock. #Loading y &lt;- read_excel(&quot;Data/Week47/IBMstock.xls&quot;) %&gt;% ts(frequency = 52) #Plotting ts.plot(y) Figure 7.1: IBM stock prices It is dififult to say if there is a trend, but there appear to be seasons. This can be futher expected with the correlogram acf(x = y ,lag.max = 52) #We take for a whole year Figure 7.2: Correlogram (acf) IBM Stock prices The correlogram suggests that there is seasonality in the data. We only have data for one year, hence 52 periods. It would be interesting to see if the patterns express it self over years. We could also express the pacf. The partial correlation coefficient is estimated by fitting autoregressive models of successively higher orders up to lag.max. pacf(x = y ,lag.max = 52) #We take for a whole year Figure 7.3: Correlogram (pacf) IBM Stock prices What approaches does this suggest? It suggests, that we should use an AR model, as the acf is tending towards 0 while the pacf quickly drops to 0. Qb Looking at the ts plot, the data does not appear to be stationary. Perhaps there is a small indication of a trend in the data, hence not constant variance around a fixed point. It therefore suggests that we move into ARIMA, where we apply d order differencing. Qc We apply AR and d, where we first try with first order, to assess if it is sufficient. p &lt;- 0 #AR order d &lt;- 1 #Differencing order q &lt;- 0 #MA order order &lt;- c(p,d,q) ARIMAmod &lt;- arima(x = y #The time-series ,order = order ) #Assessing in-samp accuracy accuracy(object = fitted(ARIMAmod) ,x = y) ## ME RMSE MAE MPE MAPE ACF1 Theil&#39;s U ## Test set 0.7166731 6.090774 4.793596 0.2270925 1.759575 0.2971583 1 We see an mean percentage error of 1,64%. That is quite low. But also expected as it is in sample. plot(ARIMAmod$residuals,ylab = &quot;Residuals&quot;) Figure 7.4: Residuals plot IBM stock ARIMA 1,1,0 The changes appear to be randomly distributed around 0. Qd Perform diagnostic checks to determine the adequacy of your fitted model Residuals to be random We see from the residuals plot above. It was difficult to get rid of heteroskedasticity Note, it was tested with only differencing, and appear to have a more constant variance with this but we still see something indicating that the variance is not entirely constant Constant variance This can also be checked using a Ljung-Box test, where the null hypothesis is that there is no relationship between the observations. Box.test(ARIMAmod$residuals ,fitdf = p+q) #Because it is applied to and ARIMA model ## ## Box-Pierce test ## ## data: ARIMAmod$residuals ## X-squared = 4.5918, df = 1, p-value = 0.03213 We see that the p-value is below the p-value (5%), hence the model is under misspecification. Although we are close to the threshold, let us assume that it is sufficient. Autocorrelation in residuals We see that the errors appear not to show autocorrelation. Although there is one period, that does appear to have a spike + the first lagged period, but it is close to the recommended threshold. acf(ARIMAmod$residuals) Then one could test other models and see if they perform better. I have been playing around with the orders, but it does not appear to help with the residuals. Qd Make forecast for the stock price in week 1. { forecast(object = ARIMAmod,h = 1) %&gt;% print() y[nrow(y),] %&gt;% print() } ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 2 304 296.1184 311.8816 291.9461 316.0539 ## IBM ## 304 We see that the forecast is 311,73 where the naive forecast (the most recent period) would just say 304. Although one must be very precautions, as it was found that the model is under misspecification rm(list = ls()) 7.5.2 Demand data, problem 7 p 403 Loading the data df &lt;- read_excel(&quot;Data/Week47/Demand.xls&quot;) y &lt;- ts(data = df,frequency = 52) Qa Plotting acf acf(y,lag.max = 52) Figure 7.5: acf Demand We see that there is clearly a trend and it appears as if we have seasons, although looking at it does not appear as if we have seasons. We can support this with a pacf pacf(y) We are able to use an autoregressive approach as acf tends towards 0 and pacf quickly drops to 0. Lastly we can check for stationarity. ts.plot(y) Figure 7.6: Demand time-series We observe non stationary data, hence we should apply differencing as well. Qb Manually doing ARIMA p = 1 d = 1 q = 0 order &lt;- c(p,d,q) ARIMAmod &lt;- arima(x = y ,order = order) plot(ARIMAmod$residuals) Figure 7.7: Residuals ARIMA Auto ARIMA We are also able to apply auto.arima() to find the most optimal combination based on the sample data. # Making the model ts.arima &lt;- auto.arima(y = y) summary(ts.arima) ## Series: y ## ARIMA(2,1,1) with drift ## ## Coefficients: ## ar1 ar2 ma1 drift ## -0.3531 -0.4859 -0.8662 0.7106 ## s.e. 0.1461 0.1399 0.1214 0.0101 ## ## sigma^2 estimated as 0.7302: log likelihood=-63.77 ## AIC=137.54 AICc=138.87 BIC=147.2 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set -0.04310129 0.8124002 0.6510066 -0.2215986 1.820533 NaN ## ACF1 ## Training set 0.005108341 We see that the this suggest a full ARIMA model with 2 order AR, 1 order differencing and 1 order MA. Now we can display the residuals to check for independence within the error terms. tsdisplay(ts.arima$residuals) Figure 7.8: Residuals time-series demand It appears as if with have independence in the residuals and also none significant spikes in the correlograms. We can make a statistical check for this as well, using the Box-Pierce test Box.test(ts.arima$residuals) ## ## Box-Pierce test ## ## data: ts.arima$residuals ## X-squared = 0.0013569, df = 1, p-value = 0.9706 We see that we cannot reject the null hypothesis, hence it is fair to assume that the observations are independent of each other. Qc equation for forecast: Cant find the constant, but one can call the coefficients with the following: ts.arima[[&quot;coef&quot;]] ## ar1 ar2 ma1 drift ## -0.3530653 -0.4859323 -0.8662332 0.7105675 Qd Forecasting demand for the coming four periods. That can be done using forecast() where h = 4 arima.fh4 &lt;- forecast(ts.arima ,h = 4 #Forecast horizon ,level = 0.95 #Confidence interval ) knitr::kable(arima.fh4,caption = &quot;Forecast with confidence intervals&quot;) Table 7.1: Forecast with confidence intervals Point Forecast Lo 95 Hi 95 2.000000 57.13051 55.45568 58.80534 2.019231 57.94525 56.23062 59.65988 2.038462 59.16331 57.38801 60.93861 2.057692 59.64408 57.78105 61.50711 plot(arima.fh4,xlim = c(1.8,2.2)) grid(col = &quot;lightgrey&quot;) rm(list = ls()) 7.5.3 Closing stock quotations, problem 13 p 409 df &lt;- read_excel(&quot;Data/Week47/ClosingStockQuatations.xls&quot;) y &lt;- ts(df,frequency = 365) y.train &lt;- y[1:145] y.test &lt;- y[146:150] ts.arima &lt;- auto.arima(y.train) tsdiag(ts.arima) Figure 7.9: Diagnostics for ARIMA We see that the residuals appear to be randomly distributed around a fixed point. Also there does not appear to be residuals that spikes, indicating autocorrelation in the residuals. Assessing the Ljung-Box statistic, we see that there are no values that go beyond the critical level of 5%. Hence it is fair to assume that the residuals are independent of each other. arima.fh5 &lt;- forecast(ts.arima,h = 5,level = 0.95) knitr::kable(arima.fh5,caption = &quot;Forecast with confidence level&quot;) Table 7.2: Forecast with confidence level Point Forecast Lo 95 Hi 95 146 133.8119 128.8361 138.7878 147 133.8119 128.6346 138.9893 148 133.8119 128.4407 139.1832 149 133.8119 128.2535 139.3704 150 133.8119 128.0724 139.5515 plot(arima.fh5,xlim = c(100,157)) grid(col = &quot;lightgrey&quot;) Figure 7.10: Forecast Stock Quotations accuracy(arima.fh5$mean,x = y.test) ## ME RMSE MAE MPE MAPE ## Test set 2.508054 3.000656 2.508054 1.82561 1.82561 We see that the accuracy is 2.5 MAE where the MAPE is 1.8 rm(list = ls()) 7.5.4 HW: Case 1 page 413-414 (q1-3) Not done 7.5.5 HW: Case 4 page 417-419 Not done 7.5.6 Sales data seasonal Not done #Arima(y = y,model = #&lt;insert fitted arima model here, to preserve coefficients&gt;#) 7.5.7 In class assignment It is done somewhere. Otherwise, do it again "],["exercises-adl.html", "7.6 Exercises - ADL", " 7.6 Exercises - ADL 7.6.1 GDP and CO2 levels df &lt;- read_excel(&quot;Data/Week48/GDP_CO2.xls&quot;) y &lt;- df$co2 x &lt;- df$gdp Now the data is loaded and we have defined the y and x variable, being CO2 levels and GDP. Now we can make the ADL with the following: adl.mod &lt;- auto_ardl(y ~ x ,data = cbind(x, y) #We want to combine the variables into 1 df ,max_order = 10 #Mandatory, could be other orders as well ,selection = &quot;AIC&quot; ) adl.mod &lt;- adl.mod$best_model adl.mod$order ## [1] 1 10 We see that according to AIC, using up to 10 lags, the optimal constellation is with 1 lag for the y variable and including 10 lags from the x variable (the additional explanatory variable) We can assess the model that we achieved, by calling the summary. summary(adl.mod) ## ## Time series regression with &quot;ts&quot; data: ## Start = 11, End = 196 ## ## Call: ## dynlm::dynlm(formula = full_formula, data = data, start = start, ## end = end) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.4432 -0.8233 -0.2912 0.9513 14.4389 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.20914 0.32287 0.648 0.51800 ## L(y, 1) 0.82598 0.03581 23.063 &lt; 0.0000000000000002 *** ## x 0.35123 0.06799 5.166 0.000000653 *** ## L(x, 1) -0.24174 0.12054 -2.005 0.04648 * ## L(x, 2) -0.05738 0.12814 -0.448 0.65486 ## L(x, 3) -0.08188 0.13565 -0.604 0.54690 ## L(x, 4) 0.35590 0.13428 2.650 0.00879 ** ## L(x, 5) -0.30012 0.13519 -2.220 0.02771 * ## L(x, 6) 0.08400 0.13591 0.618 0.53733 ## L(x, 7) 0.20019 0.13657 1.466 0.14449 ## L(x, 8) -0.42832 0.13544 -3.162 0.00185 ** ## L(x, 9) 0.38239 0.13279 2.880 0.00448 ** ## L(x, 10) -0.26598 0.08769 -3.033 0.00279 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.302 on 173 degrees of freedom ## Multiple R-squared: 0.9831, Adjusted R-squared: 0.9819 ## F-statistic: 836.7 on 12 and 173 DF, p-value: &lt; 0.00000000000000022 We see that some of the lags appaer not to be statistically significant, although, the model is estimated to be optimized to low risk, thus we assume that the model is sufficient. rm(list = ls()) 7.6.2 Short- and long-term interest rates, exchange rates 7.6.2.1 Loading data, combining time series and assessment of stationarity First the data is loaded, we are interested in the following&gt; Short term interest rates Long term interest rates Exchange rates The data is quarterly df &lt;- read_excel(&quot;Data/Week48/EuroMacroData.xlsx&quot;) stn &lt;- df$STN %&gt;% log() %&gt;% ts(frequency = 4) ltn &lt;- df$LTN %&gt;% log() %&gt;% ts(frequency = 4) een &lt;- df$EEN %&gt;% log() %&gt;% ts(frequency = 4) Now we can present the acf and pacf for each variable along with the time series for each variable. { tsdisplay(stn) tsdisplay(ltn) tsdisplay(een) } Figure 7.11: Time-Series Display Figure 7.12: Time-Series Display Figure 7.13: Time-Series Display Now we are able to group the variables in one time series frame, lets denote it with z z &lt;- ts(cbind(stn,ltn,een), frequency = 4) As we observed trend in the time series (hence non stationary data), we can make first order differencing to attempt to get rid of that and get stationary data. dz &lt;- diff(z) We can now plot the data to see the following result. plot(dz) Figure 7.14: Data with 1st order differencing Versus the raw data: plot(z) Figure 7.15: Raw data Where we see that the changes in the observations show stationarity, where the raw data is clearly not stationary. 7.6.2.2 Vector AutoRegressive Model (VAR) 7.6.2.2.1 1. Determine order of lags to be included + model estimation We apply VARselect(), which returns the following: selection: Vector with the optimal lag number according to each criterium. criteria: A matrix containing the values of the criteria up to lag.max. We are only interested in finding the optimal amount of lags, p for the VAR. Hence we apply [[“selection”]] VARselect(y = dz ,lag.max = 10 ,type = &quot;const&quot;)[[&quot;selection&quot;]] ## AIC(n) HQ(n) SC(n) FPE(n) ## 1 1 1 1 We see that the optimal amount fo lags to be included are 1. Fortunately this goes for all of the information criteria. Hence, we have the following setting \\(VAR(1)\\) Now can estimate the model, using the lag order, that we just found. var1 &lt;- VAR(y = dz ,p = 1 #How many lags to be included in the models, e.g., if p = 3, # then three lags of each variable is included. ,type = &quot;const&quot;) summary(var1) ## ## VAR Estimation Results: ## ========================= ## Endogenous variables: stn, ltn, een ## Deterministic variables: const ## Sample size: 114 ## Log Likelihood: 609.086 ## Roots of the characteristic polynomial: ## 0.4441 0.4441 0.2423 ## Call: ## VAR(y = dz, p = 1, type = &quot;const&quot;) ## ## ## Estimation results for equation stn: ## ==================================== ## stn = stn.l1 + ltn.l1 + een.l1 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## stn.l1 0.395558 0.099033 3.994 0.000118 *** ## ltn.l1 0.293962 0.169757 1.732 0.086137 . ## een.l1 0.328754 0.266664 1.233 0.220264 ## const -0.002934 0.006924 -0.424 0.672604 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.07358 on 110 degrees of freedom ## Multiple R-Squared: 0.2626, Adjusted R-squared: 0.2425 ## F-statistic: 13.06 on 3 and 110 DF, p-value: 0.000000234 ## ## ## Estimation results for equation ltn: ## ==================================== ## ltn = stn.l1 + ltn.l1 + een.l1 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## stn.l1 0.055456 0.060338 0.919 0.360056 ## ltn.l1 0.386815 0.103428 3.740 0.000294 *** ## een.l1 0.241711 0.162471 1.488 0.139686 ## const -0.003082 0.004219 -0.730 0.466653 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.04483 on 110 degrees of freedom ## Multiple R-Squared: 0.2046, Adjusted R-squared: 0.1829 ## F-statistic: 9.429 on 3 and 110 DF, p-value: 0.00001345 ## ## ## Estimation results for equation een: ## ==================================== ## een = stn.l1 + ltn.l1 + een.l1 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## stn.l1 -0.037006 0.033134 -1.117 0.26650 ## ltn.l1 -0.021367 0.056797 -0.376 0.70750 ## een.l1 0.322242 0.089221 3.612 0.00046 *** ## const -0.001004 0.002317 -0.434 0.66549 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.02462 on 110 degrees of freedom ## Multiple R-Squared: 0.1313, Adjusted R-squared: 0.1077 ## F-statistic: 5.544 on 3 and 110 DF, p-value: 0.001396 ## ## ## ## Covariance matrix of residuals: ## stn ltn een ## stn 0.00541405 0.00156222 -0.00001442 ## ltn 0.00156222 0.00200975 -0.00003693 ## een -0.00001442 -0.00003693 0.00060607 ## ## Correlation matrix of residuals: ## stn ltn een ## stn 1.000000 0.47360 -0.007963 ## ltn 0.473599 1.00000 -0.033461 ## een -0.007963 -0.03346 1.000000 We see that we get a model for each of the time-series and concludingly a covariance- and correlation matrixt of the residuals for the different models. Looking at the different model estimations, we see that that e.g., lag 1 of ltn is significant for explaining ltn, where lag 1 of the other variables are not significant and so on. As written in the code, if one was to insert p = 3 instead of 1, then we would merely have three lagged variables for each model, although it will still reflect the same signifances for the different variables. Conclusion: We are not able to explain anything with the other models, since there is statistical evidence for relationship between other lagged variables and the stn, ltn and een. Hence, we may be just as fine, just by doing an AR approach to each of the models, perhaps even just an AR(p=1) 7.6.2.2.2 2. Model diagnostics Now we can make model diagnostics to check for autocorrelation (serial correlation). This executes a Portmanteau test for correlation in the errors (i.e., autocorrelation, i.e., serial correlation). The null hypothesis is that there are no autocorrelation serial.test(var1 ,lags.pt = 10 #It is chosen to be 10, could be anything else, perhaps one could plot it ,type = &quot;PT.asymptotic&quot; ) ## ## Portmanteau Test (asymptotic) ## ## data: Residuals of VAR object var1 ## Chi-squared = 67.353, df = 81, p-value = 0.8612 As the p-value is far above the significane level, 5%, we cannot reject the null hypothesis, hence it is fair to assume, that the residuals are not serial correlated. 7.6.2.2.3 3. Response to shocks in the variables With the following, we are able to deduce how the different variables react (respond) from shocks in the variables. The y-axis expres the changes where the x-axis express the n steps ahead. Hence in this example it is quarters ahead. plot(irf(var1,boot = TRUE, ci=0.95)) Figure 7.16: Resonses from shocks Figure 7.17: Resonses from shocks Figure 7.18: Resonses from shocks First of all, we always see that shocks in the variables always show direct positive (same direction) response in period 0 on its own variable, that makes sens and it expected. As the illustrations above is based on models that contain one lagged period on the model, we don’t include all available information (also, it is likely that other variable explain the relationship). E.g., looking at shocks in stn (the first figure), we see that ltn immediately tend to move in the same direction, where the effect decays over time and after some periods, it will practically be 0. Notice, that some response have confidence intervals that are below and above 0, hence we are not really able to say how the reaction will be, same or the opposite direction. 7.6.2.2.3.1 Exogenious vs. endogenious Terms: Exogenious: if a variable is extremely exogenious, it is not explain or determined by other variables Endogenious: if a variable is extremely endogenoius, it is explianed or determined by one or more variables. This can be used to rank variables based on how much they are affected by other variables, with the most exogenious variable first (This is called Cholesky ordering), hence we rank based on the following principle: lets say, that we have four variables, we want to order according to Cholesky ordering Most exogenious Less exogenious than 1 more than 3. Hence, more endogenious than 1. but less than 3. Less exogenious than 2 more than 4. Hence, more endogenious than 2. but less than 4. Most endogenious Importance! This is important, as it matters in the end how the forecast is done. see slides from lecture 10 page 12. 7.6.2.2.4 4. Forecasting with ADL This has the following steps: Data partition Select the order of VAR Fit the model + check residuals Make the forecast Assessing accuracy 7.6.2.2.4.1 Step 1 - Data partition We apply the differenced data from the exercise. The first 90 obersvations as train data and the rest test data. insampdz &lt;- ts(dz[1:90, 1:3], frequency = 4) outsampdz &lt;- dz[91:115, 1:3] 7.6.2.2.4.2 Step 2 - Select the order of VAR We find the optimal order for p (recall VAR(p)) VARselect(insampdz ,lag.max = 10 ,type=&quot;const&quot;)[[&quot;selection&quot;]] ## AIC(n) HQ(n) SC(n) FPE(n) ## 1 1 1 1 We see that the optimal order is 1. 7.6.2.2.4.3 Step 3 - Fit the model + check residuals The data is fitted and we call the summary. fit &lt;- VAR(y = insampdz #The train data ,p = 1 #Found in step 2 ,type=&quot;const&quot;) summary(fit) ## ## VAR Estimation Results: ## ========================= ## Endogenous variables: stn, ltn, een ## Deterministic variables: const ## Sample size: 89 ## Log Likelihood: 480.864 ## Roots of the characteristic polynomial: ## 0.4301 0.4301 0.07976 ## Call: ## VAR(y = insampdz, p = 1, type = &quot;const&quot;) ## ## ## Estimation results for equation stn: ## ==================================== ## stn = stn.l1 + ltn.l1 + een.l1 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## stn.l1 0.424194 0.120074 3.533 0.000668 *** ## ltn.l1 0.098362 0.246167 0.400 0.690471 ## een.l1 0.330675 0.322101 1.027 0.307513 ## const 0.003247 0.008455 0.384 0.701865 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.07931 on 85 degrees of freedom ## Multiple R-Squared: 0.2173, Adjusted R-squared: 0.1896 ## F-statistic: 7.865 on 3 and 85 DF, p-value: 0.0001082 ## ## ## Estimation results for equation ltn: ## ==================================== ## ltn = stn.l1 + ltn.l1 + een.l1 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## stn.l1 0.127202 0.061310 2.075 0.041 * ## ltn.l1 0.114150 0.125692 0.908 0.366 ## een.l1 0.217023 0.164464 1.320 0.191 ## const 0.002846 0.004317 0.659 0.512 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.0405 on 85 degrees of freedom ## Multiple R-Squared: 0.1384, Adjusted R-squared: 0.108 ## F-statistic: 4.552 on 3 and 85 DF, p-value: 0.005238 ## ## ## Estimation results for equation een: ## ==================================== ## een = stn.l1 + ltn.l1 + een.l1 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## stn.l1 -0.038702 0.038097 -1.016 0.312572 ## ltn.l1 -0.017703 0.078103 -0.227 0.821230 ## een.l1 0.368552 0.102195 3.606 0.000523 *** ## const -0.001251 0.002682 -0.466 0.642070 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.02516 on 85 degrees of freedom ## Multiple R-Squared: 0.1488, Adjusted R-squared: 0.1187 ## F-statistic: 4.952 on 3 and 85 DF, p-value: 0.003233 ## ## ## ## Covariance matrix of residuals: ## stn ltn een ## stn 0.00629028 0.00169606 0.00007265 ## ltn 0.00169606 0.00163993 0.00009076 ## een 0.00007265 0.00009076 0.00063321 ## ## Correlation matrix of residuals: ## stn ltn een ## stn 1.0000 0.52807 0.03640 ## ltn 0.5281 1.00000 0.08907 ## een 0.0364 0.08907 1.00000 We see that there is not really any significant relationship between the different variables and only the lagged values of its own variable appear to be significant. Now we can check the residuals for independency (not having serial correlation, i.e., autocorrelation). The null hypothesis is, that there is no autocorrelation. serial.test(fit, lags.pt=10, type=&quot;PT.asymptotic&quot;) ## ## Portmanteau Test (asymptotic) ## ## data: Residuals of VAR object fit ## Chi-squared = 78.356, df = 81, p-value = 0.5626 We see the p-value being insignificant, hence we cannot reject H0 and it is fair to asume no autocorrelation. 7.6.2.2.4.4 Step 4 - Make the forecast We apply the forecast() function to forecast the coming periods fcast &lt;- forecast(object = fit ,h = 25) #Forecasting 25 quarters plot(fcast) We see the forecast having a very wide confidence interval and barely any movements after a couple of periods. This indicates that it is a bit random how it moves. If we had more seasonality, trends and cycles, then it would be easier to forecast coming periods. 7.6.2.2.4.5 Step 5 - Assessing accuracy Now we can assess accuracy of the forecasts accuracy(object = fcast$forecast$stn ,x = outsampdz[1:25,1]) #1 for stn ## ME RMSE MAE MPE ## Training set 0.000000000000000001407026 0.07750849 0.05654433 39.17323 ## Test set -0.053153958986903539207791 0.07534233 0.06293134 123.55935 ## MAPE MASE ACF1 ## Training set 174.2171 0.8031345 0.03369461 ## Test set 127.4491 0.8938532 NA accuracy(object = fcast$forecast$ltn ,x = outsampdz[1:25,2]) ## ME RMSE MAE MPE ## Training set -0.000000000000000001562976 0.03957558 0.02879253 76.29913 ## Test set -0.038925425000674265807454 0.07048620 0.06082491 98.73445 ## MAPE MASE ACF1 ## Training set 155.66489 0.7635219 -0.03261785 ## Test set 98.73445 1.6129583 NA accuracy(object = fcast$forecast$een ,x = outsampdz[1:25,3]) ## ME RMSE MAE MPE ## Training set -0.0000000000000000006126428 0.02459171 0.01934306 111.6867 ## Test set 0.0064823603697087804620391 0.02516587 0.02148333 127.4315 ## MAPE MASE ACF1 ## Training set 221.2635 0.8012928 0.03654264 ## Test set 127.4315 0.8899542 NA We see that the MAPE is close 100% or more than 100% on average. Hence the models are not performing well. That is also expected as we see the forecasts confidence intervals are very wide and not moving much. Even though all models are bad, it appears as if we are best at estimating ltn. "],["non-stationary-time-series.html", "8 Non Stationary Time-series", " 8 Non Stationary Time-series This section explicitly identifies how to check for non stationarity and how to deal with it Hence: 1. Unit roots 2. Spurious Regression 3. Cointegration Literature HW: Judgmental Forecasting and Forecast Adjustments (Combining forecasts) HW: Managing the Forecast Process Discussion on Diebold-Mariano test: http://www.phdeconomics.sssup.it/documents/Lesson19.pdf Suggestions from the slides HW: pp. 321-323 (cointegration) Unit roots: http://faculty.washington.edu/ezivot/econ584/notes/unitroot.pdf Cointegration: notes I &amp; notes II: Where notes I = http://www2.hawaii.edu/%7Ebonham/664/materials/New_Lectures3.pdf, notes II is not to be found "],["unit-roots.html", "8.1 Unit Roots", " 8.1 Unit Roots Recall from section 6.1, TS consist of four elements: T,S,C,I. T,S and C are what is called determistic components. Where the rest, I, is irregular, although this might contain some properties as well. These properties are called stochastic or random properties Definition on stochastic: having a random probability distribution or pattern that may be analyzed statistically but may not be predicted precisely. In this chapter, we explore how a time-series that visually does not reflect nonstationary data, can in fact be non stationary. One may observe, that even though the irregular component is often random (being stationary), it may have show nonstationarity. In general, one can do unit root tests, to test if the data is stationary or non stationary. In general, it can be said: Unit Root = Non Stationarity Hence; I have found unit roots in the data = i have found non stationarity in the data The following will explore how Augmented Dickey-Fuller test (ADF) can make an assessment of unit roots. 8.1.1 Augmented Dickey-Fuller (ADF) test This is basically a test for nonstationarity. Null hypothesis = x has a unit root (i.e. is non stationary) Alternate hypothesis = x has not unit root (i.e. is stationary) Be careful, this test may contradict with the Durbin-Watson test, if the variables actually do appear to be cointegrated Side note: but the variables may not be stationary, but if they are not spuriously related, hence cointegrated, then it is in fact OK for the variables to have stationarity See the equation for the ADF in the slides from L11. See exercise 8.4.1.1 with the Dairy Data. "],["spurious-regression.html", "8.2 Spurious Regression", " 8.2 Spurious Regression This is about having nonstationary data, where the nonstationarity is able to prove a relationship between a dependent variable and independent variable(s). Running a regression on this, will not make any sense, as it is an effect that is not included in the model, that is actually proving the relationship. Meaning that you don’t know if the relationship is actually true or not, but you are apparently with a statistical test able to prove it. Hence we get a spurious regression if: The variables have another variable in common, but it is not included in the model. It is a coincidence "],["cointegration.html", "8.3 Cointegration", " 8.3 Cointegration Although variables that follow each other, may in fact be cointegrated, meaning that they are not just following each other by coincidence, but they actually have a Orelationship. To check if both series (y and x) are nonstationary and the regression between these are non stationary. Then we can predict based on this relationship. One can also say, that series are cointegrated if they move with each other, e.g., have the same trend. See the following link with an example: https://www.wallstreetmojo.com/cointegration/. Rule of thumb, when the difference between variables is the same, then they are cointegrated This implies, that we don’t have to be differencing as the relationship between the variables are stationary, implying: \\[\\begin{equation} y_t-\\beta x_t=\\epsilon_{t\\ } \\tag{8.1} \\end{equation}\\] show stationarity. The intuition is that y less x times a constant value, you will be left with the error terms, that show stationarity, as subtracting the series’ with each other, will take out the unit roots. 8.3.1 Checking for cointegration The process: There are basically two options for testing for this. The second option is the better process Option 1 Check if the series have unit roots (are nonstationary) Their linear combination \\(y_t-\\beta x_t\\), does not have a unit root (= is stationary) Option 2 This can be done with an Engle-Granger (EG) test. The procedure sets forth the following: Testing if \\(y_t\\) and \\(x_t\\) are non stationary using an ADF test. Regress \\(y_t\\) ib \\(x_t\\) and test if the residuals (\\(e_t\\)) show stationarity, that can be done with and ADF (augmented Dickey-Fuller test, but also a good idea to plot) See an example of this procedure in the exercise with Dairy Data, see section 8.4.1.2 "],["exercises-3.html", "8.4 Exercises", " 8.4 Exercises 8.4.1 Dairy Data Loading the data. We have yearly data and select employment data. df &lt;- read_xls(&quot;Data/Week49/dairydata.xls&quot;) y &lt;- df$emp %&gt;% log() %&gt;% ts() #Frequency is by default 1, that is applied, as we have yearly data tsdisplay(y) Figure 8.1: Visual interptration Notice that we take log, that is done to smooth out extreme values, and make the data more normal. Based on the figure, we are able to deduce: The ts does not appear to show stationarity, that is to be further inspected in 8.4.1.1 That there is clearly a trend The acf does not imply, that the data show autocorrelation 8.4.1.1 Unit Root testing - Augmented Dickey-Fuller (ADF) We aim to find out if there is statistical evidence for unit roots (nonstationarity), that is done with the adf. First we make ADF on the time series, hence \\(y_t\\) adf.test(y) ## ## Augmented Dickey-Fuller Test ## ## data: y ## Dickey-Fuller = -1.9669, Lag order = 3, p-value = 0.5879 ## alternative hypothesis: stationary Note, the null hypothesis is that the data show nonstationarity Based on the ADF on the log of the data, we are not able to reject the null hypothesis, hence the time series does not show stationarity, that is as expected, as we saw from the ts plot. We can take the first order difference to see if we can get rid of the nonstationarity. dy &lt;- diff(y,lag = 1) #lag 1 is default, but shown for explanatory reasons adf.test(dy) #Rejecting the null, so can claim stationarity of the differenced series ## ## Augmented Dickey-Fuller Test ## ## data: dy ## Dickey-Fuller = -3.8763, Lag order = 3, p-value = 0.0216 ## alternative hypothesis: stationary Now we are able to reject the null hypothesis. We can then look at this visually. tsdisplay(dy) Figure 8.2: tsdisplay dy This just confirms the hypothesis test. Conclusion With ADF, we are able to make a statistical test for stationarity, hence it is able to support the visual interpretation. 8.4.1.2 Cointegration Now we extend the data and introduce another variable, production worker hours, hence \\(x_t\\). As with the y variable, the data is annually and we take the log of the data. As with y, we must check for stationarity, hence: x &lt;- ts(log(df$prodh)) #production worker hours plot(x) The data does not look stationary, lets do an ADF, to make a statistical test for this. adf.test(x) ## ## Augmented Dickey-Fuller Test ## ## data: x ## Dickey-Fuller = -2.4693, Lag order = 3, p-value = 0.3855 ## alternative hypothesis: stationary We are not able to reject the null hypothesis, hence x being non stationarity (i.e. non stationary in levels). Lets do first order differencing and check ADF for that: adf.test(diff(x)) ## ## Augmented Dickey-Fuller Test ## ## data: diff(x) ## Dickey-Fuller = -5.1438, Lag order = 3, p-value = 0.01 ## alternative hypothesis: stationary We see that we are able to reject H0. 8.4.1.2.1 Graphical inspection of the data To show this graphically, we can represent the following. plot(y,col = 1,ylim = c(-0.5,2.5),main = &quot;Cointegration&quot;,xlab = &quot;Years&quot;,ylab = &quot;Values&quot;) + lines(x,col = 2) + lines(residuals(lm(y~x)),col = 6,lty = 3) + grid(col = &quot;grey&quot;,nx = 10,ny = 20) ## integer(0) legend(x = &quot;topleft&quot;,legend = c(&quot;y=emp&quot;,&quot;x=prodh&quot;,&quot;residuals&quot;),lty = c(1,1,3),col = c(1,2,6),cex = 0.7) Figure 8.3: Representation of cointegration We see that the time-series’ (which are not difference) are not stationary and the perfectly fits each other. Let us assume, that these are not spurously related, then they are in fact cointegrated. When a regression is run on the two variables, we see that the residuals show stationarity, hence that is a good indication of cointegration. This can be further explored with the following two procedures: 8.4.1.2.2 Test for cointegration Statistical test - Phillips-Ouliaris test (2-step EG test), whith H0: no cointegration This is supposed to be the better option, as it uses the correct distributions, that was just briefly mentioned during class Manual process, consisting of: Fitting y on x Checking the residuals for unit roots (stationarity) using ADF. The following does both: Option 1 - Phillips-Ouliaris test # Combining the two vectors x and y z &lt;- ts(cbind(x,y)) po.test(z) #Note, this is the 2-step EG test ## ## Phillips-Ouliaris Cointegration Test ## ## data: z ## Phillips-Ouliaris demeaned = -33.724, Truncation lag parameter = 0, ## p-value = 0.01 We have H0: no cointegration. We are able to reject the null, hence it is fair to assume that there is cointegration between the two variables. Option 2 - Manual process fit &lt;- lm(y~x) #Running an OLS of y on x adf.test(x = resid(fit)) #Testing if the residuals from the estimated model contain a unit root. ## ## Augmented Dickey-Fuller Test ## ## data: resid(fit) ## Dickey-Fuller = -2.7005, Lag order = 3, p-value = 0.2924 ## alternative hypothesis: stationary ADF H0: x = non stationary We see that this does in fact not show stationarity, which contradicts with the PO test. Although the adf.test is not always applying the correct distributions, hence it may not align with the PO test, which is what we see in this. "],["combining-forecast-result-and-forecast-evaluation.html", "9 Combining Forecast Result and Forecast Evaluation", " 9 Combining Forecast Result and Forecast Evaluation Literature: From the reading list: HW: Judgmental Forecasting and Forecast Adjustments (Combining forecasts) HW: Managing the Forecast Process From the slides: Diebold-Mariano and other tests for equal forecast accuracy: http://www.phdeconomics.sssup.it/documents/Lesson19.pdf "],["why-when-and-how-to-combine-forecasts.html", "9.1 Why, when and how to combine forecasts", " 9.1 Why, when and how to combine forecasts 9.1.1 Why do we combine forecasts? If you have different approaches and you know that they are both contribution with some valuable information, making discrete selection, hence one forecast method, will permanently exclude the information from the other model. Hence, by combining methods, we are able to collect more information in the model, that would not be captured with only using a single model. 9.1.2 When to combine forecasts? Combining models is usually applied with methods, that are not from the same group, e.g., ARIMA and ARIMA, but instead e.g., ARIMA and exponential smoothing, as it approaches data in two different ways. Also, you often want to include different models that contain different information, to capture different perspectives, e.g., in terms of data analysis or in terms of different variables. 9.1.3 How to combine forecasts? How to combine forecasts? You assign weights to the forecasts, hence: \\[\\begin{equation} F_{combined}=w_1F_1+w_2F_2 \\tag{9.1} \\end{equation}\\] How to find the weights? The following are different methods; Nelson combination method! Here there are restrictions on the weights (How much? I think to one.) Granger-Ramanathan - it has shown to be better than Nelson combination method. This has no restrictions, the weights must not sum to 1. Time-varying weights: for each period of time, we are calculate the weight is each period of time. Diebold-Mariano test Are MSE’s of different forecasts equal? Says that, we are going to compare two forecasts and see if there is a difference in performance or not. Hence: H0: MSE1=MSE1 This use Diebold-Mariano test statistic If we are not able to reject H0, then there Rule of thumb, if you have two different methods yielding the same result, then you can make a DM test to see if the MSEs are actually different, if not, then you should consider combining But notice, if the models are the same approach, then you should probably just pick one "],["exercises-4.html", "9.2 Exercises", " 9.2 Exercises 9.2.1 Air passengers Loading the data df &lt;- read_excel(&quot;Data/Week49/AirPassengers.xlsx&quot;) y &lt;- ts(df[,2] #The passengers variable ,start=1949 ,frequency = 12) tsdisplay(y) We see that there is clearly an upwards trend and seasonality. Also the variance appear to be increasing, hence perhaps the composition is multiplicative. Also looking at the ACF, it become clearer with the seasonality and trend. To validate models, we are going to split the sample into two partitions, train and test data. # In-sample (75%) and out-of-sample (25%) split insamp &lt;- ts(y[1:108] #75% of the data ,frequency = 12) outsamp &lt;- y[109:144] #The rest 25%. We dont care about frequency, as we just need the observation for comparison l&lt;-length(outsamp) #generate a number called &quot;l&quot; equal to the length of the test set 9.2.1.1 Producing forecasts We are going to make two forecasts: ARIMA HoltWinters 9.2.1.1.1 Forecast 1 - ARIMA Now we can do the first forecasts. ###FORECAST 1### # # Generating the first forecast based on ARIMA models fit &lt;- auto.arima(y = insamp ,seasonal = TRUE) #This is in fact redundant, but as we see seasons, we can just as well just tell R, that this is the case summary(fit) #model of choice is ARIMA(1,1,0)(0,1,0)[12] ## Series: insamp ## ARIMA(1,1,0)(0,1,0)[12] ## ## Coefficients: ## ar1 ## -0.2411 ## s.e. 0.0992 ## ## sigma^2 estimated as 93.74: log likelihood=-350 ## AIC=704 AICc=704.13 BIC=709.11 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 0.3153345 9.032673 6.731484 0.07693443 2.923287 0.220178 ## ACF1 ## Training set 0.01032578 We get a model 1,1,0 model, implying an AR and integretion. As the ACF tend towards 0 and the pacf drastically drops after the first lag, we expected the AR(1) to be applicable, and since the data is clearly not stationary, the first order differences are also expected. Also we see (0,1,0)[12], meaning that each period is repeated for each twelve months Now we can make the model diagnostics. tsdisplay(residuals(fit) ,main = &#39;Model Residuals&#39;) We see that the residuals appear to be stationary. The ACF only has two spikes 9.2.1.1.2 - Continue here fcast &lt;- forecast(fit, h=l) plot(fcast) lines(y) accuracy(fcast, outsamp) ## ME RMSE MAE MPE MAPE MASE ## Training set 0.3153345 9.032673 6.731484 0.07693443 2.923287 0.3313104 ## Test set -1.4834898 22.132229 17.807808 -1.08004267 4.148973 0.8764653 ## ACF1 ## Training set 0.01032578 ## Test set NA 9.2.1.1.3 Forecast 2 - HoltWinters ###FORECAST 2### # # Now let us get a second forecast. Holt-Winters method could be a good choice. fit2 &lt;- HoltWinters(insamp) fcast2 &lt;- forecast(fit2, h=l) plot(fcast2) lines(y) accuracy(fcast2, outsamp) ## ME RMSE MAE MPE MAPE MASE ## Training set 1.830348 11.17066 8.422888 0.4909335 3.508573 0.414558 ## Test set -19.471423 28.81229 25.032002 -5.3401845 6.288076 1.232026 ## ACF1 ## Training set 0.4920761 ## Test set NA ##Compare and Combine forecast 1 and 2## # # We could also use a Diebold-Mariano test to see if these forecasts are significantly different from each other. dm.test(residuals(fcast), residuals(fcast2), h=l) #the null hypothesis is that the two methods have the same forecast accuracy. ## ## Diebold-Mariano Test ## ## data: residuals(fcast)residuals(fcast2) ## DM = -1.4435, Forecast horizon = 36, Loss function power = 2, p-value = ## 0.1518 ## alternative hypothesis: two.sided # # Finally let us check if combining these two forecasts will lead to an improvement in terms of RMSE. #Nelson combination method combfitN &lt;- lm(outsamp ~ fcast$mean + fcast2$mean) summary(combfitN) ## ## Call: ## lm(formula = outsamp ~ fcast$mean + fcast2$mean) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.673 -9.467 -0.076 11.159 29.892 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -120.4994 18.3512 -6.566 0.000000184 *** ## fcast$mean 0.6794 0.2895 2.347 0.0251 * ## fcast2$mean 0.5734 0.2828 2.028 0.0507 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.15 on 33 degrees of freedom ## Multiple R-squared: 0.9656, Adjusted R-squared: 0.9635 ## F-statistic: 463.4 on 2 and 33 DF, p-value: &lt; 0.00000000000000022 #the intercept is sgnificant =&gt; there is a bias, we need to correct the data for it outsampcor&lt;-outsamp-combfitN$coefficients[1] #where combfitN$coefficients[1] picks out the intercept value from the estimated regression # Now want to run an OLS without an intercept on the corrected (debiased data) #with respect to a restriction on the weights: w1 + w2 = 1 fitW &lt;- lm(outsampcor ~ 0+ offset(fcast$mean) + I(fcast2$mean-fcast$mean)) coef_2 &lt;- coef(fitW) beta_1 &lt;- 1 - coef_2 #the weight is negative, would prefer a different combination method in this case beta_2 &lt;- coef_2 #beta_1 and beta_2 will give you the weigths. # Now can use those weights to obtain a combination forecast combfcastN &lt;-beta_1*fcast$mean+beta_2*fcast2$mean plot(combfcastN) accuracy(combfcastN, outsamp) #can see that in this case the forecast combination performes worse than the individual forecasts ## ME RMSE MAE MPE MAPE ## Test set -99.4267 110.0852 99.4267 -24.27626 24.27626 #Granger-Ramanathan combination method combfit &lt;- lm(outsamp ~ fcast$mean + fcast2$mean) summary(combfit) #the coefficients in the regression will give you the weights ## ## Call: ## lm(formula = outsamp ~ fcast$mean + fcast2$mean) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.673 -9.467 -0.076 11.159 29.892 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -120.4994 18.3512 -6.566 0.000000184 *** ## fcast$mean 0.6794 0.2895 2.347 0.0251 * ## fcast2$mean 0.5734 0.2828 2.028 0.0507 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.15 on 33 degrees of freedom ## Multiple R-squared: 0.9656, Adjusted R-squared: 0.9635 ## F-statistic: 463.4 on 2 and 33 DF, p-value: &lt; 0.00000000000000022 combfcast &lt;- ts(combfit$fitted.values, frequency = 12) plot(combfcast) accuracy(combfcast, outsamp) ## ME RMSE MAE MPE MAPE ## Test set -0.000000000000003158498 14.50372 11.62376 -0.06542481 2.758189 library(readr) library(readxl) library(forecast) library(tseries) library(knitr) library(stats) library(car) "],["exam-cases.html", "10 Exam cases ", " 10 Exam cases "],["exam-2018.html", "10.1 Exam 2018", " 10.1 Exam 2018 df1 &lt;- read_excel(&quot;Data/Exams/Exam2018Case1.xlsx&quot;) #Case 1 material df2 &lt;- read_excel(&quot;Data/Exams/Exam2018Case2.xlsx&quot;) #Case 2 material 10.1.1 Case 1 Loading the data as a time series. The series are yearly starting from 1960 and ending in 2000 ts &lt;- ts(df1,frequency = 1,start = 1960) 10.1.1.1 Q1 1. Determine the key dynamic properties of each of the series. {tsdisplay(ts[,1],main = &quot;Short rates&quot;) #Short term rates tsdisplay(ts[,2],main = &quot;Medium rates&quot;) #Medium rates tsdisplay(ts[,3],main = &quot;Long rates&quot;)} #Long rates Figure 10.1: ts display of the time series Figure 10.2: ts display of the time series Figure 10.3: ts display of the time series We see that all series are non stationary, hence unit roots. Also it appears as if there is a trend, an clearly if one is looking at the first 20 years, then it is quite linear. 10.1.1.2 Q2 2. Justify a model specification for Long Rates (LR) and study post-regression residuals. Obtain forecasts for LR for the period 1991-2000 based on the in-sample period 1960-1990. How does your forecast compare to a simple moving average method? Finally, obtain a forecast also for the next 18 years (2001-2018) based on 1960-2000 data. A time series if the long term rates are is made and the the data is partitioned. y.ltn.train &lt;- ts[c(1:31),3] %&gt;% ts(frequency = 1,start = 1960) y.ltn.test &lt;- ts[c(32:41),3] tsdisplay(diff(y.ltn.train,1)) Figure 10.4: ltn inspection We see no significant spikes, hence implying a random walk. We saw earlier that the data was not stationary, hence we took first order difference. The data now look stationary, but has some extraordinary variance in the last years. Hence we can test this with an ADF, where H0: non stationarity. adf.test(diff(y.ltn.train,1)) ## ## Augmented Dickey-Fuller Test ## ## data: diff(y.ltn.train, 1) ## Dickey-Fuller = -3.5238, Lag order = 3, p-value = 0.05851 ## alternative hypothesis: stationary Based on the 5% level we are not able to reject H0, but it is very close to the significance level. It was found, that H0 can be rejected with d = 2, where p value would be ≈ 0,036, hence rejecting H0 on the five percent level. Although it would make the data more abstract from the actual data. Hence it is chosen to select the first order difference, well in mind, that it is not significant on the 5% level. But, that is also the probability of taking the wrong decision. So seeing that we have a random walk. We can assess the men of the data, and see where it lies. mean(diff(y.ltn.train)) ## [1] 0.316111 We see a mean of 0,3161 indicating, that the model has a drift. Thus we can make the following model, being an RW with a drift. fit &lt;- Arima(y.ltn.train,order = c(0,1,0),include.drift = TRUE) summary(fit) ## Series: y.ltn.train ## ARIMA(0,1,0) with drift ## ## Coefficients: ## drift ## 0.3161 ## s.e. 0.1652 ## ## sigma^2 estimated as 0.8469: log likelihood=-39.57 ## AIC=83.13 AICc=83.58 BIC=85.94 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 0.00009480282 0.8900961 0.5748442 -0.8163705 6.898777 0.8854342 ## ACF1 ## Training set 0.1662727 Now we are able to forecast 10.1.1.2.1 Forecasting using ARIMA Forecasting forecast(object = fit,h = 10) ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 1991 13.05444 11.87506 14.23382 11.25073 14.85815 ## 1992 13.37055 11.70265 15.03845 10.81972 15.92138 ## 1993 13.68666 11.64391 15.72941 10.56254 16.81078 ## 1994 14.00277 11.64401 16.36154 10.39535 17.61020 ## 1995 14.31888 11.68170 16.95607 10.28567 18.35210 ## 1996 14.63500 11.74611 17.52388 10.21683 19.05317 ## 1997 14.95111 11.83075 18.07146 10.17894 19.72328 ## 1998 15.26722 11.93142 18.60302 10.16555 20.36888 ## 1999 15.58333 12.04518 19.12148 10.17220 20.99446 ## 2000 15.89944 12.16990 19.62898 10.19561 21.60327 forecast.test &lt;- Arima(y = ts[,3] #The ltn ,model = fit) #We apply the model that was found above, hence 0,1,0 + drift knitr::kable(t(t(forecast.test$fitted[c(32:41)])),caption = &quot;Fitted values test period&quot;) Table 10.1: Fitted values test period 13.054441 11.430281 9.860281 10.242781 10.543611 10.237781 11.127781 10.122781 9.087781 8.192781 Now we can calculate accuracy accuracy(object = forecast.test$fitted[c(32:41)],x = y.ltn.test) ## ME RMSE MAE MPE MAPE ## Test set -0.731944 1.150236 0.9374434 -7.739819 9.837796 We see that we get an RMSE of 1.15 and MPE of -7.7 10.1.1.2.2 Forecasting with simple moving averages 1991 - 2000 Calculating simple moving average We know that the data is not stationary, hence we must make differencing. y.ltn &lt;- ts(ts[,3],start = 1960) dy.ltn &lt;- diff(y.ltn) dy.ltn.test &lt;- dy.ltn[31:40] ma.ltn &lt;- ma(x = dy.ltn,order = 3) plot(ma.ltn) + grid(col = &quot;lightgrey&quot;) ## integer(0) We see the MA’s above. We see that the MA’s are centered, that is not applicable for prediction purposes, as we want to predict using previous periods. FORECASTING WITH MA It is done with three different approaches: Using ma() not really good Using excel and importing, that is valid Using some loops, that is pretty cool 10.1.1.2.2.1 1. Using ma() USING MA This is not really applicable for forecasting. k &lt;- 3 #specify the order of the moving average c &lt;- rep (1/k,k) #remember that simple average is a weighted average with equal weights, #you need to specify weights for the filter command to work #Applying filter, to move the MA&#39;s to the end of the period yt3 &lt;- stats::filter(x = dy.ltn #The time-series ,filter = c #The filter (the wheights) ,sides = 1) #Plotting ts.plot(yt3) + grid(col = &quot;Lightgrey&quot;) #Plotting the MA&#39;s Figure 10.5: MA ltn data ## integer(0) Then one could make accuracy 10.1.1.2.2.2 2. Using excel Doing it in excel and then importing temp &lt;- read_excel(&quot;Data/temp.xlsx&quot;) ma3.excel &lt;- temp[,3] ma3.excel &lt;- ts(ma3.excel) accuracy ma.forecast.test &lt;- yt3[c(32:41)] accuracy(object = ma3.excel,x = dy.ltn.test) ## ME RMSE MAE MPE MAPE ## Test set -0.568935 1.061947 0.991435 129.1631 129.1631 10.1.1.2.2.3 3. Using a loop Using a loop for this purpose #Define the input data dy.ltn.train &lt;- dy.ltn[1:30] dy.ltn.test &lt;- dy.ltn[31:40] #Forecasting with a simple MA of order k k &lt;- 3 l &lt;- length(dy.ltn.train) #number of available data points #adding extra rows, as many as periods ahead want to forecast h &lt;-10 y &lt;- dy.ltn.train #generating space to save the forecasts for (i in 1:h){ y &lt;- c(y, 0) } t(t(y)) ## [,1] ## [1,] -0.06583 ## [2,] 0.41666 ## [3,] 0.51917 ## [4,] -0.01000 ## [5,] 0.93417 ## [6,] 0.14000 ## [7,] -0.13084 ## [8,] 0.04250 ## [9,] -0.01000 ## [10,] 0.09334 ## [11,] 0.01750 ## [12,] 0.48333 ## [13,] 0.21333 ## [14,] 0.82667 ## [15,] 0.83333 ## [16,] 0.40667 ## [17,] -1.02000 ## [18,] 0.28000 ## [19,] 0.32500 ## [20,] 1.32000 ## [21,] 0.12333 ## [22,] 0.23584 ## [23,] -0.53584 ## [24,] 0.54250 ## [25,] 0.94334 ## [26,] 2.15916 ## [27,] 2.65084 ## [28,] -0.60584 ## [29,] -2.61833 ## [30,] 0.97333 ## [31,] 0.00000 ## [32,] 0.00000 ## [33,] 0.00000 ## [34,] 0.00000 ## [35,] 0.00000 ## [36,] 0.00000 ## [37,] 0.00000 ## [38,] 0.00000 ## [39,] 0.00000 ## [40,] 0.00000 Now we see that we have extended the ts with 10 rows of 0. Corresponding with the period that we want to forecast. Now we can do the forecast #calculating the forecast values for (j in (l+1):(l+h)){ a&lt;-j-k b&lt;-j-1 x &lt;-y[a:b] y[j] &lt;- mean(x) } t(t(y)) ## [,1] ## [1,] -0.0658300 ## [2,] 0.4166600 ## [3,] 0.5191700 ## [4,] -0.0100000 ## [5,] 0.9341700 ## [6,] 0.1400000 ## [7,] -0.1308400 ## [8,] 0.0425000 ## [9,] -0.0100000 ## [10,] 0.0933400 ## [11,] 0.0175000 ## [12,] 0.4833300 ## [13,] 0.2133300 ## [14,] 0.8266700 ## [15,] 0.8333300 ## [16,] 0.4066700 ## [17,] -1.0200000 ## [18,] 0.2800000 ## [19,] 0.3250000 ## [20,] 1.3200000 ## [21,] 0.1233300 ## [22,] 0.2358400 ## [23,] -0.5358400 ## [24,] 0.5425000 ## [25,] 0.9433400 ## [26,] 2.1591600 ## [27,] 2.6508400 ## [28,] -0.6058400 ## [29,] -2.6183300 ## [30,] 0.9733300 ## [31,] -0.7502800 ## [32,] -0.7984267 ## [33,] -0.1917922 ## [34,] -0.5801663 ## [35,] -0.5234617 ## [36,] -0.4318067 ## [37,] -0.5118116 ## [38,] -0.4890267 ## [39,] -0.4775483 ## [40,] -0.4927955 accuracy(object = y[(length(y)-9):(length(y))],x = dy.ltn.test) ## ME RMSE MAE MPE MAPE ## Test set 0.1088786 0.8003623 0.7292919 94.18366 108.4159 plot(y,type = &#39;l&#39;,main = &quot;MA, k = 3&quot;) + abline(v = l,col = &quot;red&quot;) + grid(col = &quot;lightgrey&quot;) Figure 10.6: MA k=3 h=10 ## integer(0) Conclusion ARIMA RMSE = 1.15 MA3 RMSE = 0.800 Hence MA appear to be the better approach. 10.1.1.3 Forecasting 2001 - 2018 Forecasting 18 years 10.1.1.3.1 Using ma (loop method) #Define the input data dy.ltn &lt;- dy.ltn #Forecasting with a simple MA of order k k &lt;- 3 l &lt;- length(dy.ltn) #number of available data points #adding extra rows, as many as periods ahead want to forecast h &lt;-18 y &lt;- dy.ltn #generating space to save the forecasts for (i in 1:h){ y &lt;- c(y, 0) } t(t(y)) ## [,1] ## [1,] -0.06583 ## [2,] 0.41666 ## [3,] 0.51917 ## [4,] -0.01000 ## [5,] 0.93417 ## [6,] 0.14000 ## [7,] -0.13084 ## [8,] 0.04250 ## [9,] -0.01000 ## [10,] 0.09334 ## [11,] 0.01750 ## [12,] 0.48333 ## [13,] 0.21333 ## [14,] 0.82667 ## [15,] 0.83333 ## [16,] 0.40667 ## [17,] -1.02000 ## [18,] 0.28000 ## [19,] 0.32500 ## [20,] 1.32000 ## [21,] 0.12333 ## [22,] 0.23584 ## [23,] -0.53584 ## [24,] 0.54250 ## [25,] 0.94334 ## [26,] 2.15916 ## [27,] 2.65084 ## [28,] -0.60584 ## [29,] -2.61833 ## [30,] 0.97333 ## [31,] -1.62416 ## [32,] -1.57000 ## [33,] 0.38250 ## [34,] 0.30083 ## [35,] -0.30583 ## [36,] 0.89000 ## [37,] -1.00500 ## [38,] -1.03500 ## [39,] -0.89500 ## [40,] 0.70333 ## [41,] 0.00000 ## [42,] 0.00000 ## [43,] 0.00000 ## [44,] 0.00000 ## [45,] 0.00000 ## [46,] 0.00000 ## [47,] 0.00000 ## [48,] 0.00000 ## [49,] 0.00000 ## [50,] 0.00000 ## [51,] 0.00000 ## [52,] 0.00000 ## [53,] 0.00000 ## [54,] 0.00000 ## [55,] 0.00000 ## [56,] 0.00000 ## [57,] 0.00000 ## [58,] 0.00000 Now we see that we have extended the ts with 10 rows of 0. Corresponding with the period that we want to forecast. Now we can do the forecast #calculating the forecast values for (j in (l+1):(l+h)){ a&lt;-j-k b&lt;-j-1 x &lt;-y[a:b] y[j] &lt;- mean(x) } t(t(y)) ## [,1] ## [1,] -0.06583000 ## [2,] 0.41666000 ## [3,] 0.51917000 ## [4,] -0.01000000 ## [5,] 0.93417000 ## [6,] 0.14000000 ## [7,] -0.13084000 ## [8,] 0.04250000 ## [9,] -0.01000000 ## [10,] 0.09334000 ## [11,] 0.01750000 ## [12,] 0.48333000 ## [13,] 0.21333000 ## [14,] 0.82667000 ## [15,] 0.83333000 ## [16,] 0.40667000 ## [17,] -1.02000000 ## [18,] 0.28000000 ## [19,] 0.32500000 ## [20,] 1.32000000 ## [21,] 0.12333000 ## [22,] 0.23584000 ## [23,] -0.53584000 ## [24,] 0.54250000 ## [25,] 0.94334000 ## [26,] 2.15916000 ## [27,] 2.65084000 ## [28,] -0.60584000 ## [29,] -2.61833000 ## [30,] 0.97333000 ## [31,] -1.62416000 ## [32,] -1.57000000 ## [33,] 0.38250000 ## [34,] 0.30083000 ## [35,] -0.30583000 ## [36,] 0.89000000 ## [37,] -1.00500000 ## [38,] -1.03500000 ## [39,] -0.89500000 ## [40,] 0.70333000 ## [41,] -0.40889000 ## [42,] -0.20018667 ## [43,] 0.03141778 ## [44,] -0.19255296 ## [45,] -0.12044062 ## [46,] -0.09385860 ## [47,] -0.13561739 ## [48,] -0.11663887 ## [49,] -0.11537162 ## [50,] -0.12254263 ## [51,] -0.11818437 ## [52,] -0.11869954 ## [53,] -0.11980885 ## [54,] -0.11889759 ## [55,] -0.11913533 ## [56,] -0.11928059 ## [57,] -0.11910450 ## [58,] -0.11917347 plot(y,type = &#39;l&#39;,main = &quot;MA, k = 3&quot;) + abline(v = l,col = &quot;red&quot;) + grid(col = &quot;lightgrey&quot;) Figure 10.7: MA k=3 h=18 ## integer(0) 10.1.1.3.2 Using ARIMA (RW + drift) As we have a random walk with a drift, we merely see that the forecast follow the drift, which consist of the mean of the train data set. forecast(object = forecast.test,h = 18) %&gt;% plot() &lt;img src=“Business-Forecasting-Notebook_files/figure-html/fig.cap”Forecast with RW + drift“-1.png” width=“480” style=“display: block; margin: auto;” /&gt; 10.1.2 Q3 3. In a multiple regression framework, determine the relationship of LR with SR and MR. Study whether the residuals satisfy the OLS assumptions. df1 &lt;- read_excel(&quot;Data/Exams/Exam2018Case1.xlsx&quot;) #Case 1 material stn &lt;- df1[,1] %&gt;% ts(start = 1960) mtn &lt;- df1[,2] %&gt;% ts(start = 1960) ltn &lt;- df1[,3] %&gt;% ts(start = 1960) fit &lt;- lm(ltn ~ mtn + stn) summary(fit) ## ## Call: ## lm(formula = ltn ~ mtn + stn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.8618 -0.2749 -0.0409 0.1577 0.9019 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.07300 0.22196 -0.329 0.744 ## mtn 1.43944 0.09113 15.795 &lt; 0.0000000000000002 *** ## stn -0.40008 0.07491 -5.341 0.00000456 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3856 on 38 degrees of freedom ## Multiple R-squared: 0.9842, Adjusted R-squared: 0.9834 ## F-statistic: 1185 on 2 and 38 DF, p-value: &lt; 0.00000000000000022 We see that there is statistical evidence for relationship between the variables. Although we may have a concern for multicollinearity. vif(fit) ## mtn stn ## 20.91306 20.91306 We see that the values far above 10, hence according to the rule of thumb, we have multicollinearity. Meaning that we should remove one of the variables, as they explain the same. 10.1.2.1 Diagnostics 10.1.2.1.1 Check for heteroskedasticity plot(residuals(fit)) + abline(h = 0) + grid(col = &quot;grey&quot;) Figure 10.8: Residuals plot of the fit ## integer(0) It appears as if there can be heteroskedasticity in the residuals, meaning that there is information in the data, that is not explained by our model, hence the variance and the mean changes over time. Although it is a good idea to make a test for this. Here a Breusch-Pagan test: bptest(fit) ## ## studentized Breusch-Pagan test ## ## data: fit ## BP = 0.27942, df = 2, p-value = 0.8696 We see that there is in fact not enough evidence to reject the null, meaning it is fair to assume that the H0 is true, hence have homoskedasticity in the residuals. 10.1.2.1.2 Check for independent residuals adf.test(resid(fit)) ## ## Augmented Dickey-Fuller Test ## ## data: resid(fit) ## Dickey-Fuller = -3.2478, Lag order = 3, p-value = 0.09348 ## alternative hypothesis: stationary We reject H0, hence it is fair to assume that the residuals are non stationary, i.e. has unit root. One could also apply Durbin-Watson test, to check for independent residuals. 10.1.2.1.3 Checking residuals for normality Therefore, we can also plot a histogram of the data to assess the normal We use a Jarque Bera test, where H0: Normal distribution tseries::jarque.bera.test(resid(fit)) ## ## Jarque Bera Test ## ## data: resid(fit) ## X-squared = 1.5466, df = 2, p-value = 0.4615 hist(resid(fit)) Figure 10.9: Check for normality Based on the JB test, it is in fact normal distribution in the residuals. This can be seen from the histogram as well, although there is some small skewness. 10.1.2.1.4 Conclusion We do not meet the requirements, as we observe the following issues: Multicollinearity Autocorrelation 10.1.3 Q4 4. If residuals from the previous regression are satisfactory, interpret the estimation output from the previous step. Otherwise, make the necessary transformations so that you obtain valid estimates, and compare how the results change. 10.1.3.1 Test without integration We try to remove one variable to get rid of multicollinearity and then see how it affects the model. { fit &lt;- lm(ltn ~ stn) bptest(fit) %&gt;% print() #Check for independent residuals adf.test(resid(fit)) %&gt;% print() #Test for autocorrelation dwtest(fit) %&gt;% print() #Test for autocorrelation jarque.bera.test(resid(fit)) %&gt;% print() #Check for normality in the residuals } ## ## studentized Breusch-Pagan test ## ## data: fit ## BP = 2.0956, df = 1, p-value = 0.1477 ## ## ## Augmented Dickey-Fuller Test ## ## data: resid(fit) ## Dickey-Fuller = -3.8954, Lag order = 3, p-value = 0.02361 ## alternative hypothesis: stationary ## ## ## Durbin-Watson test ## ## data: fit ## DW = 0.82013, p-value = 0.000006044 ## alternative hypothesis: true autocorrelation is greater than 0 ## ## ## Jarque Bera Test ## ## data: resid(fit) ## X-squared = 1.5021, df = 2, p-value = 0.4719 We see that the ADF show no non stationarity, hence autocorrelation, but the DW does show non stationarity (hence autocorrelation) We can check for cointegration, to see if they are in fact cointegrated, hence it is acceptable to have variables, that are not stationary. This can be check with the Phillips-Ouliaris test (2-step EG test). z &lt;- ts(cbind(stn,ltn)) po.test(z) #Note, this is the 2-step EG test ## ## Phillips-Ouliaris Cointegration Test ## ## data: z ## Phillips-Ouliaris demeaned = -19.899, Truncation lag parameter = 0, ## p-value = 0.05735 We see that the p-value is 5.7% hence there is not sufficient evidence to reject on the five percent level, hence it is questionable. Sub conclusion The variables are not cointegrated, hence one should make the data series’ stationary by first order integration. 10.1.3.2 Test with first order integration { fit &lt;- lm(diff(ltn) ~ diff(stn)) bptest(fit) %&gt;% print() #Check for independent residuals adf.test(resid(fit)) %&gt;% print() #Test for autocorrelation dwtest(fit) %&gt;% print() #Test for autocorrelation jarque.bera.test(resid(fit)) %&gt;% print() #Check for normality in the residuals } ## ## studentized Breusch-Pagan test ## ## data: fit ## BP = 0.07672, df = 1, p-value = 0.7818 ## ## ## Augmented Dickey-Fuller Test ## ## data: resid(fit) ## Dickey-Fuller = -2.889, Lag order = 3, p-value = 0.2246 ## alternative hypothesis: stationary ## ## ## Durbin-Watson test ## ## data: fit ## DW = 1.9979, p-value = 0.4942 ## alternative hypothesis: true autocorrelation is greater than 0 ## ## ## Jarque Bera Test ## ## data: resid(fit) ## X-squared = 2.8678, df = 2, p-value = 0.2384 10.1.4 Q5 5. Based on the valid regression you obtain in the previous step, produce an 18-year (2001-2018) forecast for LR, assuming the estimated coeficients remain the same for the 18-year duration. Compare your findings to the forecasts you obtained in step 2. Interpret what leads to the diference, if any, in the forecasts. Skipped, hasnt done similar during the semester 10.1.5 Q6 6. Treat these three variables in a VAR setting, justifying the number of lags you use. Interpret the estimation output. Why are you getting these results? If possible, justify a Cholesky ordering, plot Impulse Responses and Forecast Error Variance Decompositions, and interpret them. This was not really covered during lectures. we did VAR in bivariate setting 10.1.5.1 Vector AutoRegressive Model (VAR) df1 &lt;- read_excel(&quot;Data/Exams/Exam2018Case1.xlsx&quot;) #Case 1 material sr &lt;- ts(df1$SR,start = 1960) mr &lt;- ts(df1$MR,start = 1960) lr &lt;- ts(df1$LR,start = 1960) { tsdisplay(sr) tsdisplay(mr) tsdisplay(lr) } We see that the series’ are not stationary. Hence, we must do first order differencing. dsr &lt;- ts(df1$SR,start = 1960) %&gt;% diff() dmr &lt;- ts(df1$MR,start = 1960) %&gt;% diff() dlr &lt;- ts(df1$LR,start = 1960) %&gt;% diff() par(mfrow = c(3,1)) plot(dsr,main = &quot;Diff. SR&quot;) + grid(col = &quot;grey&quot;) ## integer(0) plot(dmr,main = &quot;Diff. MR&quot;) + grid(col = &quot;grey&quot;) ## integer(0) plot(dlr,main = &quot;Diff. LR&quot;) + grid(col = &quot;grey&quot;) Figure 10.10: Plotting time series ## integer(0) We see that the data now appear stationary, and we are able to work with the series’ {acf(dsr) acf(dmr) acf(dlr)} Figure 10.11: Correlogram of the series’ Figure 10.12: Correlogram of the series’ Figure 10.13: Correlogram of the series’ 10.1.5.1.1 1. Determine order of lags to be included + model estimation dz &lt;- ts(cbind(sr,mr,lr),start = 1960) #dz = differenced z, where z equal the dataframe dz &lt;- diff(dz) VARselect(y = dz ,lag.max = 5 #if = 10, then AIC becomes -Inf, then it will select that, although that is faulty ,type = &quot;const&quot;) ## $selection ## AIC(n) HQ(n) SC(n) FPE(n) ## 1 1 1 1 ## ## $criteria ## 1 2 3 4 5 ## AIC(n) -2.1319476 -1.9610699 -1.9981813 -1.71593364 -1.5732805 ## HQ(n) -1.9478656 -1.6389263 -1.5379762 -1.11766703 -0.8369524 ## SC(n) -1.5986854 -1.0278611 -0.6650258 0.01716849 0.5597683 ## FPE(n) 0.1189635 0.1430338 0.1424012 0.20108463 0.2582111 According to all IC, we should set p = 1, hence \\(VAR(p=1)\\) Now can estimate the model, using the lag order, that we just found. var1 &lt;- VAR(y = dz ,p = 1 #How many lags to be included in the models, e.g., if p = 3, # then three lags of each variable is included. ,type = &quot;const&quot;) summary(var1) ## ## VAR Estimation Results: ## ========================= ## Endogenous variables: sr, mr, lr ## Deterministic variables: const ## Sample size: 39 ## Log Likelihood: -112.684 ## Roots of the characteristic polynomial: ## 0.307 0.307 0.1904 ## Call: ## VAR(y = dz, p = 1, type = &quot;const&quot;) ## ## ## Estimation results for equation sr: ## =================================== ## sr = sr.l1 + mr.l1 + lr.l1 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## sr.l1 -0.1984 0.5247 -0.378 0.708 ## mr.l1 1.0410 1.4028 0.742 0.463 ## lr.l1 -0.9496 1.1081 -0.857 0.397 ## const 0.1280 0.3363 0.381 0.706 ## ## ## Residual standard error: 2.075 on 35 degrees of freedom ## Multiple R-Squared: 0.02227, Adjusted R-squared: -0.06154 ## F-statistic: 0.2657 on 3 and 35 DF, p-value: 0.8497 ## ## ## Estimation results for equation mr: ## =================================== ## mr = sr.l1 + mr.l1 + lr.l1 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## sr.l1 0.005615 0.327310 0.017 0.986 ## mr.l1 0.599295 0.875110 0.685 0.498 ## lr.l1 -0.697005 0.691234 -1.008 0.320 ## const 0.151959 0.209820 0.724 0.474 ## ## ## Residual standard error: 1.294 on 35 degrees of freedom ## Multiple R-Squared: 0.04974, Adjusted R-squared: -0.03171 ## F-statistic: 0.6107 on 3 and 35 DF, p-value: 0.6126 ## ## ## Estimation results for equation lr: ## =================================== ## lr = sr.l1 + mr.l1 + lr.l1 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## sr.l1 0.07892 0.24608 0.321 0.750 ## mr.l1 0.30298 0.65792 0.461 0.648 ## lr.l1 -0.30392 0.51968 -0.585 0.562 ## const 0.13780 0.15775 0.874 0.388 ## ## ## Residual standard error: 0.973 on 35 degrees of freedom ## Multiple R-Squared: 0.08991, Adjusted R-squared: 0.0119 ## F-statistic: 1.153 on 3 and 35 DF, p-value: 0.3416 ## ## ## ## Covariance matrix of residuals: ## sr mr lr ## sr 4.304 2.482 1.6354 ## mr 2.482 1.675 1.1849 ## lr 1.635 1.185 0.9468 ## ## Correlation matrix of residuals: ## sr mr lr ## sr 1.0000 0.9243 0.8101 ## mr 0.9243 1.0000 0.9409 ## lr 0.8101 0.9409 1.0000 We see that the variables are not significant to each other and not even the dependent variables lag 1, is not significant, and not even close to. Conclusion 10.1.5.1.2 2. Model diagnostics Now we can make model diagnostics to check for autocorrelation (serial correlation). This executes a Portmanteau test for correlation in the errors (i.e., autocorrelation, i.e., serial correlation). The null hypothesis is that there are no autocorrelation serial.test(var1 ,lags.pt = 10 #It is chosen to be 10, could be anything else, perhaps one could plot it ,type = &quot;PT.asymptotic&quot; ) ## ## Portmanteau Test (asymptotic) ## ## data: Residuals of VAR object var1 ## Chi-squared = 81.77, df = 81, p-value = 0.4552 As the p-value is far above the significane level, 5%, we cannot reject the null hypothesis, hence it is fair to assume, that the residuals are not serial correlated. 10.1.5.1.3 3. Response to shocks in the variables With the following, we are able to deduce how the different variables react (respond) from shocks in the variables. The y-axis expres the changes where the x-axis express the n steps ahead. Hence in this example it is quarters ahead. plot(irf(var1,boot = TRUE, ci=0.95)) Figure 10.14: Resonses from shocks Figure 10.15: Resonses from shocks Figure 10.16: Resonses from shocks 10.1.5.1.3.1 Exogenious vs. endogenious 10.1.5.1.4 4. Forecasting with ADL 10.1.5.1.4.1 Step 1 - Data partition 10.1.5.1.4.2 Step 2 - Select the order of VAR 10.1.5.1.4.3 Step 3 - Fit the model + check residuals 10.1.5.1.4.4 Step 4 - Make the forecast 10.1.5.1.4.5 Step 5 - Assessing accuracy 10.1.6 Q7 7. Test for unit roots in each of the series, and determine whether you can obtain a cointegrating relationship. Interpret what your findings suggest. This was addressed earlier, see the process Q4 rm(list = ls()) 10.1.7 Case 2 df2 &lt;- read_excel(&quot;Data/Exams/Exam2018Case2.xlsx&quot;) #Case 2 material 10.1.7.1 Q1 dynamic properties y.badcalls &lt;- ts(df2[,4],frequency = 24) tsdisplay(y.badcalls) Figure 10.17: Visual inspection Based on the inspection, we may deduce the following: It appears as if we have non stationarity - that can be tested with DW or ADF It appears as if there is seasonality - that can be tested with decomposition Testing for stationarity: adf.test(y.badcalls) ## ## Augmented Dickey-Fuller Test ## ## data: y.badcalls ## Dickey-Fuller = -9.5429, Lag order = 12, p-value = 0.01 ## alternative hypothesis: stationary We see that the data actually is stationary A test of decomposing is done in the following question. On the basis hereon, we are able to deduce that the composition is additive 10.1.7.2 Q2 an appropriate decomposition with explanations on each component plot(decompose(y.badcalls)) Figure 10.18: Decompisition of Bad Calls We see that there is no trend in the data, but there is clearly seasons, depending on what hour it is. We also see no cycles. 10.1.7.3 Q3 linar regression bad calls on the other variables Checking for stationarity in the explanatory variables x.pressure &lt;- ts(df2[,2],frequency = 24) x.windspeed &lt;- ts(df2[,3],frequency = 24) {print(tsdisplay(x.pressure,lag.max = 500)) print(tsdisplay(x.windspeed,lag.max = 500))} ## NULL ## NULL {print(adf.test(x.pressure)) print(adf.test(x.windspeed))} ## ## Augmented Dickey-Fuller Test ## ## data: x.pressure ## Dickey-Fuller = -5.4916, Lag order = 12, p-value = 0.01 ## alternative hypothesis: stationary ## ## ## Augmented Dickey-Fuller Test ## ## data: x.windspeed ## Dickey-Fuller = -7.2395, Lag order = 12, p-value = 0.01 ## alternative hypothesis: stationary Despite the visual interpretation showing characteristics that appear to be non stationary, the ADF test show overwhelming evidence to reject the null, hence it is fair to assume, that the data is stationer fit &lt;- lm(y.badcalls ~ x.pressure + x.windspeed) summary(fit) ## ## Call: ## lm(formula = y.badcalls ~ x.pressure + x.windspeed) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.70804 -0.20978 0.02901 0.18513 1.07707 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.486495 1.004297 11.44 &lt;0.0000000000000002 *** ## x.pressure -0.353204 0.033296 -10.61 &lt;0.0000000000000002 *** ## x.windspeed 0.043073 0.001359 31.69 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2654 on 2013 degrees of freedom ## Multiple R-squared: 0.4803, Adjusted R-squared: 0.4798 ## F-statistic: 930.2 on 2 and 2013 DF, p-value: &lt; 0.00000000000000022 One could describe this then 10.1.7.4 Q4 a forecast obtained based on an appropriate smoothing method HW &lt;- HoltWinters(y.badcalls ,beta = FALSE #No trend # ,gamma = TRUE #We have seasons #We want it to calculate the optimal ) HW ## Holt-Winters exponential smoothing without trend and with additive seasonal component. ## ## Call: ## HoltWinters(x = y.badcalls, beta = FALSE) ## ## Smoothing parameters: ## alpha: 0.7433923 ## beta : FALSE ## gamma: 0.5407147 ## ## Coefficients: ## [,1] ## a 1.0597021787 ## s1 -0.1794473089 ## s2 -0.2712036049 ## s3 -0.3205401341 ## s4 -0.3207574608 ## s5 -0.3321956847 ## s6 -0.2810281488 ## s7 -0.2685732309 ## s8 -0.1190127729 ## s9 -0.0124866501 ## s10 0.0978011268 ## s11 0.1868982608 ## s12 0.2035583267 ## s13 0.2205927840 ## s14 0.1968599641 ## s15 0.1681208212 ## s16 0.1624518505 ## s17 0.1702619587 ## s18 0.1919992681 ## s19 0.1975399933 ## s20 0.1793936233 ## s21 0.1453069693 ## s22 0.0598995701 ## s23 0.0005213328 ## s24 -0.1149900150 The model is created with seasonality but not trend hence, Holt Winters smoothing. The function optimize to the optimal parameters. forecast(HW,h = 24) ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 85.00000 0.8802549 0.7489360 1.0115738 0.6794199 1.081090 ## 85.04167 0.7884986 0.6248691 0.9521280 0.5382489 1.038748 ## 85.08333 0.7391620 0.5486245 0.9296996 0.4477600 1.030564 ## 85.12500 0.7389447 0.5248548 0.9530347 0.4115224 1.066367 ## 85.16667 0.7275065 0.4922100 0.9628030 0.3676515 1.087361 ## 85.20833 0.7786740 0.5239302 1.0334178 0.3890770 1.168271 ## 85.25000 0.7911289 0.5183207 1.0639372 0.3739047 1.208353 ## 85.29167 0.9406894 0.6509407 1.2304381 0.4975570 1.383822 ## 85.33333 1.0472155 0.7414636 1.3529675 0.5796083 1.514823 ## 85.37500 1.1575033 0.8365450 1.4784616 0.6666400 1.648367 ## 85.41667 1.2466004 0.9111244 1.5820765 0.7335342 1.759667 ## 85.45833 1.2632605 0.9138695 1.6126516 0.7289130 1.797608 ## 85.50000 1.2802950 0.9175222 1.6430677 0.7254819 1.835108 ## 85.54167 1.2565621 0.8808841 1.6322402 0.6820121 1.831112 ## 85.58333 1.2278230 0.8396684 1.6159776 0.6341918 1.821454 ## 85.62500 1.2221540 0.8219117 1.6223963 0.6100362 1.834272 ## 85.66667 1.2299641 0.8179886 1.6419397 0.5999019 1.860026 ## 85.70833 1.2517014 0.8283177 1.6750852 0.6041919 1.899211 ## 85.75000 1.2572422 0.8227496 1.6917347 0.5927432 1.921741 ## 85.79167 1.2390958 0.7937715 1.6844201 0.5580311 1.920161 ## 85.83333 1.2050091 0.7491104 1.6609079 0.5077722 1.902246 ## 85.87500 1.1196017 0.6533683 1.5858352 0.4065593 1.832644 ## 85.91667 1.0602235 0.5838796 1.5365675 0.3317183 1.788729 ## 85.95833 0.9447122 0.4584679 1.4309565 0.2010657 1.688359 We see the above produce point forecasts + confidence intervals. The following plots the forecast plot(forecast(HW,h = 24),xlim = c(75,87)) Figure 10.19: HW forecast smoothing 10.1.7.5 Q5 Using ARIMA arima.bc &lt;- auto.arima(y.badcalls) forecast(arima.bc,h = 24) ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 85.00000 0.8952382 0.7597392 1.0307372 0.6880104 1.102466 ## 85.04167 0.7915810 0.6146908 0.9684711 0.5210508 1.062111 ## 85.08333 0.6938403 0.4895408 0.8981398 0.3813912 1.006289 ## 85.12500 0.6974756 0.4730226 0.9219285 0.3542044 1.040747 ## 85.16667 0.7282253 0.4884601 0.9679905 0.3615361 1.094915 ## 85.20833 0.8758832 0.6232167 1.1285497 0.4894632 1.262303 ## 85.25000 0.8537510 0.5902471 1.1172550 0.4507565 1.256746 ## 85.29167 0.9649759 0.6923134 1.2376385 0.5479745 1.381977 ## 85.33333 1.0753636 0.7949200 1.3558072 0.6464621 1.504265 ## 85.37500 1.2120036 0.9249268 1.4990804 0.7729575 1.651050 ## 85.41667 1.3387255 1.0459649 1.6314861 0.8909867 1.786464 ## 85.45833 1.3620451 1.0643968 1.6596933 0.9068313 1.817259 ## 85.50000 1.3819627 1.0800996 1.6838259 0.9203029 1.843623 ## 85.54167 1.3872987 1.0817924 1.6928050 0.9200672 1.854530 ## 85.58333 1.3670515 1.0583905 1.6757125 0.8949952 1.839108 ## 85.62500 1.3303459 1.0189487 1.6417430 0.8541050 1.806587 ## 85.66667 1.2923932 0.9786197 1.6061666 0.8125181 1.772268 ## 85.70833 1.2818309 0.9659914 1.5976704 0.7987960 1.764866 ## 85.75000 1.2748361 0.9571985 1.5924738 0.7890513 1.760621 ## 85.79167 1.2786251 0.9594213 1.5978289 0.7904451 1.766805 ## 85.83333 1.2578344 0.9372655 1.5784033 0.7675666 1.748102 ## 85.87500 1.1380432 0.8162837 1.4598027 0.6459545 1.630132 ## 85.91667 1.0700041 0.7472057 1.3928025 0.5763266 1.563682 ## 85.95833 0.9407264 0.6170210 1.2644318 0.4456617 1.435791 plot(forecast(arima.bc,h = 24),xlim = c(75,87)) 10.1.7.6 Q6 a forecast combination of the two previously chosen methods and their evaluations Let us use the HoltWinters smoothing and the arima from q4 and q5. First we must find out if it is fair to assume, that the models perform equally good. That can be done with Diebold-Mariano test. Where H0: is that the MSE’s are not significantly far from each other. dm.test(e1 = resid(HW) ,e2 = resid(arima.bc)[25:2016] ,alternative = &quot;two.sided&quot; #Default, but shown for pedogical reasons. meaning that H1 is a two sided test ) ## ## Diebold-Mariano Test ## ## data: resid(HW)resid(arima.bc)[25:2016] ## DM = -1.7642, Forecast horizon = 1, Loss function power = 2, p-value = ## 0.07786 ## alternative hypothesis: two.sided We are not able to reject the null hypothesis, hence the accuracies can be assumed to be the same, hence equally good models. 10.1.7.6.1 Combined model Now we can make the combined model, this can be done with two approaches: Nelson Granger-Ramanathan comb.df &lt;- as.data.frame(cbind(as.matrix(y.badcalls[25:2016]) ,as.vector(HW$fitted[,1]) ,as.vector(arima.bc$fitted[25:2016]))) names(comb.df) &lt;- c(&quot;y.badcalls&quot;,&quot;x.HW&quot;,&quot;x.arima&quot;) combfit &lt;- lm(y.badcalls ~ x.HW + x.arima ,data = comb.df) summary(combfit) #the coefficients in the regression will give you the weights ## ## Call: ## lm(formula = y.badcalls ~ x.HW + x.arima, data = comb.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.91363 -0.04814 -0.00527 0.04445 1.19600 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.060838 0.007844 7.756 0.0000000000000139 *** ## x.HW 0.693959 0.042933 16.164 &lt; 0.0000000000000002 *** ## x.arima 0.255147 0.044773 5.699 0.0000000138793768 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.09884 on 1989 degrees of freedom ## Multiple R-squared: 0.9279, Adjusted R-squared: 0.9278 ## F-statistic: 1.28e+04 on 2 and 1989 DF, p-value: &lt; 0.00000000000000022 combfcast &lt;- ts(combfit$fitted.values, frequency = 24) plot(combfcast) accuracy(combfcast,x = comb.df$y.badcalls) ## ME RMSE MAE MPE MAPE ## Test set -0.000000000000000001959061 0.09877032 0.06512255 -0.7175288 5.916592 "],["communicating-technical-findings.html", "11 Communicating Technical Findings", " 11 Communicating Technical Findings This chapter is about efficient report writing Litteratur Link to article: https://www.wgtn.ac.nz/learning-teaching/support/approach/steps-to-teaching-success/resources/WSBG-report-writing-guide-2017.pdf Lectures Lecture 6 The following path contain a folder with a template for reports: Business Forecasting/Week 46 Simple Linear Regression/BusinessReportRMD "],["Methods.html", "12 Methods and Performance Measurement ", " 12 Methods and Performance Measurement "],["ForecastMethods.html", "12.1 Forecasting Methods", " 12.1 Forecasting Methods Naive forecasts This is merely the current period is assumed to be the best predictor for the future, hence it can be written as: \\[\\begin{equation} \\hat{Y}_{t+1}=Y_t \\tag{12.1} \\end{equation}\\] where, \\(Y_t\\) = the last period, hence \\(\\hat{Y}_{t+1}\\) = the following period. Therefore, the error can merely be written as: \\(e_t=Y_{t+1}-\\hat{Y}_{t+1}\\), being the actual amount compared with the foretasted value. One can make several iterations to account for trending, the growth rate, or seasonal data. Those being: \\(\\hat{Y}_{t+1}=Y_t+(Y_t-Y_{t-1})\\), to account for trending data (non stationary data) \\(\\hat{Y}_{t+1}=Y_t * \\frac{Y_t}{Y_{t-1}}\\), to account for the growth rate, notice that it only assess the growth rate to the prior period. \\(\\hat{Y}_{t+1}=Y_{t-3}+\\frac{Y_t-Y_{t-4}}{4}\\), to account for quarterly trending data, the periods can naturally be changed by changing the formula, e.g. to 12. but notice, that this is just replicating previous periods, hence also previous seasons 12.1.1 Using Averages We have the following: Simple averages, which merely takes the average of all observations. Moving averages, which account for the given time frame. This can be extended by, Double moving averages, often seen when you need the center value of a period consisting of an even number of periods, where there is no actual median value, thus one can extend the MA with a double MA. 12.1.1.1 Simple Averages One may assume that it is sufficient to apply the average of all observations, to predict the next period, hence we can say: \\[\\begin{equation} \\hat{Y}_{t+1}=\\frac{1}{n}\\sum^t_{i=1}Y_i \\tag{12.2} \\end{equation}\\] This is appropriate if the data has shown historical stability, thus without seasons, trends and etc. 12.1.1.2 Moving Average (MA) One may apply a moving average instead, accounting for k periods, also one could extend this by adding weights. For practical purposes only a k period MA is show: \\[\\begin{equation} \\hat{Y}_{t+1}=\\frac{Y_t+Y_{t-1}+...+Y_{t-k}}{k} \\tag{12.3} \\end{equation}\\] this may be applied to remove seasonal effect, either by k=4 or 12 if the data is respectively quarterly or monthly 12.1.1.3 Double Moving Average This is simply doing moving averages twice, hence it is an extension of equation (12.3) As mentioned, often seen when one wants the median when using an even number of periods, e.g. 12 months, hence double MA can be applied. \\[\\begin{equation} M_t=\\hat{Y}_{t+1}=\\frac{Y_t+Y_{t-1}+...+Y_{t-k+1}}{k} \\tag{12.4} \\end{equation}\\] \\[\\begin{equation} M&#39;_t=\\frac{M_t+M_{t-1}+...+M_{t-k+1}}{k} \\tag{12.5} \\end{equation}\\] \\[\\begin{equation} a_t=M_t+\\left(M_t-M_t\\right) \\end{equation}\\] \\[\\begin{equation} b_t=\\frac{2}{k-1}\\left(M_t-M&#39;_t\\right) \\end{equation}\\] Thence we are able to say: \\[\\begin{equation} \\hat{Y}_{t+p}=a_t+b_t*p \\tag{12.6} \\end{equation}\\] 12.1.2 Linear regressions Linear regression with a trend: that is normal linear regression, where the trend is added as a counter, which will account for the trend, given it is linear. Linear regression with seasonal dummies and a trend: That is making dummy variables for each period, using seasonaldummary(), to have a variable accounting for each period. The you can add a trend variable seq(1,n,1), e.g., linear counter, when having linear trend. See 6.5.5 12.1.3 Non linear regressions Non linear regression with trend Causal regression 12.1.4 Smoothing methods 12.1.4.1 Exponential smoothing This is exponentially weighted moving average of all historical values, meaning that the most recent value will be assigned the most weight. Hence we merely add different weights to past periods, thus there is no specific way to adjust for trend and seasonality, which is a limitation of exponential smoothing, it can be written as: \\[\\begin{equation} \\hat{Y}_{t+1}=\\alpha Y_t+\\left(1-\\alpha\\right)\\hat{Y}_t \\end{equation}\\] thus: \\[\\begin{equation} =\\hat{Y}_t+\\alpha(Y_t-\\hat{Y}_t) \\end{equation}\\] Where \\(\\alpha\\) = the smoothing constant, thus is can be between 0 and 1. The higher alpha the largest weight to the most recent observation. Then how to choose the smoothing parameter \\(\\alpha\\)? For stable predictions, choose a high alpha For sensitive predictions, choose low alpha Test different alpha values and compare the models using the performance measures in section 12.2. 12.1.4.2 Holt’s exponential smoothing Exponential smoothing method with adjustment for trend, hence we introduce a new tuning parameter, hence we have \\(\\alpha\\) and \\(\\beta\\) \\(\\alpha\\) = Weight to the most recent observations \\(\\beta\\) = adjustment for trend. R will automatically set this, when beta = TRUE Hence the smoothing now consists of two elements: The level estimate \\[\\begin{equation} L_t=\\alpha Y_t+\\left(1-\\alpha\\right)\\left(L_{t-1}+T_{t-1}\\right) \\tag{12.7} \\end{equation}\\] The trend estimate \\[\\begin{equation} T_t=\\beta\\left(L_t-L_{t-1}\\right)+\\left(1-\\beta\\right)T_{t-1} \\tag{12.8} \\end{equation}\\] Thus, the forecasting of p periods into the future, can be explained by: \\[\\begin{equation} \\hat{Y}_{t+p}=L_t+pT_t \\tag{12.9} \\end{equation}\\] To apply: use HoltWinters() and select parameters that lowers the performance measurements. When one assigns large weights the model will become more sensitive to changes in the observed data. One can either set the initial value to 0 or take the average of the first few observations. If \\(\\alpha = \\beta\\), then we have the Brown’s double exponential smoothing model. If \\(\\beta\\) = 0, then we merely have a simple exponential smoothing. 12.1.4.3 Winters’ exponential smoothing Exponential smoothing method with adjustment for trend and seasonality, hence we introduce two new tuning parameters, hence we have \\(\\alpha\\), \\(\\beta\\) (as in Holt’s) and \\(\\gamma\\) Hence the smoothing now consists of three elements: The level estimate \\[\\begin{equation} L_t=\\alpha\\frac{Y_t}{S_{t-s}}+\\left(1-\\alpha\\right)\\left(L_{t-1}+T_{t-1}\\right) \\tag{12.10} \\end{equation}\\] The trend estimate \\[\\begin{equation} T_t=\\beta\\left(L_t-L_{t-1}\\right)+\\left(1-\\beta\\right)T_{t-1} \\tag{12.11} \\end{equation}\\] The seasonality estimate \\[\\begin{equation} S_t=\\gamma\\frac{Y_t}{L_t}+\\left(1-\\gamma\\right)S_{t-s} \\tag{12.12} \\end{equation}\\] Thus, the forecasting of p periods into the future, can be explained by: \\[\\begin{equation} \\hat{Y}_{t+p}=\\left(L_t+pT_t\\right)S_{t-s+p} \\tag{12.13} \\end{equation}\\] To apply: use HoltWinters() and select parameters that lowers the performance measurements. When one assigns large weights the model will become more sensitive to changes in the observed data. One can either set the initial value to 0 or take the average of the first few observations. if \\(\\beta = \\gamma = 0\\) the model is merely simple exponential smoothing. 12.1.4.4 Moving Averages, see section 12.1.1.2 12.1.5 ARIMA Decomposition of the time series AR: MA: ARMA: ARIMA: "],["PerformanceMeasurements.html", "12.2 Performance Measurements", " 12.2 Performance Measurements 12.2.1 Error terms Mean error (ME): \\[\\begin{equation} ME=\\frac{1}{n}\\sum_{ }^{ }\\left(Y_t-\\hat{Y}_t\\right)$ \\tag{12.14} \\end{equation}\\] Mean Absolute Deviation (error): \\[\\begin{equation} MAD\\left(i.e.\\ MAE\\right)\\ =\\ \\frac{1}{n}\\cdot\\sum_{ }^{ }\\left|Y_t-\\hat{Y}_t\\right| \\tag{12.15} \\end{equation}\\] Mean Percentage Error (MPE): \\[\\begin{equation} MPE\\ =\\ \\frac{1}{n}\\ \\sum_{ }^{ }\\frac{\\left(Y_t-\\hat{Y}_t\\right)}{Y_t} \\tag{12.16} \\end{equation}\\] Mean Absolute Percentage Error (MAPE): \\[\\begin{equation} MAPE\\ =\\ \\frac{1}{n}\\ \\sum_{ }^{ }\\frac{|\\left(Y_t-\\hat{Y}_t\\right)|}{|Y_t|} \\tag{12.17} \\end{equation}\\] Mean-Squared Error (MSE): \\[\\begin{equation} MSE=\\frac{1}{n}\\sum_{ }^{ }(Y_t-\\hat{Y}_t)^2 \\tag{12.18} \\end{equation}\\] Root Mean-Squared Error: \\[\\begin{equation} RMSE=\\sqrt{MSE} \\tag{12.18} \\end{equation}\\] 12.2.2 Multicollinearity VIF \\[\\begin{equation} VIF_j=\\frac{1}{1-R_j^2} \\tag{5.7} \\end{equation}\\] Where \\(j = 1,...,k\\) Thus, we see that Rsquare is obtained from regression each IDV against the remaining variables. We can then have the following outputs: VIF = 1, no milticollinearity VIF &gt; 10, indicates multicollinearity If one gets an indication of multicollinearity, then one should drop one of the correlated variables. "],["formulas.html", "12.3 Formulas", " 12.3 Formulas auto.arima() from forecast package, automize the orders and drift. With possibility to select ic and selection method (stepwise vs. non stepwise) This may yield an ARIMA(0,0,0)(0,1,0)[12] saying that ARIMA is 0,0,0 order, but the seasonality is of differencing order 1. We see that the frequency is 12, meaning that each period is subtracted with the previous period (found the difference). tsdisplay() from forecast package, does acf, pacf and residuals, if the timeseries$residuals are made as input diff() does desired order of differencing Arima(), practically does the same. Although we are able to call another model, hence other coefficients, thus the coefficients will not be estimated again That is pretty useful!!! "],["loops.html", "12.4 Loops!!!!", " 12.4 Loops!!!! 12.4.1 HoltWinters Smoothing - finding optimal frequency This is develop by anton # y &lt;- read_excel(&quot;prob8p92HW.xlsx&quot;) # #&#39; *set up list of different possible seasonality components* # m &lt;- list() # m[[1]] &lt;- 2 # m[[2]] &lt;- 4 # #&#39; *run loop calculating RMSE for winters exponential smoothing with different frequencies* # RMSE &lt;- matrix(0,length(m),1) # for (i in seq(length(m))) # { # yt &lt;- ts(y, frequency = m[[i]]) # fit &lt;- HoltWinters(yt # ,alpha=0.6 #setting alpha # ,beta=TRUE #trend # ,gamma=TRUE) #seasonality # for.fit &lt;- forecast(fit) # acc.fit &lt;- accuracy(for.fit) # RMSE[i,1] &lt;- acc.fit [1,2] #putting RMSE for different frequencies in matrix # } # #&#39; *finding frequency with smallest RMSE* # best.freq &lt;- which.min(RMSE) # best.freq &lt;- m[best.freq] # best.freq 12.4.2 ARIMA - it is not really that useful, does not have all combinations Testing different orders # #Insert data as timeseries # y &lt;- read_excel(&quot;Data/Week47/IBMstock.xls&quot;) %&gt;% ts(frequency = 52 # #,start = # ) # #Import matrix of differencing combinations # OrderMatrix &lt;- read_excel(&quot;Data/Week47/OrdersMatrix.xlsx&quot;) # # { # RMSE &lt;- as.matrix(0) # boxtest &lt;- as.matrix(0) # # for (i in seq(from = 1,to = nrow(OrderMatrix),by = 1)) { # # print(i) # p &lt;- as.numeric(OrderMatrix[i,1]) #AR order # d &lt;- as.numeric(OrderMatrix[i,2]) #Differencing order # q &lt;- as.numeric(OrderMatrix[i,3]) #MA order # order &lt;- c(p,d,q) # # ARIMAmod &lt;- arima(x = y #The time-series # ,order = order # ) # # #Assessing in-samp accuracy RMSE # RMSE[i] &lt;- accuracy(object = fitted(ARIMAmod),x = y)[2] #2 for RMSE # # #Storing hypothesis test of independence # boxtest[i] &lt;- Box.test(ARIMAmod$residuals # ,fitdf = p+q)$p.value #Because it is applied to and ARIMA model # } # # #The optimal combination based on the highest Box-Pierce test (similar to Ljung-Box) # OrderMatrix[which.max(boxtest),] # } "]]

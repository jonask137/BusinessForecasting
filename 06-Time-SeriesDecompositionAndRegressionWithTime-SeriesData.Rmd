

```{r librariesTS,include=FALSE}
library(forecast)
library(readxl)
```


# Time-Series Decomposition and Regression with Time-Series Data

*This chapter identifies what elements a time series can be broken into, hence it elaborates on the trend, seasonality, cycles and irregular movements*


**Literature**

+ HW: Time Series and Their Components
+ HW: Regression with Time Series Data



## Time Series and Their Components (HW) {#TSComponents}

Basically a Time Series is variables that are collected over time. The variables are highly likely to have autocorrelation 

*Autocorellation: Variables are automatically dependant on each other over time, and the mere aspect of these synergies (patterns) where one will often be able to prove correlation between the variables*

One approach to assessing time series is by decomposing the patterns by finding the components hereof, these are:

1. Trend(T): if it is linear, then it can be explained by $\hat{T}_t=\beta_0+\beta_1t$, hence we apply the linear function, hence what in statistics is $\hat{y}=\hat{T}$ in time series

2. Cyclical(T or C)*Note, often included in practice as the trend, as it can be difficult to extinguish*

3. Seasonal(S)

4. Irregular(random)(I) - ***Notice! We dont want to use this for assembling the model, as it is random***

*These are also called deterministic variables*


**The purpose** of decomposing the time series data, can be either for exploration or prediction. Hence you can estimate the coefficients of the components by breaking down the data.

Although the typical purpose of time series is exploration of the data and assess if there are seasons, trends etc. and perhaps to pinpoint whether you are above or below the season/trend/cycle. 

If the Y observations is the sum of the components, then we have **additive model**, if they are the product of the components, then it is called **multiplicative model**

Time series is typically an additive model, if the variance is more or less the same, it is a multiplicative model if the variance increases with time

*Note, one can transform a multiplicative model to an additive model by taking the logarithm*


### Additional on trend

**Quadratic trend**

*e.g. where we have curvature*

$$\hat{T}_t=\beta_0+\beta_1t+\beta_1t^2$$


**Exponential trend**

*e.g. exponentially growing population*

$$\hat{T}_t=\beta_0*\beta_2^t$$

*NOTE, one may transform this into a logistic trend instead, as continuous exponential trend is not typical*

\

### Additional on seasonal pattern

+ One can manually rule out seasonality by adding seasonal index (that is hard coding the expected index in the respective periods)
  + ***One must rule out other factors before doing this!***


**Seasonally adjusted data**

For additive

$$Y_t - S_t = T_t + I_t$$

For multiplicative

$$\frac{Y_t}{S_t} = T_t * I_t$$

One does often take out seasonality to better compare data and also create short term forecasts.


### Cyclical and Irregular Variations

One can often rule out (or at least smooth out) irregularities by taking the moving average

\

## Methods of decomposing

There are three approaches to decomposing (probably more).

1. Decomposing, with the following approach:
    a. Deseasonalizing using seasonal dummies
    b. Detrending using a trend variable

2. Decomposing with the following:
    a. Deseasonalizing using MA (moving averages not ARMA). Where you apply the season, e.g., quarterly, then 4 periods
    b.Detrending using a trend variable
    
3. Using `decompose()`
    *a. Note: this use moving averages, see the documentation*



## Regression with time series data


One of the assumptions for regression models, is that the errors are independent (uncorrelated), THAT IS RARELY THE CASE WITH TIME SERIES. Hence one must be very precautions.

## The success criteria and process


*The following elaborates on success criteria and the process*

### Success Criteria

*Ultimately we want to be able to answer the following:*

1. Do we have trend?
2. Do we have cyclical movements?
3. Do we have seasons?
4. Do we have autocorellation (elaborated in section \@ref(Autocorrelation))? If yes:
    a. If RHO = 1, then we can take first differences
    b. If RHO <> 1, then we can do the generalized differences., thus implies the following:
        1. Do an OLS and get the residuals
        2. Use the residuals in the following equation $e_t=\rho e_{t-1}+\sigma_t$, using OLS as estimated rho ($\hat{\rho}$)
            a. If rho = 0, then 0 autocorrelation


### The Process {#DecomposingWithMA}

**Deseasonalizing and detrending based on moving averages and accounting for cyclical moves**

*Note, if you do not have seasonality, then jump to section 2. trend etc.*

#### Desaesonalizing

a. Remove the short-term fluctuations
b. If we have even number of periods, one must center the data. Whith odd period numbers, you can merely center with the period in the middle. The procedure with even number of periods is the following:

**1. Find the MA_t with equation \@ref(eq:MAt) and \@ref(eq:MAt2)**

\begin{equation}
MA_t = \frac{(Y_{t-2}+Y_{t-1}+Y_{t}+Y_{t+1}+)}{4}
(\#eq:MAt)
\end{equation} and 

\begin{equation}
MA_{t+1} = \frac{(Y_{t-1}+Y_{t}+Y_{t+1}+Y_{t+2}+)}{4}
(\#eq:MAt2)
\end{equation}


*Note, that the MA for each is centered in the center and rounded up to the coming period*

**2. Then find the centered MA**

Then do the average of the two periods, which will find the actual center: 

\begin{equation}
CMA_t=\frac{(MA_t+MA_{t+1})}{2}
(\#eq:CMAt)
\end{equation}

Notice, that the example is with quarterly MA, hence the MAs are divided with 4, this could have been monthly and then onw would divide by 12.

c. $CMA_t$ is representing the depersonalized data.
d. One can find the seasonal factor by saying 

\begin{equation}
SF_t=\frac{Y_t}{CMA_t}
(\#eq:SFt)
\end{equation}

*This explains whether one as above or below the expect season level.*

e. CONCLUSION: IF $SF_t > 1$, then Y is greater than the quarterly (or what other period is used) average or, $SF_t < 1$, then the Y is less than the quarterly average

\
    
> Alternative, deseasonalizing data can be done by dividing the raw data with some seasonal index, that is adding dummy variables for the periods. Although by using the index, one assume, that the same seasonality is the same as preivous periods.

\

#### Long-term trend

Long-term trend, this is estimated from the deseasonalized data. This is estimated using simple linear regression. Basically the detrended data consists of the residuals between the actual data and the estimated data by using the trend variable (the counter 1 to n).

*Task 1*

We must find out if the trend is linear or quadratic.
  
Linear: $C\hat{M}A_t=f(t)=\beta_0 + \beta_1t$

Quadratic: $C\hat{M}A_t=f(t)=\beta_0 + \beta_1t+\beta_2t^2$

Where $t$ is the time indicator and 1 = the first observation and increases by 1 thereafter.

Now we have obtained the centered moving-average trend

\begin{equation}
CMAT = C\hat{M}A
(\#eq:CMAT)
\end{equation}
    
 
#### Cyclical Component
   
Cyclical component, one can compare the CMA with the CMAT to find the cyclical factor. Thus, the cyclical factor is: 

\begin{equation}
CF = \frac{CMA}{CMAT}
(\#CF)
\end{equation}

If CF > 1, the deseasonalised value is above the long-term trend of the data. If the opposite, then below.
    

#### Time-Series decomposition forecast

Now we can do the reverse procedure, using the factors, that we have just found.

The reverse procedure is assembling the predicted Y based on the factors that have just been found.

\begin{equation}
\hat{Y}=CMAT*SI*CF*I
(\#eq:Forecast)
\end{equation}

Where, 

+ CMAT = T, 
+ S is the SF, 
+ CF is the CF and 
+ I is the irregular component (this is assumed to be 1 given its random nature, if one expects a boom or shock, this can be modeled with)


### Autocorellation {#Autocorellation}

ALWAYS ALWAYS CHECK FOR AUTOCORRELATION.

What to do?

+ You are missing some variable, find the missing variable(s). In practice this can be very difficult.
+ Do differencing
+ Use autoregressive model approach, where you are using lagged variables as variables to predict the coming period
  + We are going to talk about ARIMA (The box and jenkins methodology, more about this in section \@ref(ARIMA))


If our residuals have autocorelation, it means that there is some relationship in the model, that our model does not account four. In worst case, we can end up 'proving' some relationships between variables, that are in fact not true, but it is rather autocorellation that is proving the model, and the not the relationship between the variables and the dependent variables.



## Exercises

The section contain several exercises covering:

+ Decomposing and forecasting without cycles
+ Decomposing and forecasting with cycles
+ Time Series regression with other variables
+ Fitting trend variable
+ Decomposing using OLS, decompose and CMA



### Decomposition + Forecast Exercise

We use the Alomega Foodstore sales data to show the following:

a. How to make a decomposition of a time series, using `decompose()`
b. How to use `decompose()` to retrieve the different components and store these
c. How to inteprete the components
d. How to forecast using the components

```{r}
y <- read_excel("Data/Week47/Alomegafoodstores.xlsx")
y <- y$Sales
y <- ts(data = y #The data
        ,frequency = 12 #Data is monthly
        ,end = c(2006,12)
        )
options(scipen = 999)
tsdisplay(y)
```

It appears as if we have seasonality, but a trend is dificult to say.

It appears as if the data is additive, as the variance appear to be more or less the same over time.

#### a. Decomposing the time series

Now we can decompose the elements using `decompose()`, where moving averages are applied to deseasonalize the data.

But just for the sake of it, the multuplicative decomposition is made as well.

```{r}
y.decom.add <- decompose(x = y #The ts
                         ,type = "additive" #The composition
                         )
y.decom.mult <- decompose(x = y #The ts
                         ,type = "multiplicative" #The composition
                         )
plot(y.decom.add)
```



#### b. Retrieving components

Recall that we have the following components:

1. Trend (T), Tr in this example, as t is the transpose function
2. Seasons(S)
3. Irregular(I) - **NOTICE: We don't want to use this for forecasting, as it is random, hence it can be left out from the assembly yhat**

We see no cycles, and assume that they are not there.

```{r}
Tr <- y.decom.add$trend
S <- y.decom.add$seasonal
I <- y.decom.add$random
```

#### c. Interpreting the resutls

For the sake of it, the additive and multiplicative decompositions are plotted.

```{r}
plot(y.decom.add)
plot(y.decom.mult)
```

We see that the decomposition finds a trend and also it appears as there are some reoccurring patterns in the seasons.

But notice that the seasonal component is different, now it is a factor, which is relative to the trend.

#### d. Forecasting using the components

##### Fitting the model

We see that we are able to construct the in sample data 1:1 if the composition is created with all three components.

Let us forecast only using the trend and season component.

We see that we area able to plot the following:

```{r}
yhat<-Tr+S #Notice, that I is left out, as it is a random component
plot(y, type="l")
lines(Tr,col="green")
lines(yhat,col="red")
```

We can assess the accuracy of the composition of the trend and seasons.

```{r}
accuracy(yhat,y)
```

We see that the mean percentage error is -2,78 percent, hence it appears as there is a tendency for the model to be underestimating y.

##### Forecasting using the composition

Now we can apply the model in a forecasting setting to forecast h horizons.

```{r}
forecast.decom.add <- forecast(object = yhat,h = 18)
t(t(forecast.decom.add$mean))
```

18 months are chosen, as the function forecast from the middle of 2006, where it is was not able to fit the components, as it applies CMA on the months.

Using 18 months, we get forecasts for the last half a year and there after a whole year

We can also plot the forecast

```{r}
plot(forecast.decom.add)
```

We can assess the accuracy of the first 6 forecasted periods, as they are known

```{r}
y.test <- y[(length(y)-5):length(y)]
accuracy(object = forecast.decom.add$mean[1:6],x = y.test)
```

We see that the accuracy of these periods are 3.56% (MPE).



```{r}
rm(list = ls())
```



### Decomposition with cycles + Forecasting

Process:

1. 
2. 
3. 

```{r}
y <- read_excel("Data/Week47/Alomegafoodstores.xlsx")
y <- y$Sales
y <- ts(data = y #The data
        ,frequency = 12 #Data is monthly
        ,end = c(2006,12)
        )
```


#### 1. Finding CMA (deseasonalizing)

```{r}
CMA <- ma(x = y #The data
          ,order = 12 #When the same period occurs
          ,centre = TRUE) #As we have an even period
```

We have now deseasonalized the data, as the moving average has removed the effect of the seasons.

#### 2. Finding SF

That is, identifying whether we are above or below the deseasonalized data.

```{r}
SF<-y/CMA
plot(SF, type="l")
abline(h = 1 #Plotted, as when one is on 1, then you are neither above or below
       ,col = "red")
```

#### 3. Identifying Trend

We introduce a trend variable to take out the effect of the trend

```{r}
trend <- seq(1:length(y)) #Making counter
linmod <- lm(CMA ~ trend
             ,na.action = "na.exclude")
```

Notice that we don't care about the model, we just need the fitted values for identifying the cycle factor.

#### 4. Finding detrended and deseasonalized data

In step 1 we deseasonalized the data, based on the fitted model, we have detrended the 

```{r}
CMAT <- linmod$fitted.values
```

#### 5. Cycle identifying factor

We the deseasonalized and detrended data, we are able to idenitify if the deseasonalized data is above or below the seasons.

```{r,fig.cap="Cycle Factor"}
CF <- na.exclude(CMA) / CMAT
ts.plot(CF)
abline(h = 1,col = "red")
```

We want to look for wavelike patterns. We don't see any, hence it is fair to assume, that we don't have any cycles.

If we had, what to do?

- When multiplicative: One can apply the cycle factor in the regression
- When additive: One can apply the difference (between CMA and CMAT) in the regression.


End of exercise:

```{r}
rm(list = ls())
```



### Time series regression {#TimeSeriesRegression}

This example show how one can apply other variables in a regression

```{r}
Alomegasales <- read_excel("Data/Week47/Alomegafoodstores.xlsx")
y <- read_excel("Data/Week47/Alomegafoodstores.xlsx")
y <- y$Sales
y <- ts(data = y #The data
        ,frequency = 12 #Data is monthly
        ,end = c(2006,12)
        )

#fetch explanatory variables
paper<-ts(Alomegasales$Paper, frequency=12)
tv<-ts(Alomegasales$TV, frequency=12)
month <- seasonaldummy(y) #creates seasonal dummies, if such not available in the data
#Regression model for sales
reg <-lm(y~paper+tv+month)
summary(reg)
```

We see the model summary above. Now we can make the regression

```{r}
#obtain residuals from the model to perform model quality checks
res<-reg$residuals
plot(res)
```

We see that the residuals appear homoskedastic.

We can check them for normality

```{r}
#check for normality
hist(res)
```

By visual inspection, they appear to be normally distributed.

```{r}
library(tseries)
jarque.bera.test(res) #Jarque-Bera test the null of normality
```

H0: Normality

Cannot reject.

Now we can check for multicollinearity

```{r}
#testing for multicollinearity
library(car)
vif(reg)
```

We see that they are all ≈ 2 or less, hence no problem

We can check the residuals for autocorrelation (serial correlation). We dont want this

```{r}
#serial correlation
#acf(res)
plot(acf(res,plot=F)[1:20])#alternative command if you want to skip the 0 lag
```

Despite a few significant spikes, it appears as if it is not autocorrelated. This can be tested with the Durbin Watson test.

```{r}
#Durbin-Watson serial correlation test
library(lmtest)
dwtest(reg)
```

H0: No autocorrelation

We cannot reject on the five percent level





\

\

### Alomega Food Stores, case 6 p. 166 + case 7 p. 348 (Another example)

```{r}
y <- read_excel("Data/Week47/Alomegafoodstores.xlsx")
str(y)
```

We see that there are 21 variables, where sales is the DV and all others are IDVs, consisting of continous and factors.

Now we are interested in constructing a time series, which consist of the dependent variable. That is done in the following:

```{r,fig.cap="Time series Alomega Food"}
y <- ts(data = y$Sales #The dependent variable
        ,end = c(2006,12) #The end date
        ,frequency = 12 #The frequency, 12 as we are working with months
        )
options(scipen = 999)
plot(y)
```

Now we want to address if there is:

+ Trend: visually it does not look like. But it will be tested by testing the time series against a trend variable
+ Seasons:  
+ Cycles: THis is difficult to say, also as we only have data for four years

**Trend**

```{r}
#Creating trend variable
trend <- seq(from = 1,to = length(y),by = 1)

#Creating a linear model with the trend variable
lm.fit <- lm(y ~ trend)
summary(lm.fit)
```

We see that the trend variable appear to be non significant, hence there is not enough evidence to reject the null hypothesis, being that there is no relationship between the dependent- and independent variable.

```{r}
accuracy(object = lm.fit #The fitted values from the linear model
         ,y = y #The actual value
         )
```

We see the RMSE of 125.940, this can also be plotted to see the fitted values against the residuals, where the mean absolute error is just below 100.000 units. Notice, that this test in done in sample.

```{r,fig.cap="Residuals plot"}
plot(x = lm.fit$fitted.values,y = lm.fit$residuals,main = "Resduals plot") + 
  grid(col = "lightgrey") + 
  abline(h = 0,lty = 3,col = "blue")
```

#### What might Jackson Tilson say about the forecasts?

With mean average percentage error of 28%, the forecast may not be super good. Although it was shown, that there is statistical evidence for the model being significant. Hence, we must try to convince him, that it does in fact contribute with some knowledge.

__What other forecast methods could be used?__

E.g., smoothing or moving averages.


#### P.348 Case 7 (Alomega Food Stores)

This is an extension of the case above. It has questions regarding what other things she might do.

__How is Julie sure that it is the right predictors__

We see on page 348, that some of the variables has a p-vale that is above the 5 percent level, hence there is a greater risk, that there is no statistical evidence for a relationship between the individual variables and the dependent variable.

Also changes in VIF for the variables, although none are close to 10 and according to the rule of thumb, there is no indication of multicollinearity

__How would you sell the model__

Focus on how it prepares them for future changes, planning staffing, procurement, etc.

__How may the model indicate future advertizing?__

We see that it makes sense to spend money on all three types of advertisements.

__What conditions to be aware of?__

- Autocorrelation in the error terms?
- Multicollinearity

Other models that could be used?

- ARIMA
- Smoothing


```{r}
rm(list = ls())
```


### Monthly Sales Data {#ex:MonthlySalesData}

Loading the data and storing only the y variable as a timeseries

```{r,fig.cap="Visual interpretation"}
df <- read_excel("Data/Week47/Salesdataseasonal.xlsx")
y <- ts(df$Sales
        ,frequency = 12 #Monthly data
        ,start = c(1979,1) #start year 1971 month 1
        )
#Plotting for visual interpretation
plot(y)
```


Goal:

a. Seasonal dummies
b. Trend variable

Hereafter different approaches to decomposing data is presented

1. OLS using seasonal dummies and trend variable
2. Decomposition using `decompose()`
3. Using CMA (centered moving averages)


___a. finding seasonal dummies___

Finding seasonal dummies using a trend

```{r}
#Creting matrix of dummies for each month
month <- seasonaldummy(x = y)
```

This can be shown with the following table

```{r,echo=F,fig.cap="Seasonal dummy matrix inspection"}
knitr::kable(head(month))
```

We see that a matrix has been created identifying what month each period belongs to.

___b. finding the trend variable___

In addition hereof, we must create a trend variable to account for the trend:

```{r}
trend <- seq(1,to = length(y),by = 1)
```


___1. OLS using seasonal dummies and trend variable___

```{r}
#Constructing linear model
lmmod <- lm(y ~ month + trend)
summary(lmmod)
```

What can we deduct from this?

- January, June, September and November are not significant on the 5% level in the full model. Hence the dummies may not explain the sales for these months
- There is overwhelming evidence for trend being present in the data

Lets now look into the fitted values and plot


```{r}
{#Plotting fitted values
plot(lmmod$fitted.values,main = 'Fitted values',ylab = 'Sales') + 
  lines(lmmod$fitted.values,col = 'red') +
  grid(col = 'lightgrey')

#Plotting residuals
plot(lmmod$residuals,main = 'Residuals',ylab = 'Residuals') + 
  lines(lmmod$residuals,col = 'green') +
  grid(col = 'lightgrey')}
```


__2. Decomposition using `decompose()`__

The following is able to decompose the series based on assuming additivity and multiplicative relationship.

Although as we see that the variance is increasing over time, then one may expect that the multiplicative approach is more appropriate.

```{r,fig.cap="Additive decomposition"}
#Additive
decompSalesAdd <- decompose(x = y #The timeseries
                            ,type = "additive") #for additive decomposition
#Plotting
plot(decompSalesAdd)
```


```{rfig.cap="Multiplicative decomposition"}
#Multiplicative
decompSalesMult <- decompose(x = y #The timeseries
                             ,type = "multiplicative") #for multiplicative decomposition
#Plotting
plot(decompSalesMult)
```


__3. CMA__

see process in \@ref(DecomposingWithMA)

1. Find the MA_t with equation (6.1) and (6.2) = __Deseasonalizing data__

```{r,fig.cap="CMA"}
#Finding centered MA
CMA <- ma(x = y #The time series
          ,order = 12 #We want the average of 12 periods
          ,centre = TRUE) #As order is equal, then we want to make a centered MA

plot(CMA)
```

We see that the centered moving averages have removed the seasons from the data. But there is clearly still a trend.

We are now able to __estimate the trend__ using the deseasonalized data with with trend variable.

_Just as we would deseasonalize any other data!!_

2. Then find the centered MA (CMAT) = __detrending data__

```{r,fig.cap="Cycle"}
#Detrending data 
linmod <- lm(CMA ~ trend
             ,na.action = "na.exclude") #We want to remove rows with NAs

  # NOTE: we dont really care about the model, we only want the fitted values
  # as they represent the estimated linear trend

#Estimated trend
CMAT <- linmod$fitted.values #Extracting trend estimates and saving in an object
plot(CMAT)
```

With the deseasonalized data and the trend estimates, we are able to assess whether we are above or below the trend in all of deseasonalized data.

__The cycle__

```{r,fig.cap="Cycle"}
#Identifying the cycle
Cycle <- na.exclude(CMA) / CMAT
ts.plot(Cycle)
abline(h=1, col = "red")
```

Now we see the whether we are above or below the trend in the respective periods, e.g., period 60 appear to be far below the trend level indicating it is a the lowest of the seasons.













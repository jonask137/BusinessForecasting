

```{r librariesTS,include=FALSE}
library(forecast)
library(readxl)
```


# Time-Series Decomposition and Regression with Time-Series Data

*This chapter identifies what elements a time series can be broken into, hence it elaborates on the trend, seasonality, cycles and irregular movements*


**Literature**

+ HW: Time Series and Their Components
+ HW: Regression with Time Series Data



## Time Series and Their Components (HW) {#TSComponents}

Basically a Time Series is variables that are collected over time. The variables are highly likely to have autocorrelation 

*Autocorellation: Variables are automatically dependant on each other over time, and the mere aspect of these synergies (patterns) where one will often be able to prove correlation between the variables*

One approach to assessing time series is by decomposing the patterns by finding the components hereof, these are:

1. Trend(T): if it is linear, then it can be explained by $\hat{T}_t=\beta_0+\beta_1t$, hence we apply the linear function, hence what in statistics is $\hat{y}=\hat{T}$ in time series

2. Cyclical(T or C)*Note, often included in practice as the trend, as it can be difficult to extinguish*

3. Seasonal(S)

4. Irregular(random)(I)

*These are also called deterministic variables*


**The purpose** of decomposing the time series data, can be either for exploration or prediction. Hence you can estimate the coefficients of the components by breaking down the data.

Although the typical purpose of time series is exploration of the data and assess if there are seasons, trends etc. and perhaps to pinpoint whether you are above or below the season/trend/cycle. 

If the Y observations is the sum of the components, then we have **additive model**, if they are the product of the components, then it is called **multiplicative model**

Time series is typically an additive model, if the variance is more or less the same, it is a multiplicative model if the variance increases with time

*Note, one can transform a multiplicative model to an additive model by taking the logarithm*


### Additional on trend

**Quadratic trend**

*e.g. where we have curvature*

$$\hat{T}_t=\beta_0+\beta_1t+\beta_1t^2$$


**Exponential trend**

*e.g. exponentially growing population*

$$\hat{T}_t=\beta_0*\beta_2^t$$

*NOTE, one may transform this into a logistic trend instead, as continuous exponential trend is not typical*

\

### Additional on seasonal pattern

+ One can manually rule out seasonality by adding seasonal index (that is hard coding the expected index in the respective periods)
  + ***One must rule out other factors before doing this!***


**Seasonally adjusted data**

For additive

$$Y_t - S_t = T_t + I_t$$

For multiplicative

$$\frac{Y_t}{S_t} = T_t * I_t$$

One does often take out seasonality to better compare data and also create short term forecasts.


### Cyclical and Irregular Variations

One can often rule out (or at least smooth out) irregularities by taking the moving average

\

## Methods of decomposing

There are three approaches to decomposing (probably more).

1. Decomposing, with the following approach:
    a. Deseasonalizing using seasonal dummies
    b. Detrending using a trend variable

2. Decomposing with the following:
    a. Deseasonalizing using MA (moving averages not ARMA). Where you apply the season, e.g., quarterly, then 4 periods
    b.Detrending using a trend variable
    
3. Using `decompose()`
    *a. Note: this use moverages, see the documentation*



## Regression with time series data


One of the assumptions for regression models, is that the errors are independent (uncorrelated), THAT IS RARELY THE CASE WITH TIME SERIES. Hence one must be very precautions.

## The success criteria and process


*The following elaborates on success criteria and the process*

### Success Criteria

*Ultimately we want to be able to answer the following:*

1. Do we have trend?
2. Do we have cyclical movements?
3. Do we have seasons?
4. Do we have autocorellation (elaborated in section \@ref(Autocorrelation))? If yes:
    a. If RHO = 1, then we can take first differences
    b. If RHO <> 1, then we can do the generalized differences., thus implies the following:
        1. Do an OLS and get the residuals
        2. Use the residuals in the following equation $e_t=\rho e_{t-1}+\sigma_t$, using OLS as estimated rho ($\hat{\rho}$)
            a. If rho = 0, then 0 autocorrelation


### The Process {#DecomposingWithMA}

**Deseasonalizing and detrending based on moving averages and accounting for cyclical moves**

*Note, if you do not have seasonality, then jump to section 2. trend etc.*

#### Desaesonalizing

a. Remove the short-term fluctuations
b. If we have even number of periods, one must center the data. Whith odd period numbers, you can merely center with the period in the middle. The procedure with even number of periods is the following:

**1. Find the MA_t with equation \@ref(eq:MAt) and \@ref(eq:MAt2)**

\begin{equation}
MA_t = \frac{(Y_{t-2}+Y_{t-1}+Y_{t}+Y_{t+1}+)}{4}
(\#eq:MAt)
\end{equation} and 

\begin{equation}
MA_{t+1} = \frac{(Y_{t-1}+Y_{t}+Y_{t+1}+Y_{t+2}+)}{4}
(\#eq:MAt2)
\end{equation}


*Note, that the MA for each is centered in the center and rounded up to the coming period*

**2. Then find the centered MA**

Then do the average of the two periods, which will find the actual center: 

\begin{equation}
CMA_t=\frac{(MA_t+MA_{t+1})}{2}
(\#eq:CMAt)
\end{equation}

Notice, that the example is with quarterly MA, hence the MAs are divided with 4, this could have been monthly and then onw would divide by 12.

c. $CMA_t$ is representing the depersonalized data.
d. One can find the seasonal factor by saying 

\begin{equation}
SF_t=\frac{Y_t}{CMA_t}
(\#eq:SFt)
\end{equation}

*This explains whether one as above or below the expect season level.*

e. CONCLUSION: IF $SF_t > 1$, then Y is greater than the quarterly (or what other period is used) average or, $SF_t < 1$, then the Y is less than the quarterly average

\
    
> Alternative, deseasonalizing data can be done by dividing the raw data with some seasonal index, that is adding dummy variables for the periods. Although by using the index, one assume, that the same seasonality is the same as preivous periods.

\

#### Long-term trend

Long-term trend, this is estimated from the deseasonalized data. This is estimated using simple linear regression. Basically the detrended data consists of the residuals between the actual data and the estimated data by using the trend variable (the counter 1 to n).

*Task 1*

We must find out if the trend is linear or quadratic.
  
Linear: $C\hat{M}A_t=f(t)=\beta_0 + \beta_1t$

Quadratic: $C\hat{M}A_t=f(t)=\beta_0 + \beta_1t+\beta_2t^2$

Where $t$ is the time indicator and 1 = the first observation and increases by 1 thereafter.

Now we have obtained the centered moving-average trend

\begin{equation}
CMAT = C\hat{M}A
(\#eq:CMAT)
\end{equation}
    
 
#### Cyclical Component
   
Cyclical component, one can compare the CMA with the CMAT to find the cyclical factor. Thus, the cyclical factor is: 

\begin{equation}
CF = \frac{CMA}{CMAT}
(\#CF)
\end{equation}

If CF > 1, the deseasonalised value is above the long-term trend of the data. If the opposite, then below.
    

#### Time-Series decomposition forecast

Now we can do the reverse procedure, using the factors, that we have just found.

The reverse procedure is assembling the predicted Y based on the factors that have just been found.

\begin{equation}
\hat{Y}=CMAT*SI*CF*I
(\#eq:Forecast)
\end{equation}

Where, 

+ CMAT = T, 
+ S is the SF, 
+ CF is the CF and 
+ I is the irregular component (this is assumed to be 1 given its random nature, if one expects a boom or shock, this can be modeled with)


### Autocorellation {#Autocorellation}

ALWAYS ALWAYS CHECK FOR AUTOCORRELATION.

What to do?

+ You are missing some variable, find the missing variable(s). In practice this can be very difficult.
+ Do differencing
+ Use autoregressive model approach, where you are using lagged variables as variables to predict the coming period
  + We are going to talk about ARIMA (The box and jenkins methodology, more about this in section \@ref(ARIMA))


If our residuals have autocorelation, it means that there is some relationship in the model, that our model does not account four. In worst case, we can end up 'proving' some relationships between variables, that are in fact not true, but it is rather autocorellation that is proving the model, and the not the relationship between the variables and the dependent variables.



## Exercises

### Alomega Food Stores, case 6 p. 166 + case 7 p. 348

```{r}
y <- read_excel("Data/Week47/Alomegafoodstores.xlsx")
str(y)
```

We see that there are 21 variables, where sales is the DV and all others are IDVs, consisting of continous and factors.

Now we are interested in constructing a time series, which consist of the dependent variable. That is done in the following:

```{r,fig.cap="Time series Alomega Food"}
y <- ts(data = y$Sales #The dependent variable
        ,end = c(2006,12) #The end date
        ,frequency = 12 #The frequency, 12 as we are working with months
        )
options(scipen = 999)
plot(y)
```

Now we want to address if there is:

+ Trend: visually it does not look like. But it will be tested by testing the time series against a trend variable
+ Seasons:  
+ Cycles: THis is difficult to say, also as we only have data for four years

**Trend**

```{r}
#Creating trend variable
trend <- seq(from = 1,to = length(y),by = 1)

#Creating a linear model with the trend variable
lm.fit <- lm(y ~ trend)
summary(lm.fit)
```

We see that the trend variable appear to be non significant, hence there is not enough evidence to reject the null hypothesis, being that there is no relationship between the dependent- and independent variable.

```{r}
accuracy(object = lm.fit #The fitted values from the linear model
         ,y = y #The actual value
         )
```

We see the RMSE of 125.940, this can also be plotted to see the fitted values against the residuals, where the mean absolute error is just below 100.000 units. Notice, that this test in done in sample.

```{r,fig.cap="Residuals plot"}
plot(x = lm.fit$fitted.values,y = lm.fit$residuals,main = "Resduals plot") + 
  grid(col = "lightgrey") + 
  abline(h = 0,lty = 3,col = "blue")
```

#### What might Jackson Tilson say about the forecasts?

With mean average percentage error of 28%, the forecast may not be super good. Although it was shown, that there is statistical evidence for the model being significant. Hence, we must try to convince him, that it does in fact contribute with some knowledge.

__What other forecast methods could be used?__

E.g., smoothing or moving averages.


#### P.348 Case 7 (Alomega Food Stores)

This is an extension of the case above. It has questions regarding what other things she might do.

__How is Julie sure that it is the right predictors__

We see on page 348, that some of the variables has a p-vale that is above the 5 percent level, hence there is a greater risk, that there is no statistical evidence for a relationship between the individual variables and the dependent variable.

Also changes in VIF for the variables, although none are close to 10 and according to the rule of thumb, there is no indication of multicollinearity

__How would you sell the model__

Focus on how it prepares them for future changes, planning staffing, procurement, etc.

__How may the model indicate future advertizing?__

We see that it makes sense to spend money on all three types of advertisements.

__What conditions to be aware of?__

- Autocorrelation in the error terms?
- Multicollinearity

Other models that could be used?

- ARIMA
- Smoothing


```{r}
rm(list = ls())
```


### Monthly Sales Data {#ex:MonthlySalesData}

Loading the data and storing only the y variable as a timeseries

```{r,fig.cap="Visual interpretation"}
df <- read_excel("Data/Week47/Salesdataseasonal.xlsx")
y <- ts(df$Sales
        ,frequency = 12 #Monthly data
        ,start = c(1979,1) #start year 1971 month 1
        )
#Plotting for visual interpretation
plot(y)
```


Goal:

a. Seasonal dummies
b. Trend variable

Hereafter different approaches to decomposing data is presented

1. OLS using seasonal dummies and trend variable
2. Decomposition using `decompose()`
3. Using CMA (centered moving averages)


___a. finding seasonal dummies___

Finding seasonal dummies using a trend

```{r}
#Creting matrix of dummies for each month
month <- seasonaldummy(x = y)
```

This can be shown with the following table

```{r,echo=F,fig.cap="Seasonal dummy matrix inspection"}
knitr::kable(head(month))
```

We see that a matrix has been created identifying what month each period belongs to.

___b. finding the trend variable___

In addition hereof, we must create a trend variable to account for the trend:

```{r}
trend <- seq(1,to = length(y),by = 1)
```


___1. OLS using seasonal dummies and trend variable___

```{r}
#Constructing linear model
lmmod <- lm(y ~ month + trend)
summary(lmmod)
```

What can we deduct from this?

- January, June, September and November are not significant on the 5% level in the full model. Hence the dummies may not explain the sales for these months
- There is overwhelming evidence for trend being present in the data

Lets now look into the fitted values and plot


```{r}
{#Plotting fitted values
plot(lmmod$fitted.values,main = 'Fitted values',ylab = 'Sales') + 
  lines(lmmod$fitted.values,col = 'red') +
  grid(col = 'lightgrey')

#Plotting residuals
plot(lmmod$residuals,main = 'Residuals',ylab = 'Residuals') + 
  lines(lmmod$residuals,col = 'green') +
  grid(col = 'lightgrey')}
```


__2. Decomposition using `decompose()`__

The following is able to decompose the series based on assuming additivity and multiplicative relationship.

Although as we see that the variance is increasing over time, then one may expect that the multiplicative approach is more appropriate.

```{r,fig.cap="Additive decomposition"}
#Additive
decompSalesAdd <- decompose(x = y #The timeseries
                            ,type = "additive") #for additive decomposition
#Plotting
plot(decompSalesAdd)
```


```{rfig.cap="Multiplicative decomposition"}
#Multiplicative
decompSalesMult <- decompose(x = y #The timeseries
                             ,type = "multiplicative") #for multiplicative decomposition
#Plotting
plot(decompSalesMult)
```


__3. CMA__

see process in \@ref(DecomposingWithMA)

1. Find the MA_t with equation (6.1) and (6.2) = __Deseasonalizing data__

```{r,fig.cap="CMA"}
#Finding centered MA
CMA <- ma(x = y #The time series
          ,order = 12 #We want the average of 12 periods
          ,centre = TRUE) #As order is equal, then we want to make a centered MA

plot(CMA)
```

We see that the centered moving averages have removed the seasons from the data. But there is clearly still a trend.

We are now able to __estimate the trend__ using the deseasonalized data with with trend variable.

_Just as we would deseasonalize any other data!!_

2. Then find the centered MA (CMAT) = __detrending data__

```{r,fig.cap="Cycle"}
#Detrending data 
linmod <- lm(CMA ~ trend
             ,na.action = "na.exclude") #We want to remove rows with NAs

  # NOTE: we dont really care about the model, we only want the fitted values
  # as they represent the estimated linear trend

#Estimated trend
CMAT <- linmod$fitted.values #Extracting trend estimates and saving in an object
plot(CMAT)
```

With the deseasonalized data and the trend estimates, we are able to assess whether we are above or below the trend in all of deseasonalized data.

__The cycle__

```{r,fig.cap="Cycle"}
#Identifying the cycle
Cycle <- na.exclude(CMA) / CMAT
ts.plot(Cycle)
abline(h=1, col = "red")
```

Now we see the whether we are above or below the trend in the respective periods, e.g., period 60 appear to be far below the trend level indicating it is a the lowest of the seasons.















```{r libraries,include=FALSE}
library(forecast)
library(readxl)
```


# Time-Series Decomposition and Regression with Time-Series Data

*This chapter identifies what elements a time series can be broken into, hence it elaborates on the trend, seasonality, cycles and irregular movements*


**Literature**

+ HW: Time Series and Their Components
+ HW: Regression with Time Series Data



## Time Series and Their Components (HW)

Basically a Time Series is variables that are collected over time. The variables are highly likely to have autocorrelation 

*Autocorellation: Variables are automatically dependant on each other over time, and the mere aspect of these synergies (patterns) where one will often be able to prove correlation between the variables*

One approach to assessing time series is by decomposing the patterns by finding the components hereof, these are:

1. Trend(T): if it is linear, then it can be explained by $\hat{T}_t=\beta_0+\beta_1t$, hence we apply the linear function, hence what in statistics is $\hat{y}=\hat{T}$ in time series

2. Cyclical(T or C)*Note, often included in practice as the trend, as it can be difficult to extinguish*

3. Seasonal(S)

4. Irregular(random)(I)


**The purpose** of decomposing the time series data, can be either for exploration or prediction. Hence you can estimate the coefficients of the components by breaking down the data.

Although the typical purpose of time series is exploration of the data and assess if there are seasons, trends etc. and perhaps to pinpoint whether you are above or below the season/trend/cycle. 

If the Y observations is the sum of the components, then we have **additive model**, if they are the product of the components, then it is called **multiplicative model**

Time series is typically an additive model, if the variance is more or less the same, it is a multiplicative model if the variance increases with time

*Note, one can transform a multiplicative model to an additive model by taking the logarithm*


### Additional on trend

**Quadratic trend**

*e.g. where we have curvature*

$$\hat{T}_t=\beta_0+\beta_1t+\beta_1t^2$$


**Exponential trend**

*e.g. exponentially growing population*

$$\hat{T}_t=\beta_0*\beta_2^t$$

*NOTE, one may transform this into a logistic trend instead, as continuous exponential trend is not typical*

\

### Additional on seasonal pattern

+ One can manually rule out seasonality by adding seasonal index (that is hard coding the expected index in the respective periods)
  + ***One must rule out other factors before doing this!***


**Seasonally adjusted data**

For additive

$$Y_t - S_t = T_t + I_t$$

For multiplicative

$$\frac{Y_t}{S_t} = T_t * I_t$$

One does often take out seasonality to better compare data and also create short term forecasts.


### Cyclical and Irregular Variations

One can often rule out (or at least smooth out) irregularities by taking the moving average

\

## Regression with time series data


One of the assumptions for regression models, is that the errors are independent (uncorrelated), THAT IS RARELY THE CASE WITH TIME SERIES. Hence one must be very precautions.

## The success criteria and process


*The following elaborates on success criteria and the process*

### Success Criteria

*Ultimately we want to be able to answer the following:*

1. Do we have trend?
2. Do we have cyclical movements?
3. Do we have seasons?
4. Do we have autocorellation (elaborated in section \@ref(Autocorellation))? If yes:
    a. If RHO = 1, then we can take first differences
    b. If RHO <> 1, then we can do the generalized differences., thus implies the following:
        1. Do an OLS and get the residuals
        2. Use the residuals in the following equation $e_t=\rho e_{t-1}+ \sigma_t$, using OLS as estimated rho ($\hat{\rho}$)
            a. If rho = 0, then 0 autocorellation


### The Process

**Deseasonalizing and detrending based on moving averages and accounting for cyclical moves**

*Note, if you do not have seasonality, then jump to section 2. trend etc.*

#### Desaesonalizing

a. Remove the short-term fluctuations
b. If we have even number of periods, one must center the data. Whith odd period numbers, you can merely center with the period in the middle. The procedure with even number of periods is the following:

**1. Find the MA_t with equation \@ref(eq:MAt) and \@ref(eq:MAt2)**

\begin{equation}
MA_t = \frac{(Y_{t-2}+Y_{t-1}+Y_{t}+Y_{t+1}+)}{4}
(\#eq:MAt)
\end{equation} and 

\begin{equation}
MA_{t+1} = \frac{(Y_{t-1}+Y_{t}+Y_{t+1}+Y_{t+2}+)}{4}
(\#eq:MAt2)
\end{equation}


*Note, that the MA for each is centered in the center and rounded up to the coming period*

**2. Then find the centered MA**

Then do the average of the two periods, which will find the actual center: 

\begin{equation}
CMA_t=\frac{(MA_t+MA_{t+1})}{2}
(\#eq:CMAt)
\end{equation}

Notice, that the example is with quarterly MA, hence the MAs are divided with 4, this could have been monthly and then onw would divide by 12.

c. $CMA_t$ is representing the depersonalized data.
d. One can find the seasonal factor by saying 

\begin{equation}
SF_t=\frac{Y_t}{CMA_t}
(\#eq:SFt)
\end{equation}

*This explains whether one as above or below the expect season level.*

e. CONCLUSION: IF $SF_t > 1$, then Y is greater than the quarterly (or what other period is used) average or, $SF_t < 1$, then the Y is less than the quarterly average

\
    
> Alternative, deseasonalizing data can be done by dividing the raw data with some seasonal index, that is adding dummy variables for the periods. Although by using the index, one assume, that the same seasonality is the same as preivous periods.

\

#### Long-term trend

Long-term trend, this is estimated from the deseasonalized data. This is estimated using simple linear regression. Basically the detrended data consists of the residuals between the actual data and the estimated data by using the trend variable (the counter 1 to n).

*Task 1*

We must find out if the trend is linear or quadratic.
  
Linear: $C\hat{M}A_t=f(t)=\beta_0 + \beta_1t$

Quadratic: $C\hat{M}A_t=f(t)=\beta_0 + \beta_1t+\beta_2t^2$

Where $t$ is the time indicator and 1 = the first observation and increases by 1 thereafter.

Now we have obtained the centered moving-average trend

\begin{equation}
CMAT = C\hat{M}A
(\#eq:CMAT)
\end{equation}
    
 
#### Cyclical Component
   
Cyclical component, one can compare the CMA with the CMAT to find the cyclical factor. Thus, the cyclical factor is: 

\begin{equation}
CF = \frac{CMA}{CMAT}
(\#CF)
\end{equation}

If CF > 1, the deseasonalised value is above the long-term trend of the data. If the opposite, then below.
    

#### Time-Series decomposition forecast

Now we can do the reverse procedure, using the factors, that we have just found.

The reverse procedure is assembling the predicted Y based on the factors that have just been found.

\begin{equation}
\hat{Y}=CMAT*SI*CF*I
(\#eq:Forecast)
\end{equation}

Where, 

+ CMAT = T, 
+ S is the SF, 
+ CF is the CF and 
+ I is the irregular component (this is assumed to be 1 given its random nature, if one expects a boom or shock, this can be modeled with)


### Autocorellation {#Autocorellation}

ALWAYS ALWAYS CHECK FOR AUTOCORRELATION.

What to do?

+ You are missing some variable, find the missing variable(s). In practice this can be very difficult.
+ Do differencing
+ Use autoregressive model approach, where you are using lagged variables as variables to predict the coming period
  + We are going to talk about ARIMA (The box and jenkins methodology, more about this in section \@ref(ARIMA))


If our residuals have autocorelation, it means that there is some relationship in the model, that our model does not account four. In worst case, we can end up 'proving' some relationships between variables, that are in fact not true, but it is rather autocorellation that is proving the model, and the not the relationship between the variables and the dependent variables.



## Exercises

### Alomega Food Stores, case 6 p. 166 + case 7 p. 348

```{r}
y <- read_excel("Data/Week47/Alomegafoodstores.xlsx")
str(y)
```

We see that there are 21 variables, where sales is the DV and all others are IDVs, consisting of continous and factors.

Now we are interested in constructing a time series, which consist of the dependent variable. That is done in the following:

```{r,fig.cap="Time series Alomega Food"}
y <- ts(data = y$Sales #The dependent variable
        ,end = c(2006,12) #The end date
        ,frequency = 12 #The frequency, 12 as we are working with months
        )
options(scipen = 999)
plot(y)
```

Now we want to address if there is:

+ Trend: visually it does not look like. But it will be tested by testing the time series against a trend variable
+ Seasons:  
+ Cycles: THis is difficult to say, also as we only have data for four years

**Trend**

```{r}
#Creating trend variable
trend <- seq(from = 1,to = length(y),by = 1)

#Creating a linear model with the trend variable
lm.fit <- lm(y ~ trend)
summary(lm.fit)
```

We see that the trend variable appear to be non significant, hence there is not enough evidance to reject the null hypothesis, being that there is no relationship between the dependent- and independent variable.

```{r}
accuracy(object = lm.fit #The fitted values from the linear model
         ,y = y #The actual value
         )
```

We see the RMSE of 125.940, this can also be plotted to see the fitted values against the residuals, where the mean absolute error is just below 100.000 units. Notice, that this test in done in sample.

```{r,fig.cap="Residuals plot"}
plot(x = lm.fit$fitted.values,y = lm.fit$residuals,main = "Resduals plot") + 
  grid(col = "lightgrey") + 
  abline(h = 0,lty = 3,col = "blue")
```



### Monthly Sales Data







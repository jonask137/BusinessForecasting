---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r librariesARIMA,include=FALSE}
library(readxl)
library(ARDL)
library(vars)
library(dynlm)
```


# ARIMA Models + ADL and Box-Jenkins Methodology {#ARIMA}

*Considering a single time series $Y_t$, the interest is in determining the relationship between $Y_t$ and its past.*

**Literature**

+ HW: The Box-Jenkins (ARIMA) Methodology

+ ADL: [https://www.econometrics-with-r.org/14-5-apatadlm.html

+ Additional on ADL: http://faculty.washington.edu/ezivot/econ584/stck_watson_var.pdf
  + Note, just the intuitions behind are important, not the over-technical
details or derivations.


***go ARIMA or ADL or VAR??***

ARIMA does univariate assessment (only one variable) --> $ARIMA_{(p,d,q)}$

ADL does multivariate assessment while also doing AR on the dependent variable, hence we are able to assess effects from another lagged variables. *Note, this only takes on one other variable* --> $ADL_{(p)}$

So, what to do if we have more variables we want to include? Then we do VAR (Vector AuroRegressive mdoel) with 

With VAR, we are able to see relationship between several variables, and do forecast for each of the variables with p order lags, both from the other variables and also the variable itself --> $VAR_{(p)}$


## ARIMA {#ARIMAΗ2}

Basically it is a framework for adding AR and or MA into the regression model, where:

- AR: A time-series is predicted using its own history, can be explained by: $Y_t=β_0+β_1Y_{t-1}+ε_1$ 
- MA: Model predicts based on current and past shocks to the series. An MA(q) model: $Y_t=\mu+\epsilon_t-\omega_1\ \epsilon_{e-1}-...-\omega_q\ \epsilon_{e-q}$

**It is important to notice, that it is not by default an advantage of including both methods, but one should select the appropriate method depending on the looks correlogram**

ARIMA is specifically good for short term forecasts as it utilize previous observations to forecast future values.

It is able to represent both stationary and nonstationary data.

Often, one encounter difficulties making correct model specifications, meaning meeting the assumptions. This is crucial for predicting reliable forecasts

__Key concepts__

+ ARMA vs. ARIMA: ARMA is an approach to stationary data. If the data is nonstationary, one must make the data stationary by differencing. leading to ARIMA. Hence:
      1. ARMA: When we have stationarity
      2. ARIMA: When we have non-stationarity
    
+ Random walk: this is merely when you have an 0,1,0 ARIMA, hence only differencing. Thus no AR and MA

+ Drift: that is a constant that can be added to the equation. It will make the model tend upwards or downwards. Imagine having an ARIMA 0,1,0 with a drift. Then you will only have the most recent $y_{t-1}$ and then you will have the constant, which will force the forecast upwards if positive and vise verca.




The notations for ARIMA are:

\begin{equation}
ARIMA_{(p,d,q)}
(\#eq:ARIMApdq)
\end{equation}

*Where:*

+ p = the order of AR
 
+ d = the order of differencing (integration) (NOTE: if this is 0, then the model is reduced to an ARMA model)
  + if we only have differencing, then we call this a random walk
  + this is I, as it is also called Integration order

+ q = the order of MA, hence if q = 2, then MA for $t_{-1}$ and $t_{-2}$ is included in the regression model.

\

### Elaborating on AR models

AR = Autoregressive 

__AR models are appropriate with stationary data__

_This makes sense, as if the data is non stationary (where the variance is constant)_

This can generalized with AR(p), where p = the order of AR, meaning how many prior periods to $Y_t$ to be included. Then how do we select an apporpriate number of lags?

1. The autocorrelation function (ACF): selection criteria: the ACF should decline to zero exponentially fast
2. The partial autocorelation function (PACF)

See the full equation on page 360.

Process:

1. Select order of p
2. Calculate coefficients for each lagged period
3. Forecast using the coefficients
4. Evaluate constantly if the coefficients are still applicable


__Assumptions__

- Data is stationary. If not, then one must deal with that


\

### Elaborating on MA models

MA = Moving average

___IMPORTANT NOTE: this has nothing to do with regular Moving Average, as with using past periods___

This approach applies use residuals (between actual values and fitted values/or forecasted values) multiplied with a coefficient to forecast the coming period. As with AR, we are able to include x amount of previous periods. Thus, we are able to describe MA with:

\begain{equation}
Y_t=\mu+\epsilon_t-\omega_1\epsilon_{t-1}-\omega_2\epsilon_{t-2}-...-\omega_q\epsilon_{t-q}
(\#eq:MAq)
\end{equation}

where:

+ $Y_t$ = the forecast for time period t.
+ $\mu$ = just a constant that is applied in the calculation
+ $\epsilon_t$ = the error term as in any other regression
+ $\omega$ = the coefficients that we are to calculate for each period
  + NOTICE: these can be interpret as wheights put on each period. But it does NOT need to summarize to 1, it can be above and below
+ $\epsilon_{1-q}$ = The error (residual) for each period


Process: 

1. Select order of 1
2. Calculate coefficients for each lagged period
3. Forecast using the coefficients
4. Evaluate constantly if the coefficients are still applicable

Then how do we select an apporpriate number of lags to be included?

1. The autocorrelation function (ACF): selection criteria: the ACF should decline to zero exponentially fast
2. The partial autocorelation function (PACF)



\

## Model Building Process

The Box-Jenkins model-building strategy. It has th following steps

1. Model Identification
    i. Assessment for stationarity - can be done with ACF correlogram.
        a. If found, then use difference, hence ARIMA
    ii. Identify what form of model to be used, e.g., MA, ARMA, ARIMA, or AR. This is done by assessing the time-series' ACF (see examples on page 357 - 359)

2. Model Specification: This is estimating model parameters
  
3. Model Checking
    i. Make model diagnostics:
        a. Residuals to be random, no autocorelation left in them. Can also be checked with Ljung-Box Q stats
        b. No heteroskedasticity
        c. No spike in the ACF, must be within $±2/\sqrt{n}$ confidence interval from 0.
    ii. If several model. Choose the one with the lowest AIC or BIC, depending on the goal.
        a. if that concludes indecisive result, then choose the simplest model (principle of parsimony)
    
4. Forecasting with the Model


## Advantages and disadvantages for ARIMA models

Advantages:

+ Box-Jenkins is a stable tool to get a model for short term accurate forecasts
+ The model is flexible, but dont be fooled by complexity
+ Formal testing procedures are available, such as AIC, BIC etc.

Disadvantages:

+ Large amount of data is needed
+ Each time new data arrives, the model must be estimated again. That is because the parameters follow the most recent information.
+ Construction is based on trial and error



## ADL 

Basically ADL is doing and Autoregressive (AR) model and then we add another variable (this could be anything) which is lagged. Notice that we can model with the order of lagged variables to be included in the model.

As the model is based on AR models, we must have stationarity in the data, just as in normal AR models.

Hence we have the following equation:

\begin{equation}
ADL_{(p,q)}
(\#eq:ADLpq)
\end{equation}


Where

+ p = the order of AR

+ q = the number of lags for the added variable


Procedure:

1. Create the AR model

2. Find the other relevant variable. Transform it to a timeseries using, `ts()`

3. Include the lags in the model using `lag(<ts.object>,<number of lags>)`

Example of how it may look:

```{r}
# GDPGR_ADL22 <- dynlm(GDPGrowth_ts ~ #The dependant variable
#                        L(GDPGrowth_ts) #The AR(1),The dependant variable lagged
#                      + L(GDPGrowth_ts, 2)  #The AR(2),The dependant variable lagged
#                      + L(TSpread_ts)  #Another TS lagged
#                      + L(TSpread_ts, 2) #Another TS lagged two periods
#                      ,start = c(1962, 1), end = c(2012, 4))
```

*L() = lag(), if number of lags are not *

```{r}
#TB3MS <- xts(USMacroSWQ$TB3MS, USMacroSWQ$Date)["1960::2012"]
```


### Vector autoregressive (VAR)

In the approach in ADL we only work with one other variable. Although with VAR models, we are able to include multiple independent variables. This can be written as:

\begin{equation}
VAR(p)
(\#eq:VARp)
\end{equation}

Where:

+ p = p lags of the variable

#### Selection criteria

One may use AIC, BIC or make out of sample assessment to select the best model, hence identifying what lags to be included in the model.



## Exercises - ARIMA


### IBM stock, problem 12 p 405

**Qa**

The data in Table P-12 are weekly prices for IBM stock.

```{r,fig.cap="IBM stock prices"}
#Loading
y <- read_excel("Data/Week47/IBMstock.xls") %>% ts(frequency = 52)

#Plotting
ts.plot(y)
```

It is dififult to say if there is a trend, but there appear to be seasons. This can be futher expected with the correlogram

```{r,fig.cap="Correlogram (acf) IBM Stock prices"}
acf(x = y
    ,lag.max = 52) #We take for a whole year
```

The correlogram suggests that there is seasonality in the data. We only have data for one year, hence 52 periods. It would be interesting to see if the patterns express it self over years.

We could also express the pacf.

The partial correlation coefficient is estimated by fitting autoregressive models of successively higher orders up to lag.max.


```{r,fig.cap="Correlogram (pacf) IBM Stock prices"}
pacf(x = y
    ,lag.max = 52) #We take for a whole year
```

__What approaches does this suggest?__

It suggests, that we should use an AR model, as the acf is tending towards 0 while the pacf quickly drops to 0.

\

**Qb**

Looking at the ts plot, the data does not appear to be stationary. Perhaps there is a small indication of a trend in the data, hence not constant variance around a fixed point.

It therefore suggests that we move into ARIMA, where we apply d order differencing.

**Qc**

We apply AR and d, where we first try with first order, to assess if it is sufficient.

```{r}
p <- 0 #AR order
d <- 1 #Differencing order
q <- 0 #MA order
order <- c(p,d,q)

ARIMAmod <- arima(x = y #The time-series
                  ,order = order
                  )

#Assessing in-samp accuracy
accuracy(object = fitted(ARIMAmod)
         ,x = y)
```

We see an mean percentage error of 1,64%. That is quite low. But also expected as it is in sample.

```{r,fig.cap="Residuals plot IBM stock ARIMA 1,1,0"}
plot(ARIMAmod$residuals,ylab = "Residuals")
```

The changes appear to be randomly distributed around 0.

**Qd**

Perform diagnostic checks to determine the adequacy of your fitted model

_Residuals to be random_

We see from the residuals plot above. It was difficult to get rid of heteroskedasticity

*Note, it was tested with only differencing, and appear to have a more constant variance with this*

*but we still see something indicating that the variance is not entirely constant*

**Constant variance**

This can also be checked using a Ljung-Box test, where the null hypothesis is that there is no relationship between the observations.

```{r}
Box.test(ARIMAmod$residuals
         ,fitdf = p+q) #Because it is applied to and ARIMA model
```

We see that the p-value is below the p-value (5%), hence the model is under misspecification. Although we are close to the threshold, let us assume that it is sufficient.


**Autocorrelation in residuals**

We see that the errors appear not to show autocorrelation. Although there is one period, that does appear to have a spike + the first lagged period, but it is close to the recommended threshold.

```{r}
acf(ARIMAmod$residuals)
```

Then one could test other models and see if they perform better. I have been playing around with the orders, but it does not appear to help with the residuals.



**Qd**

Make forecast for the stock price in week 1.

```{r}
{
  forecast(object = ARIMAmod,h = 1) %>% print()
  y[nrow(y),]  %>% print()
}
```

We see that the forecast is 311,73 where the naive forecast (the most recent period) would just say 304.


*Although one must be very precautions, as it was found that the model is under misspecification*


```{r}
rm(list = ls())
```



### Demand data, problem 7 p 403

Loading the data

```{r}
df <- read_excel("Data/Week47/Demand.xls")
y <- ts(data = df,frequency = 52)
```

**Qa**

Plotting acf

```{r,fig.cap="acf Demand"}
acf(y,lag.max = 52)
```

We see that there is clearly a trend and it appears as if we have seasons, although looking at it does not appear as if we have seasons.

We can support this with a pacf

```{r}
pacf(y)
```

We are able to use an autoregressive approach as acf tends towards 0 and pacf quickly drops to 0.

Lastly we can check for stationarity.

```{r,fig.cap="Demand time-series"}
ts.plot(y)
```

We observe non stationary data, hence we should apply differencing as well.


**Qb**

***Manually doing ARIMA***

```{r,fig.cap="Residuals ARIMA"}
p = 1
d = 1
q = 0
order <- c(p,d,q)
ARIMAmod <- arima(x = y
                  ,order = order)
plot(ARIMAmod$residuals)
```

***Auto ARIMA***

We are also able to apply `auto.arima()` to find the most optimal combination based on the sample data.

```{r}
# Making the model
ts.arima <- auto.arima(y = y)
summary(ts.arima)
```

We see that the this suggest a full ARIMA model with 2 order AR, 1 order differencing and 1 order MA.

Now we can display the residuals to check for independence within the error terms.

```{r,fig.cap="Residuals time-series demand"}
tsdisplay(ts.arima$residuals)
```

It appears as if with have independence in the residuals and also none significant spikes in the correlograms.

We can make a statistical check for this as well, using the Box-Pierce test

```{r}
Box.test(ts.arima$residuals)
```

We see that we cannot reject the null hypothesis, hence it is fair to assume that the observations are independent of each other.


**Qc**

equation for forecast:

Cant find the constant, but one can call the coefficients with the following: 

```{r}
ts.arima[["coef"]]
```


**Qd**

Forecasting demand for the coming four periods. That can be done using `forecast()` where h = 4

```{r}
arima.fh4 <- forecast(ts.arima
                       ,h = 4 #Forecast horizon
                       ,level = 0.95 #Confidence interval
                       )
knitr::kable(arima.fh4,caption = "Forecast with confidence intervals")
```

```{r}
plot(arima.fh4,xlim = c(1.8,2.2))
grid(col = "lightgrey")
```


```{r}
rm(list = ls())
```


### Closing stock quotations, problem 13 p 409

```{r,fig.cap="Diagnostics for ARIMA"}
df <- read_excel("Data/Week47/ClosingStockQuatations.xls")
y <- ts(df,frequency = 365)
y.train <- y[1:145]
y.test <- y[146:150]

ts.arima <- auto.arima(y.train)
tsdiag(ts.arima)
```

We see that the residuals appear to be randomly distributed around a fixed point. Also there does not appear to be residuals that spikes, indicating autocorrelation in the residuals. Assessing the Ljung-Box statistic, we see that there are no values that go beyond the critical level of 5%. Hence it is fair to assume that the residuals are independent of each other.

```{r}
arima.fh5 <- forecast(ts.arima,h = 5,level = 0.95)
knitr::kable(arima.fh5,caption = "Forecast with confidence level")
```


```{r,fig.cap="Forecast Stock Quotations"}
plot(arima.fh5,xlim = c(100,157))
grid(col = "lightgrey")
```

```{r}
accuracy(arima.fh5$mean,x = y.test)
```

We see that the accuracy is 2.5 MAE where the MAPE is 1.8


```{r}
rm(list = ls())
```


### HW: Case 1 page 413-414 (q1-3)

Not done

### HW: Case 4 page 417-419

Not done

### Sales data seasonal

Not done

```{r}
#Arima(y = y,model = #<insert fitted arima model here, to preserve coefficients>#)
```


### In class assignment

It is done somewhere. Otherwise, do it again



## Exercises - ADL


### GDP and CO2 levels

```{r}
df <- read_excel("Data/Week48/GDP_CO2.xls")
y <- df$co2
x <- df$gdp
```

Now the data is loaded and we have defined the y and x variable, being CO2 levels and GDP.

Now we can make the ADL with the following:

```{r}
adl.mod <- auto_ardl(y ~ x
                     ,data = cbind(x, y) #We want to combine the variables into 1 df
                     ,max_order = 10 #Mandatory, could be other orders as well
                     ,selection = "AIC"
                     )
adl.mod <- adl.mod$best_model
adl.mod$order
```

We see that according to AIC, using up to 10 lags, the optimal constellation is with 1 lag for the y variable and including 10 lags from the x variable (the additional explanatory variable)

We can assess the model that we achieved, by calling the summary.

```{r}
summary(adl.mod)
```

We see that some of the lags appaer not to be statistically significant, although, the model is estimated to be optimized to low risk, thus we assume that the model is sufficient.


```{r}
rm(list = ls())
```


### Short- and long-term interest rates, exchange rates

#### Loading data, combining time series and assessment of stationarity

First the data is loaded, we are interested in the following>

1. Short term interest rates
2. Long term interest rates
3. Exchange rates

The data is quarterly

```{r}
df <- read_excel("Data/Week48/EuroMacroData.xlsx")
stn <- df$STN %>% log() %>% ts(frequency = 4)
ltn <- df$LTN %>% log() %>% ts(frequency = 4)
een <- df$EEN %>% log() %>% ts(frequency = 4)
```

Now we can present the acf and pacf for each variable along with the time series for each variable.

```{r,fig.cap="Time-Series Display"}
{
  tsdisplay(stn)
  tsdisplay(ltn)
  tsdisplay(een)
}
```

Now we are able to group the variables in one time series frame, lets denote it with z

```{r}
z <- ts(cbind(stn,ltn,een), frequency = 4)
```

As we observed trend in the time series (hence non stationary data), we can make first order differencing to attempt to get rid of that and get stationary data.

```{r}
dz <- diff(z)
```

We can now plot the data to see the following result.

```{r,fig.cap="Data with 1st order differencing"}
plot(dz)
```

Versus the raw data:

```{r,fig.cap="Raw data"}
plot(z)
```

Where we see that the changes in the observations show stationarity, where the raw data is clearly not stationary.


#### Vector AutoRegressive Model (VAR)

##### 1. Determine order of lags to be included + model estimation

We apply `VARselect()`, which returns the following:

+ selection: Vector with the optimal lag number according to each criterium.
+ criteria: A matrix containing the values of the criteria up to lag.max.

We are only interested in finding the optimal amount of lags, p for the VAR. Hence we apply [["selection"]]

```{r}
VARselect(y = dz
          ,lag.max = 10
          ,type = "const")[["selection"]]
```

We see that the optimal amount fo lags to be included are 1. Fortunately this goes for all of the information criteria.

Hence, we have the following setting $VAR(1)$

Now can estimate the model, using the lag order, that we just found.

```{r}
var1 <- VAR(y = dz
            ,p = 1 #How many lags to be included in the models, e.g., if p = 3,
                   # then three lags of each variable is included.
            ,type = "const")
summary(var1)
```

We see that we get a model for each of the time-series and concludingly a covariance- and correlation matrixt of the residuals for the different models.

Looking at the different model estimations, we see that that e.g., lag 1 of ltn is significant for explaining ltn, where lag 1 of the other variables are not significant and so on.

As written in the code, if one was to insert p = 3 instead of 1, then we would merely have three lagged variables for each model, although it will still reflect the same signifances for the different variables.

__Conclusion:__

We are not able to explain anything with the other models, since there is statistical evidence for relationship between other lagged variables and the stn, ltn and een. Hence, we may be just as fine, just by doing an AR approach to each of the models, perhaps even just an AR(p=1)

\

##### 2. Model diagnostics

Now we can make model diagnostics to check for autocorrelation (serial correlation).

This executes a Portmanteau test for correlation in the errors (i.e., autocorrelation, i.e., serial correlation). The null hypothesis is that there are no autocorrelation

```{r}
serial.test(var1
            ,lags.pt = 10 #It is chosen to be 10, could be anything else, perhaps one could plot it
            ,type = "PT.asymptotic"
            )
```

As the p-value is far above the significane level, 5%, we cannot reject the null hypothesis, hence it is fair to assume, that the residuals are not serial correlated.


##### 3. Response to shocks in the variables

With the following, we are able to deduce how the different variables react (respond) from shocks in the variables.

The y-axis expres the changes where the x-axis express the n steps ahead. Hence in this example it is quarters ahead.

```{r,fig.cap="Resonses from shocks"}
plot(irf(var1,boot = TRUE, ci=0.95))
```

First of all, we always see that shocks in the variables always show direct positive (same direction) response in period 0 on its own variable, that makes sens and it expected. As the illustrations above is based on models that contain one lagged period on the model, we don't include all available information (also, it is likely that other variable explain the relationship).

E.g., looking at shocks in stn (the first figure), we see that ltn immediately tend to move in the same direction, where the effect decays over time and after some periods, it will practically be 0.

Notice, that some response have confidence intervals that are below and above 0, hence we are not really able to say how the reaction will be, same or the opposite direction.


###### Exogenious vs. endogenious

Terms:

- Exogenious: if a variable is extremely exogenious, it is not explain or determined by other variables
- Endogenious: if a variable is extremely endogenoius, it is explianed or determined by one or more variables.

This can be used to rank variables based on how much they are affected by other variables, with the most exogenious variable first (**This is called Cholesky ordering**), hence we rank based on the following principle:

*lets say, that we have four variables, we want to order according to Cholesky ordering*

1. Most exogenious
2. Less exogenious than 1 more than 3. Hence, more endogenious than 1. but less than 3.
3. Less exogenious than 2 more than 4. Hence, more endogenious than 2. but less than 4.
4. Most endogenious

**Importance!**

This is important, as it matters in the end how the forecast is done. see slides from lecture 10 page 12.


##### 4. Forecasting with ADL

This has the following steps:

1. Data partition
2. Select the order of VAR
3. Fit the model + check residuals
4. Make the forecast
5. Assessing accuracy


###### Step 1 - Data partition

We apply the differenced data from the exercise. The first 90 obersvations as train data and the rest test data.

```{r}
insampdz <- ts(dz[1:90, 1:3], frequency = 4)
outsampdz <- dz[91:115, 1:3]
```

###### Step 2 - Select the order of VAR

We find the optimal order for p (recall VAR(p))

```{r}
VARselect(insampdz
          ,lag.max = 10
          ,type="const")[["selection"]]
```

We see that the optimal order is 1.

###### Step 3 - Fit the model + check residuals

The data is fitted and we call the summary.

```{r}
fit <- VAR(y = insampdz #The train data
           ,p = 1 #Found in step 2
           ,type="const")
summary(fit)
```

We see that there is not really any significant relationship between the different variables and only the lagged values of its own variable appear to be significant. 

Now we can check the residuals for independency (not having serial correlation, i.e., autocorrelation). The null hypothesis is, that there is no autocorrelation.

```{r}
serial.test(fit, lags.pt=10, type="PT.asymptotic")
```

We see the p-value being insignificant, hence we cannot reject H0 and it is fair to asume no autocorrelation.

###### Step 4 - Make the forecast

We apply the `forecast()` function to forecast the coming periods

```{r}
fcast <- forecast(object = fit
                  ,h = 25) #Forecasting 25 quarters
plot(fcast)
```

We see the forecast having a very wide confidence interval and barely any movements after a couple of periods.

This indicates that it is a bit random how it moves. If we had more seasonality, trends and cycles, then it would be easier to forecast coming periods.


###### Step 5 - Assessing accuracy

Now we can assess accuracy of the forecasts

```{r}
accuracy(object = fcast$forecast$stn
         ,x = outsampdz[1:25,1]) #1 for stn
```

```{r}
accuracy(object = fcast$forecast$ltn
         ,x = outsampdz[1:25,2])
```

```{r}
accuracy(object = fcast$forecast$een
         ,x = outsampdz[1:25,3])
```


We see that the MAPE is close 100% or more than 100% on average. Hence the models are not performing well. That is also expected as we see the forecasts confidence intervals are very wide and not moving much.

Even though all models are bad, it appears as if we are best at estimating ltn.














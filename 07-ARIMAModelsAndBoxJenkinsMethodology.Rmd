---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r librariesARIMA,include=FALSE}
library(readxl)
```


# ARIMA Models and Box-Jenkins Methodology {#ARIMA}

*Explanation here*

*Considering a single time series $Y_t$, the interest is in determining the relationship between $Y_t$ and its past.*

**Literature**

+ HW: The Box-Jenkins (ARIMA) Methodology


## ARIMA {#ARIMAΗ2}

Basically it is a framework for adding AR and or MA into the regression model, where:

- AR: A time-series is predicted using its own history, can be explained by: $Y_t=β_0+β_1Y_{t-1}+ε_1$ 
- MA: Model predicts based on current and past shocks to the series. An MA(q) model: $Y_t=\mu+\epsilon_t-\omega_1\ \epsilon_{e-1}-...-\omega_q\ \epsilon_{e-q}$

**It is important to notice, that it is not by default an advantage of including both methods, but one should select the appropriate method depending on the looks corellogram**

__Key concepts__

ARMA that is the approach to stationary data. If the data is nonstationary, one must make the data stationary by differencing. leading to ARIMA. Hence:

1. ARMA: When we have stationarity
2. ARIMA: When we have non-stationarity


The notations for ARIMA are:

$$ARIMA_{(p,d,q)}$$

*Where:*

+ p = the order of AR
 
+ d = the order of differencing (NOTE: if this is 0, then the model is reduced to an ARMA model)

+ q = the order of MA, hence if q = 2, then MA for $t_{-1}$ and $t_{-2}$ is included in the regression model.

\

### Elaborating on AR models

AR = Autoregressive 

__AR models are appropriate with stationary data__

_This makes sense, as if the data is non stationary (where the variance is constant)_

This can generalized with AR(p), where p = the order of AR, meaning how many prior periods to $Y_t$ to be included. Then how do we select an apporpriate number of lags?

1. The autocorrelation function (ACF): selection criteria: the ACF should decline to zero exponentially fast
2. The partial autocorelation function (PACF)

See the full equation on page 360.

Process:

1. Select order of p
2. Calculate coefficients for each lagged period
3. Forecast using the coefficients
4. Evaluate constantly if the coefficients are still applicable


__Assumptions__

- Data is stationary. If not, then one must deal with that


\

### Elaborating on MA models

MA = Moving average

___IMPORTANT NOTE: this has nothing to do with regular Moving Average, as with using past periods___

This approach applies use residuals (between actual values and fitted values/or forecasted values) multiplied with a coefficient to forecast the coming period. As with AR, we are able to include x amount of previous periods. Thus, we are able to describe MA with:

\begain{equation}
$Y_t=\mu+\epsilon_t-\omega_1\epsilon_{t-1}-\omega_2\epsilon_{t-2}-...-\omega_q\epsilon_{t-q}$
(\#eq:MA(q))
\end{equation}

where:

+ $Y_t$ = the forecast for time period t.
+ $\mu$ = just a constant that is applied in the calculation
+ $\epsilon_t$ = the error term as in any other regression
+ $\omega$ = the coefficients that we are to calculate for each period
  + NOTICE: these can be interpret as wheights put on each period. But it does NOT need to summarize to 1, it can be above and below
+ $\epsilon_{1-q}$ = The error (residual) for each period


Process: 

1. Select order of 1
2. Calculate coefficients for each lagged period
3. Forecast using the coefficients
4. Evaluate constantly if the coefficients are still applicable

Then how do we select an apporpriate number of lags to be included?

1. The autocorrelation function (ACF): selection criteria: the ACF should decline to zero exponentially fast
2. The partial autocorelation function (PACF)



\

## Model Building Process

The Box-Jenkins model-building strategy. It has th following steps

1. Model Identification
    i. Assessment for stationarity - can be done with ACF correlogram.
        a. If found, then use difference, hence ARIMA
    ii. Identify what form of model to be used, e.g., MA, ARMA, ARIMA, or AR. This is done by assessing the time-series' ACF (see examples on page 357 - 359)

2. Model Specification: This is estimating model parameters
  
3. Model Checking
    i. Make model diagnostics:
        a. Residuals to be random, no autocorelation left in them. Can also be checked with Ljung-Box Q stats
        b. No heteroskedasticity
        c. No spike in the ACF, must be within $±2/\sqrt{n}$ confidence interval from 0.
    ii. If several model. Choose the one with the lowest AIC or BIC, depending on the goal.
        a. if that concludes indecisive result, then choose the simplest model (principle of parsimony)
    
4. Forecasting with the Model


## Advantages and disadvantages for ARIMA models

Advantages:

+ Box-Jenkins is a stable tool to get a model for short term accurate forecasts
+ The model is flexible, but dont be fooled by complexity
+ Formal testing procedures are available, such as AIC, BIC etc.

Disadvantages:

+ Large amount of data is needed
+ Each time new data arrives, the model must be estimated again. That is because the parameters follow the most recent information.
+ Construction is based on trial and error







\

## ADL 

Basically ADL is doing and Autoregressive (AR) model and then we add another variable, that is lagged. Notice that we can model with the order of lagged variables to be included in the model.

Hence we have the following equation:

$$ADL_{(p,q)}$$

Where

+ p = the order of AR

+ q = the number of variables included with the lagged added timeseries


Procedure:

1. Create the AR model

2. Find the other relevant variable. Transform it to a timeseries using, `ts()`

3. Include the lags in the model using `lag(<ts.object>,<number of lags>)`

Example of how it may look:

```{r}
# GDPGR_ADL22 <- dynlm(GDPGrowth_ts ~ #The dependant variable
#                        L(GDPGrowth_ts) #The AR(1),The dependant variable lagged
#                      + L(GDPGrowth_ts, 2)  #The AR(2),The dependant variable lagged
#                      + L(TSpread_ts)  #Another TS lagged
#                      + L(TSpread_ts, 2) #Another TS lagged two periods
#                      ,start = c(1962, 1), end = c(2012, 4))
```

```{r}
#TB3MS <- xts(USMacroSWQ$TB3MS, USMacroSWQ$Date)["1960::2012"]
```


## Exercises


### IBM stock, problem 12 p 405

**Qa**

The data in Table P-12 are weekly prices for IBM stock.

```{r,fig.cap="IBM stock prices"}
#Loading
y <- read_excel("Data/Week47/IBMstock.xls") %>% ts(frequency = 52)

#Plotting
ts.plot(y)
```

It is dififult to say if there is a trend, but there appear to be seasons. This can be futher expected with the correlogram

```{r,fig.cap="Correlogram (acf) IBM Stock prices"}
acf(x = y
    ,lag.max = 52) #We take for a whole year
```

The correlogram suggests that there is seasonality in the data. We only have data for one year, hence 52 periods. It would be interesting to see if the patterns express it self over years.

We could also express the pacf.

The partial correlation coefficient is estimated by fitting autoregressive models of successively higher orders up to lag.max.


```{r,fig.cap="Correlogram (pacf) IBM Stock prices"}
pacf(x = y
    ,lag.max = 52) #We take for a whole year
```

__What approaches does this suggest?__

It suggests, that we should use an AR model, as the acf is tending towards 0 while the pacf quickly drops to 0.

\

**Qb**

Looking at the ts plot, the data does not appear to be stationary. Perhaps there is a small indication of a trend in the data, hence not constant variance around a fixed point.

It therefore suggests that we move into ARIMA, where we apply d order differencing.

**Qc**

We apply AR and d, where we first try with first order, to assess if it is sufficient.

```{r}
p <- 0 #AR order
d <- 1 #Differencing order
q <- 0 #MA order
order <- c(p,d,q)

ARIMAmod <- arima(x = y #The time-series
                  ,order = order
                  )

#Assessing in-samp accuracy
accuracy(object = fitted(ARIMAmod)
         ,x = y)
```

We see an mean percentage error of 1,64%. That is quite low. But also expected as it is in sample.

```{r,fig.cap="Residuals plot IBM stock ARIMA 1,1,0"}
plot(ARIMAmod$residuals,ylab = "Residuals")
```

The changes appear to be randomly distributed around 0.

**Qd**

Perform diagnostic checks to determine the adequacy of your fitted model

_Residuals to be random_

We see from the residuals plot above. It was difficult to get rid of heteroskedasticity

*Note, it was tested with only differencing, and appear to have a more constant variance with this*

*but we still see something indicating that the variance is not entirely constant*

**Constant variance**

This can also be checked using a Ljung-Box test, where the null hypothesis is that there is no relationship between the observations.

```{r}
Box.test(ARIMAmod$residuals
         ,fitdf = p+q) #Because it is applied to and ARIMA model
```

We see that the p-value is below the p-value (5%), hence the model is under misspecification. Although we are close to the threshold, let us assume that it is sufficient.


**Autocorrelation in residuals**

We see that the errors appear not to show autocorrelation. Although there is one period, that does appear to have a spike + the first lagged period, but it is close to the recommended threshold.

```{r}
acf(ARIMAmod$residuals)
```

Then one could test other models and see if they perform better. I have been playing around with the orders, but it does not appear to help with the residuals.



**Qd**

Make forecast for the stock price in week 1.

```{r}
{
  forecast(object = ARIMAmod,h = 1) %>% print()
  y[nrow(y),]  %>% print()
}
```

We see that the forecast is 311,73 where the naive forecast (the most recent period) would just say 304.


*Although one must be very precautions, as it was found that the model is under misspecification*


```{r}
rm(list = ls())
```



### Demand data, problem 7 p 403

```{r}
df <- read_excel("Data/Week47/Demand.xls")
```





### Closing stock quotations, problem 13 p 403

### HW: Case 1 page 413-414 (q1-3)

### HW: Case 4 page 417-419

### Sales data seasonal




















